{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulazeezb/1_SparkFundsInvestmentAnalysis/blob/master/V13_Goku.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking GPU Version"
      ],
      "metadata": {
        "id": "NAjscLW7Sk9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "t8nxH9ZvSaY6",
        "outputId": "fc2ad37a-9750-48d7-b504-6e8c5d537f23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May  3 10:16:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "KEPzY7qCS2Lc",
        "outputId": "d66b4d7e-b2a8-43c4-d2af-caab9cf0aee4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Part 1: Getting Started - Install Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23TOba33L4qf",
        "outputId": "a860cafc-db35-4103-9d30-f071757ed201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wrds\n",
            "  Downloading wrds-3.1.6-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from wrds) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from wrds) (1.5.3)\n",
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from wrds) (1.10.1)\n",
            "Collecting sqlalchemy<2\n",
            "  Downloading SQLAlchemy-1.4.48-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2->wrds) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->wrds) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->wrds) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->wrds) (1.16.0)\n",
            "Installing collected packages: sqlalchemy, psycopg2-binary, wrds\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.10\n",
            "    Uninstalling SQLAlchemy-2.0.10:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.10\n",
            "Successfully installed psycopg2-binary-2.9.6 sqlalchemy-1.4.48 wrds-3.1.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1\n",
            "竢ｬ Downloading https://github.com/conda-forge/miniforge/releases/download/23.1.0-1/Mambaforge-23.1.0-1-Linux-x86_64.sh...\n",
            "沒ｦ Installing...\n",
            "沒 Adjusting configuration...\n",
            "洸ｹ Patching environment...\n",
            "竢ｲ Done in 0:00:12\n",
            "沐 Restarting kernel...\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../libgl1-mesa-glx_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
            "Selecting previously unselected package swig4.0.\n",
            "Preparing to unpack .../swig4.0_4.0.1-5build1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.1-5build1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.1-5build1_all.deb ...\n",
            "Unpacking swig (4.0.1-5build1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
            "Setting up swig4.0 (4.0.1-5build1) ...\n",
            "Setting up swig (4.0.1-5build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ],
      "source": [
        "## install finrl library\n",
        "!pip install wrds\n",
        "!pip install swig\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n"
      ],
      "metadata": {
        "id": "haJsmd15TCdH",
        "outputId": "d634e3eb-b1b6-4286-ccc4-98e540e92573",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-kghd2c2q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-kghd2c2q\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit a3598dae504bcd834d12b17110b5aa91c1c5305d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2\n",
            "  Cloning https://github.com/quantopian/pyfolio.git to /tmp/pip-install-mo0oou19/pyfolio_50c0b2fea63f45b89ee7b47db6a887e8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/quantopian/pyfolio.git /tmp/pip-install-mo0oou19/pyfolio_50c0b2fea63f45b89ee7b47db6a887e8\n",
            "  Resolved https://github.com/quantopian/pyfolio.git to commit 4b901f6d73aa02ceb6d04b7d83502e5c6f2e81aa\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-mo0oou19/elegantrl_30aea606f62048b59c5c4dd0dde8f352\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-mo0oou19/elegantrl_30aea606f62048b59c5c4dd0dde8f352\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit b974a806e6235f59055c954418e54640fa549331\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting stable-baselines3<2.0.0,>=1.6.2\n",
            "  Downloading stable_baselines3-1.8.0-py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m174.5/174.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym>=0.17\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jqdatasdk\n",
            "  Downloading jqdatasdk-1.8.11-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stockstats>=0.4.0\n",
            "  Downloading stockstats-0.5.2-py2.py3-none-any.whl (20 kB)\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.2.18-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas>=1.1.5\n",
            "  Downloading pandas-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alpaca_trade_api>=2.1.0\n",
            "  Downloading alpaca_trade_api-3.0.0-py3-none-any.whl (33 kB)\n",
            "Collecting lz4\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.17.3\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ccxt>=1.66.32\n",
            "  Downloading ccxt-3.0.90-py2.py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib-metadata==4.13.0\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Collecting scikit-learn>=0.21.0\n",
            "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrds>=3.1.6\n",
            "  Using cached wrds-3.1.6-py3-none-any.whl (12 kB)\n",
            "Collecting ray[default,tune]>=2.0.0\n",
            "  Downloading ray-2.4.0-cp310-cp310-manylinux2014_x86_64.whl (58.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting exchange_calendars==3.6.3\n",
            "  Downloading exchange_calendars-3.6.3.tar.gz (152 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyluach\n",
            "  Downloading pyluach-2.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting python-dateutil\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toolz in /usr/local/lib/python3.10/site-packages (from exchange_calendars==3.6.3->finrl==0.3.5) (0.12.0)\n",
            "Collecting korean_lunar_calendar\n",
            "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting PyYAML==6.0\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>2 in /usr/local/lib/python3.10/site-packages (from alpaca_trade_api>=2.1.0->finrl==0.3.5) (2.28.2)\n",
            "Collecting deprecation==2.1.0\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting msgpack==1.0.3\n",
            "  Downloading msgpack-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m323.7/323.7 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11,>=9.0\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.10/site-packages (from alpaca_trade_api>=2.1.0->finrl==0.3.5) (1.26.15)\n",
            "Collecting aiohttp==3.8.1\n",
            "  Downloading aiohttp-3.8.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websocket-client<2,>=0.56.0\n",
            "  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs>=17.3.0\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting charset-normalizer<3.0,>=2.0\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Collecting packaging\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.10/site-packages (from ccxt>=1.66.32->finrl==0.3.5) (65.6.3)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.10/site-packages (from ccxt>=1.66.32->finrl==0.3.5) (2022.12.7)\n",
            "Collecting aiodns>=1.1.1\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.10/site-packages (from ccxt>=1.66.32->finrl==0.3.5) (40.0.1)\n",
            "Collecting gym-notices>=0.0.4\n",
            "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
            "Collecting cloudpickle>=1.2.0\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting tzdata>=2022.1\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
            "Collecting protobuf!=3.19.5,>=3.15.3\n",
            "  Downloading protobuf-4.22.3-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting virtualenv<20.21.1,>=20.0.24\n",
            "  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonschema\n",
            "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio<=1.51.3,>=1.42.0\n",
            "  Downloading grpcio-1.51.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click>=7.0\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open\n",
            "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prometheus-client>=0.7.1\n",
            "  Downloading prometheus_client-0.16.0-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m122.5/122.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp-cors\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting colorful\n",
            "  Downloading colorful-0.5.5-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencensus\n",
            "  Downloading opencensus-0.11.2-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic\n",
            "  Downloading pydantic-1.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py-spy>=0.2.0\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpustat>=1.0.0\n",
            "  Downloading gpustat-1.1.tar.gz (97 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tabulate\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Collecting joblib>=1.1.1\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting scipy>=1.3.2\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym>=0.17\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch>=1.11\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf!=3.19.5,>=3.15.3\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlalchemy<2\n",
            "  Using cached SQLAlchemy-1.4.48-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "Collecting psycopg2-binary\n",
            "  Using cached psycopg2_binary-2.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Collecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting thriftpy2>=0.3.9\n",
            "  Downloading thriftpy2-0.4.16.tar.gz (643 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m643.4/643.4 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymysql>=0.7.6\n",
            "  Downloading PyMySQL-1.0.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.39.3-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing>=2.3.1\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow>=6.2.0\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting ipython>=3.2.3\n",
            "  Downloading ipython-8.13.1-py3-none-any.whl (797 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m797.6/797.6 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seaborn>=0.7.1\n",
            "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting empyrical>=0.5.0\n",
            "  Downloading empyrical-0.5.5.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting frozendict>=2.3.4\n",
            "  Downloading frozendict-2.3.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multitasking>=0.0.7\n",
            "  Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
            "Collecting lxml>=4.9.1\n",
            "  Downloading lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting html5lib>=1.1\n",
            "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting appdirs>=1.4.4\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting beautifulsoup4>=4.11.1\n",
            "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycares>=4.0.0\n",
            "  Downloading pycares-4.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/site-packages (from cryptography>=2.6.1->ccxt>=1.66.32->finrl==0.3.5) (1.15.1)\n",
            "Collecting pandas-datareader>=0.2\n",
            "  Downloading pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil>=5.6.0\n",
            "  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blessed>=1.17.1\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-ml-py>=11.450.129\n",
            "  Downloading nvidia_ml_py-11.525.112-py3-none-any.whl (35 kB)\n",
            "Collecting webencodings\n",
            "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Collecting backcall\n",
            "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30\n",
            "  Downloading prompt_toolkit-3.0.38-py3-none-any.whl (385 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting decorator\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting pygments>=2.4.0\n",
            "  Downloading Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pexpect>4.3\n",
            "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting traitlets>=5\n",
            "  Downloading traitlets-5.9.0-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pickleshare\n",
            "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting stack-data\n",
            "  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>2->alpaca_trade_api>=2.1.0->finrl==0.3.5) (3.4)\n",
            "Collecting greenlet!=0.4.17\n",
            "  Downloading greenlet-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (613 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m613.7/613.7 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ply<4.0,>=3.4\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11->stable-baselines3<2.0.0,>=1.6.2->finrl==0.3.5) (0.40.0)\n",
            "Collecting cmake\n",
            "  Downloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lit\n",
            "  Downloading lit-16.0.2.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting platformdirs<4,>=2.4\n",
            "  Downloading platformdirs-3.5.0-py3-none-any.whl (15 kB)\n",
            "Collecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym[box2d]\n",
            "  Downloading gym-0.26.1.tar.gz (719 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m719.9/719.9 kB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading gym-0.26.0.tar.gz (710 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m710.3/710.3 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading gym-0.25.2.tar.gz (734 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m734.5/734.5 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading gym-0.25.1.tar.gz (732 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m732.2/732.2 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading gym-0.25.0.tar.gz (720 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m720.4/720.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m696.4/696.4 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading gym-0.24.0.tar.gz (694 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m694.4/694.4 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading gym-0.22.0.tar.gz (631 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m631.1/631.1 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyglet>=1.4.0\n",
            "  Downloading pyglet-2.0.6-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
            "  Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencensus-context>=0.1.3\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Collecting google-api-core<3.0.0,>=1.0.0\n",
            "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wcwidth>=0.1.4\n",
            "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt>=1.66.32->finrl==0.3.5) (2.21)\n",
            "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
            "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<3.0dev,>=2.14.1\n",
            "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parso<0.9.0,>=0.8.0\n",
            "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ptyprocess>=0.5\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting pure-eval\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting executing>=1.2.0\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting asttokens>=2.1.0\n",
            "  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
            "Collecting mpmath>=0.19\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyasn1<0.6.0,>=0.4.6\n",
            "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: finrl, exchange_calendars, gym, elegantrl, gputil, pyfolio, empyrical, gpustat, thriftpy2, box2d-py, lit\n",
            "  Building wheel for finrl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for finrl: filename=finrl-0.3.5-py3-none-any.whl size=4668722 sha256=b5d9990feddfce4dac1109677bd1616fcf78abb54bcf1b30c280638a7696b8d0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o1ri3xm8/wheels/72/3b/1a/0fc805a8cc65ecd5bfe4f74a3c586b6075678b8ba53fd8f749\n",
            "  Building wheel for exchange_calendars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for exchange_calendars: filename=exchange_calendars-3.6.3-py3-none-any.whl size=182636 sha256=23f096d03830ef288577bdfe3d565944669463917f5dd94ee9e6b0f3ac389c18\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/ca/8f/1e1c90cc79fb3ca9b5413ff58e3fccf3baf2182c994c6dfd37\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mﾃ予u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m笏\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m笊ｰ笏>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for gym\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for gym\n",
            "  Building wheel for elegantrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elegantrl: filename=elegantrl-0.3.6-py3-none-any.whl size=195067 sha256=0789852c45710b7487ad7f8c787906a832371da5364d4beb8b432436bec3090a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o1ri3xm8/wheels/c0/51/a5/b05f165548221bc570f7223babd33e2992fa873cdcebe2d229\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7408 sha256=43e5e00b64330ceca82b0b2332a55029631afd35b9eaa8cc4d707613f9482a34\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "  Building wheel for pyfolio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfolio: filename=pyfolio-0.9.2+75.g4b901f6-py3-none-any.whl size=75775 sha256=20701b364f22ff791c94f70c98771137c58f37cd70be901db94f0347119b406d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o1ri3xm8/wheels/c4/1d/91/8ab5d1c88a11b06a63dcd6a69ea81547e2247123232949bb26\n",
            "  Building wheel for empyrical (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for empyrical: filename=empyrical-0.5.5-py3-none-any.whl size=39779 sha256=c578b7489ee7afb8225a9b08a994523f91ace1d8158d73439aed4bb57a97ea77\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/2e/f2/d6d2d9a1eb8fbbd9949bb5d4c00f753e3b74e5bd7ed10b1d36\n",
            "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.1-py3-none-any.whl size=26298 sha256=850b0fd864ce416107c0d0b9767d57e9cdf9ff3bfde28198282522a39b54e4df\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/d0/2c/1e02440645c2318ba03aea99993a44a9108dc8f74de0bd370b\n",
            "  Building wheel for thriftpy2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thriftpy2: filename=thriftpy2-0.4.16-cp310-cp310-linux_x86_64.whl size=522576 sha256=3fc1e580c849dad53dd870db6113764321778d37943e080e91ba649187f354b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/f5/1d/fe404692e1c8aaea45220c322d1d0f32c9fd40eb0e2bdd571e\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=494649 sha256=6c45d1a8612e8d313e6ad0cec227d2e960d69438b46b43768f1a2329279b4286\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-16.0.2-py3-none-any.whl size=88190 sha256=79ee9f70ccdf2114f6d535e42be62da6c2718265d0841cdb0f06aee40707ef47\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/31/6f/140862d5c69ddd665b3ed9485f4c93aa9c84cb34b359cba3ce\n",
            "Successfully built finrl exchange_calendars elegantrl gputil pyfolio empyrical gpustat thriftpy2 box2d-py lit\n",
            "Failed to build gym\n",
            "Installing collected packages: webencodings, wcwidth, pytz, pyglet, py-spy, pure-eval, ptyprocess, ply, pickleshare, opencensus-context, nvidia-ml-py, multitasking, msgpack, mpmath, lit, korean_lunar_calendar, gputil, executing, distlib, colorful, cmake, box2d-py, backcall, appdirs, zipp, websockets, websocket-client, tzdata, typing-extensions, traitlets, threadpoolctl, tabulate, sympy, soupsieve, smart-open, six, PyYAML, pyrsistent, pyparsing, pymysql, pyluach, pygments, pyasn1, psycopg2-binary, psutil, protobuf, prompt-toolkit, prometheus-client, platformdirs, pillow, pexpect, parso, packaging, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, multidict, MarkupSafe, lz4, lxml, kiwisolver, joblib, grpcio, greenlet, frozenlist, frozendict, fonttools, filelock, decorator, cycler, cloudpickle, click, charset-normalizer, cachetools, attrs, async-timeout, yarl, virtualenv, thriftpy2, tensorboardX, sqlalchemy, scipy, rsa, python-dateutil, pydantic, pycares, pyasn1-modules, nvidia-cusolver-cu11, nvidia-cudnn-cu11, matplotlib-inline, jsonschema, jinja2, jedi, importlib-metadata, html5lib, gym, googleapis-common-protos, deprecation, contourpy, blessed, beautifulsoup4, asttokens, aiosignal, stack-data, scikit-learn, ray, pandas, matplotlib, gpustat, google-auth, aiohttp, aiodns, yfinance, wrds, stockstats, seaborn, pandas-datareader, jqdatasdk, ipython, google-api-core, exchange_calendars, ccxt, alpaca_trade_api, aiohttp-cors, opencensus, empyrical, pyfolio, triton, torch, stable-baselines3, elegantrl, finrl\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.1.0\n",
            "    Uninstalling charset-normalizer-3.1.0:\n",
            "      Successfully uninstalled charset-normalizer-3.1.0\n",
            "  Running setup.py install for gym ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: gym was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. pip 23.1 will enforce this behaviour change. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.2 PyYAML-6.0 aiodns-3.0.0 aiohttp-3.8.1 aiohttp-cors-0.7.0 aiosignal-1.3.1 alpaca_trade_api-3.0.0 appdirs-1.4.4 asttokens-2.2.1 async-timeout-4.0.2 attrs-23.1.0 backcall-0.2.0 beautifulsoup4-4.12.2 blessed-1.20.0 box2d-py-2.3.5 cachetools-5.3.0 ccxt-3.0.90 charset-normalizer-2.1.1 click-8.1.3 cloudpickle-2.2.1 cmake-3.26.3 colorful-0.5.5 contourpy-1.0.7 cycler-0.11.0 decorator-5.1.1 deprecation-2.1.0 distlib-0.3.6 elegantrl-0.3.6 empyrical-0.5.5 exchange_calendars-3.6.3 executing-1.2.0 filelock-3.12.0 finrl-0.3.5 fonttools-4.39.3 frozendict-2.3.8 frozenlist-1.3.3 google-api-core-2.11.0 google-auth-2.17.3 googleapis-common-protos-1.59.0 gpustat-1.1 gputil-1.4.0 greenlet-2.0.2 grpcio-1.51.3 gym-0.21.0 html5lib-1.1 importlib-metadata-4.13.0 ipython-8.13.1 jedi-0.18.2 jinja2-3.1.2 joblib-1.2.0 jqdatasdk-1.8.11 jsonschema-4.17.3 kiwisolver-1.4.4 korean_lunar_calendar-0.3.1 lit-16.0.2 lxml-4.9.2 lz4-4.3.2 matplotlib-3.7.1 matplotlib-inline-0.1.6 mpmath-1.3.0 msgpack-1.0.3 multidict-6.0.4 multitasking-0.0.11 networkx-3.1 numpy-1.24.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-ml-py-11.525.112 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 opencensus-0.11.2 opencensus-context-0.1.3 packaging-23.1 pandas-2.0.1 pandas-datareader-0.10.0 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 pillow-9.5.0 platformdirs-3.5.0 ply-3.11 prometheus-client-0.16.0 prompt-toolkit-3.0.38 protobuf-3.20.3 psutil-5.9.5 psycopg2-binary-2.9.6 ptyprocess-0.7.0 pure-eval-0.2.2 py-spy-0.3.14 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycares-4.3.0 pydantic-1.10.7 pyfolio-0.9.2+75.g4b901f6 pyglet-2.0.6 pygments-2.15.1 pyluach-2.2.0 pymysql-1.0.3 pyparsing-3.0.9 pyrsistent-0.19.3 python-dateutil-2.8.2 pytz-2023.3 ray-2.4.0 rsa-4.9 scikit-learn-1.2.2 scipy-1.10.1 seaborn-0.12.2 six-1.16.0 smart-open-6.3.0 soupsieve-2.4.1 sqlalchemy-1.4.48 stable-baselines3-1.8.0 stack-data-0.6.2 stockstats-0.5.2 sympy-1.11.1 tabulate-0.9.0 tensorboardX-2.6 threadpoolctl-3.1.0 thriftpy2-0.4.16 torch-2.0.0 traitlets-5.9.0 triton-2.0.0 typing-extensions-4.5.0 tzdata-2023.3 virtualenv-20.21.0 wcwidth-0.2.6 webencodings-0.5.1 websocket-client-1.5.1 websockets-10.4 wrds-3.1.6 yarl-1.9.2 yfinance-0.2.18 zipp-3.15.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Check if the additional packages needed are present, if not install them"
      ],
      "metadata": {
        "id": "DMaIlOcVTJfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install trading_calendars\n",
        "# !pip install alpaca_trade_api\n",
        "# !pip install ccxt\n",
        "# !pip install jqdatasdk\n",
        "# !pip install wrds\n",
        "\n",
        "# !pip install lz4\n",
        "# !pip install ray[tune]\n",
        "# !pip install tensorboardX\n",
        "# !pip install gputil\n",
        "\n",
        "#%%capture\n",
        "if True:\n",
        "    # installing packages\n",
        "    !pip install pyfolio-reloaded  #original pyfolio no longer maintained\n",
        "    !pip install optuna\n",
        "    !pip install -U \"ray[rllib]\"\n",
        "    !pip install plotly\n",
        "    !pip install ipywidgets\n",
        "    !pip install -U kaleido   # enables saving plots to file\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "UfmXkH1rTFM7",
        "outputId": "d9c4bdf1-f8a1-446e-8be1-566dd0884c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyfolio-reloaded\n",
            "  Downloading pyfolio_reloaded-0.9.5-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.10/site-packages (from pyfolio-reloaded) (2.0.1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/site-packages (from pyfolio-reloaded) (1.10.1)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.10/site-packages (from pyfolio-reloaded) (2023.3)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.10/site-packages (from pyfolio-reloaded) (8.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.16.1 in /usr/local/lib/python3.10/site-packages (from pyfolio-reloaded) (1.2.2)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.10/site-packages (from pyfolio-reloaded) (3.7.1)\n",
            "Collecting empyrical-reloaded>=0.5.8\n",
            "  Downloading empyrical_reloaded-0.5.9-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/site-packages (from pyfolio-reloaded) (1.24.3)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.10/site-packages (from pyfolio-reloaded) (0.12.2)\n",
            "Requirement already satisfied: pandas-datareader>=0.4 in /usr/local/lib/python3.10/site-packages (from empyrical-reloaded>=0.5.8->pyfolio-reloaded) (0.10.0)\n",
            "Collecting bottleneck>=1.3.0\n",
            "  Downloading Bottleneck-1.3.7-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (354 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m354.0/354.0 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: yfinance>=0.1.63 in /usr/local/lib/python3.10/site-packages (from empyrical-reloaded>=0.5.8->pyfolio-reloaded) (0.2.18)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio-reloaded) (2.15.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio-reloaded) (0.1.6)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio-reloaded) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio-reloaded) (5.9.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio-reloaded) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio-reloaded) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio-reloaded) (5.1.1)\n",
            "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio-reloaded) (0.6.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio-reloaded) (3.0.38)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio-reloaded) (0.18.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded) (23.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded) (4.39.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded) (9.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded) (1.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded) (3.0.9)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas>=0.18.1->pyfolio-reloaded) (2023.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.16.1->pyfolio-reloaded) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.16.1->pyfolio-reloaded) (1.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded) (0.8.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/site-packages (from pandas-datareader>=0.4->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (4.9.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from pandas-datareader>=0.4->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (2.28.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=3.2.3->pyfolio-reloaded) (0.2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->pyfolio-reloaded) (1.16.0)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/site-packages (from yfinance>=0.1.63->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (2.3.8)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/site-packages (from yfinance>=0.1.63->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (1.1)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/site-packages (from yfinance>=0.1.63->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (1.4.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/site-packages (from yfinance>=0.1.63->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (0.0.11)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/site-packages (from yfinance>=0.1.63->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (4.12.2)\n",
            "Requirement already satisfied: cryptography>=3.3.2 in /usr/local/lib/python3.10/site-packages (from yfinance>=0.1.63->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (40.0.1)\n",
            "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded) (0.2.2)\n",
            "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded) (1.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded) (2.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance>=0.1.63->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (2.4.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/site-packages (from cryptography>=3.3.2->yfinance>=0.1.63->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (1.15.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/site-packages (from html5lib>=1.1->yfinance>=0.1.63->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->pandas-datareader>=0.4->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->pandas-datareader>=0.4->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->pandas-datareader>=0.4->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->pandas-datareader>=0.4->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (2022.12.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance>=0.1.63->empyrical-reloaded>=0.5.8->pyfolio-reloaded) (2.21)\n",
            "Installing collected packages: bottleneck, empyrical-reloaded, pyfolio-reloaded\n",
            "Successfully installed bottleneck-1.3.7 empyrical-reloaded-0.5.9 pyfolio-reloaded-0.9.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.1.1-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/site-packages (from optuna) (1.4.48)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from optuna) (23.1)\n",
            "Collecting cmaes>=0.9.1\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from optuna) (1.24.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/site-packages (from optuna) (6.0)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.10.4-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.10.4 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ray[rllib] in /usr/local/lib/python3.10/site-packages (2.4.0)\n",
            "Requirement already satisfied: virtualenv<20.21.1,>=20.0.24 in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (20.21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (2.28.2)\n",
            "Requirement already satisfied: grpcio<=1.51.3,>=1.42.0 in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (1.51.3)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (1.24.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (23.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (1.3.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (23.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (4.17.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (8.1.3)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (1.3.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (1.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (3.12.0)\n",
            "Collecting scikit-image\n",
            "  Downloading scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium==0.26.3\n",
            "  Downloading Gymnasium-0.26.3-py3-none-any.whl (836 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m836.9/836.9 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (2.0.1)\n",
            "Collecting rich\n",
            "  Downloading rich-13.3.5-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m238.7/238.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer\n",
            "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-tree\n",
            "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (0.9.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (1.10.1)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (2.6)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/site-packages (from ray[rllib]) (4.3.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/site-packages (from gymnasium==0.26.3->ray[rllib]) (2.2.1)\n",
            "Collecting gymnasium-notices>=0.0.1\n",
            "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/site-packages (from virtualenv<20.21.1,>=20.0.24->ray[rllib]) (0.3.6)\n",
            "Requirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/site-packages (from virtualenv<20.21.1,>=20.0.24->ray[rllib]) (3.5.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/site-packages (from jsonschema->ray[rllib]) (0.19.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->ray[rllib]) (2023.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->ray[rllib]) (2023.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->ray[rllib]) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->ray[rllib]) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->ray[rllib]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->ray[rllib]) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->ray[rllib]) (3.4)\n",
            "Collecting markdown-it-py<3.0.0,>=2.2.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich->ray[rllib]) (2.15.1)\n",
            "Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.10/site-packages (from scikit-image->ray[rllib]) (9.5.0)\n",
            "Collecting tifffile>=2019.7.26\n",
            "  Downloading tifffile-2023.4.12-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m219.4/219.4 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imageio>=2.4.1\n",
            "  Downloading imageio-2.28.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lazy_loader>=0.1\n",
            "  Downloading lazy_loader-0.2-py3-none-any.whl (8.6 kB)\n",
            "Collecting PyWavelets>=1.1.1\n",
            "  Downloading PyWavelets-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/site-packages (from scikit-image->ray[rllib]) (3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from typer->ray[rllib]) (4.5.0)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->ray[rllib]) (1.16.0)\n",
            "Installing collected packages: gymnasium-notices, dm-tree, typer, tifffile, PyWavelets, mdurl, lazy_loader, imageio, gymnasium, scikit-image, markdown-it-py, rich\n",
            "Successfully installed PyWavelets-1.4.1 dm-tree-0.1.8 gymnasium-0.26.3 gymnasium-notices-0.0.1 imageio-2.28.1 lazy_loader-0.2 markdown-it-py-2.2.0 mdurl-0.1.2 rich-13.3.5 scikit-image-0.20.0 tifffile-2023.4.12 typer-0.9.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting plotly\n",
            "  Downloading plotly-5.14.1-py2.py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tenacity>=6.2.0\n",
            "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from plotly) (23.1)\n",
            "Installing collected packages: tenacity, plotly\n",
            "Successfully installed plotly-5.14.1 tenacity-8.2.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.0.6-py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m331.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting widgetsnbextension~=4.0.7\n",
            "  Downloading widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (8.13.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\n",
            "Collecting jupyterlab-widgets~=3.0.7\n",
            "  Downloading jupyterlab_widgets-3.0.7-py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m198.2/198.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipykernel>=4.5.1\n",
            "  Downloading ipykernel-6.22.0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m150.0/150.0 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-core!=5.0.*,>=4.12\n",
            "  Downloading jupyter_core-5.3.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzmq>=20\n",
            "  Downloading pyzmq-25.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Collecting comm>=0.1.1\n",
            "  Downloading comm-0.1.3-py3-none-any.whl (6.6 kB)\n",
            "Collecting tornado>=6.1\n",
            "  Downloading tornado-6.3.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m426.8/426.8 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting jupyter-client>=6.1.12\n",
            "  Downloading jupyter_client-8.2.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting debugpy>=1.6.5\n",
            "  Downloading debugpy-1.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (23.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.5.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
            "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
            "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
            "Installing collected packages: widgetsnbextension, tornado, pyzmq, nest-asyncio, jupyterlab-widgets, jupyter-core, debugpy, comm, jupyter-client, ipykernel, ipywidgets\n",
            "Successfully installed comm-0.1.3 debugpy-1.6.7 ipykernel-6.22.0 ipywidgets-8.0.6 jupyter-client-8.2.0 jupyter-core-5.3.0 jupyterlab-widgets-3.0.7 nest-asyncio-1.5.6 pyzmq-25.0.2 tornado-6.3.1 widgetsnbextension-4.0.7\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "Successfully installed kaleido-0.2.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==1.5.3\n",
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "9Zkmtvr-TFn_",
        "outputId": "5d145d08-719a-4818-e4b6-cfd184dd207e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.5.3\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/site-packages (from pandas==1.5.3) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas==1.5.3) (2023.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/site-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.1\n",
            "    Uninstalling pandas-2.0.1:\n",
            "      Successfully uninstalled pandas-2.0.1\n",
            "Successfully installed pandas-1.5.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/site-packages (0.26.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/site-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.10/site-packages (from gymnasium) (0.0.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/site-packages (from gymnasium) (1.24.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"../FinRL-Library\")"
      ],
      "metadata": {
        "id": "cMn1VNKCR0ld"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Import packages"
      ],
      "metadata": {
        "id": "BbOMJnGdTRYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# matplotlib.use('Agg')\n",
        "import datetime\n",
        "import optuna\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "%matplotlib inline\n",
        "from finrl import config\n",
        "from finrl import config_tickers\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv as StockTradingEnv_numpy\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from finrl.agents.rllib.models import DRLAgent as DRLAgent_rllib\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "import joblib\n",
        "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
        "import ray\n",
        "from pprint import pprint\n",
        "import kaleido\n",
        "\n",
        "\n",
        "\n",
        "import itertools\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(f'Torch device: {device}')"
      ],
      "metadata": {
        "id": "tG5qIkVHTNv8",
        "outputId": "1c1a81b3-999b-4857-9564-95af91ccd6d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_tickers.DOW_30_TICKER"
      ],
      "metadata": {
        "id": "DB11ygQLVLUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fMaPm8MsUo0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n",
        "    os.makedirs(\"./\" + config.DATA_SAVE_DIR)\n",
        "if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n",
        "    os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)\n",
        "if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n",
        "    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n",
        "if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
        "    os.makedirs(\"./\" + config.RESULTS_DIR)"
      ],
      "metadata": {
        "id": "FAZpwTH3VVi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collecting data and preprocessing"
      ],
      "metadata": {
        "id": "cqJ8ngFWVcNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#config_tickers.DOW_30_TICKER = [\"PYPL\"]"
      ],
      "metadata": {
        "id": "4M4UwcP4Ap_o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom ticker list dataframe download\n",
        "#TODO save df to avoid download\n",
        "path_pf = '/content/ticker_data.csv'\n",
        "if Path(path_pf).is_file():\n",
        "  print('Reading ticker data')\n",
        "  df = pd.read_csv(path_pf)\n",
        "  \n",
        "else:\n",
        "  print('Downloading ticker data')\n",
        "  ticker_list = config_tickers.DOW_30_TICKER\n",
        "  df = YahooDownloader(start_date = '2009-01-01',\n",
        "                     end_date = '2023-04-30',\n",
        "                     ticker_list = ticker_list).fetch_data()\n",
        "  df.to_csv('ticker_data.csv')"
      ],
      "metadata": {
        "id": "7BJNlsAuVZ-o",
        "outputId": "b66f77aa-71c0-430d-82ca-50f9f5cd2107",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ticker data\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Shape of DataFrame:  (105581, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_processed_full(processed):\n",
        "  list_ticker = processed[\"tic\"].unique().tolist()\n",
        "  list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n",
        "  combination = list(itertools.product(list_date,list_ticker))\n",
        "\n",
        "  processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\")\n",
        "  processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
        "  processed_full = processed_full.sort_values(['date','tic'])\n",
        "\n",
        "  processed_full = processed_full.fillna(0)\n",
        "  processed_full.sort_values(['date','tic'],ignore_index=True).head(5)\n",
        "\n",
        "  processed_full.to_csv('processed_full.csv')\n",
        "  return processed_full"
      ],
      "metadata": {
        "id": "RD6wwtGSVZVU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can add technical indicators and turbulence factor to dataframe\n",
        "#Just set the use_technical_indicator=True, use_vix=True and use_turbulence=True\n",
        "def create_techind():\n",
        "  fe = FeatureEngineer(\n",
        "                    use_technical_indicator=True,\n",
        "                    tech_indicator_list = config.INDICATORS,\n",
        "                    use_vix=True,\n",
        "                    use_turbulence=True,\n",
        "                    user_defined_feature = False)\n",
        "\n",
        "  processed = fe.preprocess_data(df)\n",
        "  return processed"
      ],
      "metadata": {
        "id": "UNqEALmbVoJo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load price and technical indicator data from file if available\n",
        "path_pf = '/content/processed_full.csv'\n",
        "if Path(path_pf).is_file():\n",
        "  print('Reading processed_full data')\n",
        "  processed_full = pd.read_csv(path_pf)\n",
        "\n",
        "else:\n",
        "  print('Creating processed_full file')\n",
        "  processed=create_techind()\n",
        "  processed_full=create_processed_full(processed)"
      ],
      "metadata": {
        "id": "Qr1Qd-lMVpzg",
        "outputId": "e1b8065e-4381-4a31-8c6b-1d258ed49f78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating processed_full file\n",
            "Successfully added technical indicators\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Shape of DataFrame:  (3604, 8)\n",
            "Successfully added vix\n",
            "Successfully added turbulence index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "date_col = \"date\"\n",
        "tic_col = \"tic\"\n",
        "\n",
        "init_train_trade_data = processed_full.sort_values([date_col, tic_col])\n",
        "\n",
        "init_train_trade_data = processed_full.fillna(0)\n",
        "\n",
        "init_train_data = data_split(\n",
        "    init_train_trade_data, '2020-01-01', '2020-05-01')\n",
        "init_trade_data = data_split(\n",
        "    init_train_trade_data, '2021-05-01','2021-10-01')\n",
        "\n",
        "print(f'Number of training samples: {len(init_train_data)}')\n",
        "print(f'Number of testing samples: {len(init_train_trade_data)}')"
      ],
      "metadata": {
        "id": "KFs4AjWIVr2s",
        "outputId": "55600d1c-ea7c-475d-caac-798b64799720",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 2407\n",
            "Number of testing samples: 104516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_trade_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "GI2qgPGFTerU",
        "outputId": "33e02491-802f-4077-ddf5-04723bb6a334"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date   tic        open        high         low       close  \\\n",
              "0  2021-05-03  AAPL  132.039993  134.070007  131.830002  130.963547   \n",
              "0  2021-05-03  AMGN  240.669998  247.020004  240.550003  230.412491   \n",
              "0  2021-05-03   AXP  154.589996  156.050003  154.009995  150.957855   \n",
              "0  2021-05-03    BA  234.110001  237.100006  233.809998  235.190002   \n",
              "0  2021-05-03   CAT  230.000000  230.929993  227.210007  218.290512   \n",
              "\n",
              "       volume  day      macd     boll_ub     boll_lb     rsi_30      cci_30  \\\n",
              "0  75135100.0  0.0  1.805506  135.539561  126.477478  53.856512   50.478426   \n",
              "0   3587700.0  0.0 -0.410712  248.223095  221.790974  50.842034  -81.617873   \n",
              "0   2726200.0  0.0  2.754005  151.027342  139.179352  62.209653  175.796203   \n",
              "0   9887800.0  0.0 -2.593339  260.290298  227.831702  49.445883 -102.331978   \n",
              "0   3182900.0  0.0  1.167007  223.005762  216.688293  56.081921  -17.678374   \n",
              "\n",
              "       dx_30  close_30_sma  close_60_sma        vix  turbulence  \n",
              "0  20.583396    127.527259    126.185906  18.309999   20.080451  \n",
              "0   5.785497    234.734409    226.585521  18.309999   20.080451  \n",
              "0  22.874057    142.745663    138.440772  18.309999   20.080451  \n",
              "0  17.827725    245.819000    237.101333  18.309999   20.080451  \n",
              "0   2.780977    218.974402    211.601994  18.309999   20.080451  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-111e660f-f571-4a5c-b084-4917f326279c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>tic</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>day</th>\n",
              "      <th>macd</th>\n",
              "      <th>boll_ub</th>\n",
              "      <th>boll_lb</th>\n",
              "      <th>rsi_30</th>\n",
              "      <th>cci_30</th>\n",
              "      <th>dx_30</th>\n",
              "      <th>close_30_sma</th>\n",
              "      <th>close_60_sma</th>\n",
              "      <th>vix</th>\n",
              "      <th>turbulence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-03</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>132.039993</td>\n",
              "      <td>134.070007</td>\n",
              "      <td>131.830002</td>\n",
              "      <td>130.963547</td>\n",
              "      <td>75135100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.805506</td>\n",
              "      <td>135.539561</td>\n",
              "      <td>126.477478</td>\n",
              "      <td>53.856512</td>\n",
              "      <td>50.478426</td>\n",
              "      <td>20.583396</td>\n",
              "      <td>127.527259</td>\n",
              "      <td>126.185906</td>\n",
              "      <td>18.309999</td>\n",
              "      <td>20.080451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-03</td>\n",
              "      <td>AMGN</td>\n",
              "      <td>240.669998</td>\n",
              "      <td>247.020004</td>\n",
              "      <td>240.550003</td>\n",
              "      <td>230.412491</td>\n",
              "      <td>3587700.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.410712</td>\n",
              "      <td>248.223095</td>\n",
              "      <td>221.790974</td>\n",
              "      <td>50.842034</td>\n",
              "      <td>-81.617873</td>\n",
              "      <td>5.785497</td>\n",
              "      <td>234.734409</td>\n",
              "      <td>226.585521</td>\n",
              "      <td>18.309999</td>\n",
              "      <td>20.080451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-03</td>\n",
              "      <td>AXP</td>\n",
              "      <td>154.589996</td>\n",
              "      <td>156.050003</td>\n",
              "      <td>154.009995</td>\n",
              "      <td>150.957855</td>\n",
              "      <td>2726200.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.754005</td>\n",
              "      <td>151.027342</td>\n",
              "      <td>139.179352</td>\n",
              "      <td>62.209653</td>\n",
              "      <td>175.796203</td>\n",
              "      <td>22.874057</td>\n",
              "      <td>142.745663</td>\n",
              "      <td>138.440772</td>\n",
              "      <td>18.309999</td>\n",
              "      <td>20.080451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-03</td>\n",
              "      <td>BA</td>\n",
              "      <td>234.110001</td>\n",
              "      <td>237.100006</td>\n",
              "      <td>233.809998</td>\n",
              "      <td>235.190002</td>\n",
              "      <td>9887800.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.593339</td>\n",
              "      <td>260.290298</td>\n",
              "      <td>227.831702</td>\n",
              "      <td>49.445883</td>\n",
              "      <td>-102.331978</td>\n",
              "      <td>17.827725</td>\n",
              "      <td>245.819000</td>\n",
              "      <td>237.101333</td>\n",
              "      <td>18.309999</td>\n",
              "      <td>20.080451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-03</td>\n",
              "      <td>CAT</td>\n",
              "      <td>230.000000</td>\n",
              "      <td>230.929993</td>\n",
              "      <td>227.210007</td>\n",
              "      <td>218.290512</td>\n",
              "      <td>3182900.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.167007</td>\n",
              "      <td>223.005762</td>\n",
              "      <td>216.688293</td>\n",
              "      <td>56.081921</td>\n",
              "      <td>-17.678374</td>\n",
              "      <td>2.780977</td>\n",
              "      <td>218.974402</td>\n",
              "      <td>211.601994</td>\n",
              "      <td>18.309999</td>\n",
              "      <td>20.080451</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-111e660f-f571-4a5c-b084-4917f326279c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-111e660f-f571-4a5c-b084-4917f326279c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-111e660f-f571-4a5c-b084-4917f326279c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Environment"
      ],
      "metadata": {
        "id": "L3VlkfBMW8e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Portfolio:\n",
        "    def __init__(self):\n",
        "        self.num_shares = 0\n",
        "        self.total_cost = 0.0\n",
        "        self.avg_cost = 0.0\n",
        "\n",
        "    def buy(self, num_shares, price):\n",
        "        self.total_cost += num_shares * price\n",
        "        self.num_shares += num_shares\n",
        "        self.avg_cost = self.total_cost / self.num_shares\n",
        "\n",
        "    def sell(self, num_shares):\n",
        "        self.total_cost -= self.avg_cost * num_shares\n",
        "        self.num_shares -= num_shares\n",
        "\n",
        "    def get_num_shares(self):\n",
        "        return self.num_shares\n",
        "\n",
        "    def get_total_cost(self):\n",
        "        return self.total_cost\n",
        "\n",
        "    def get_avg_cost(self):\n",
        "        return self.avg_cost\n",
        "\n",
        "# Example usage\n",
        "portfolio = Portfolio()\n",
        "\n",
        "# Buy 50 shares at $126.81\n",
        "portfolio.buy(50, 126.81)\n",
        "\n",
        "# Sell 25 shares\n",
        "portfolio.sell(25)\n",
        "\n",
        "# Buy 30 shares at $125.03\n",
        "portfolio.buy(30, 125.03)\n",
        "\n",
        "# Sell 10 shares\n",
        "portfolio.sell(10)\n",
        "\n",
        "\n",
        "print(\"Number of shares: \", portfolio.get_num_shares())\n",
        "print(\"Total cost: \", round(portfolio.get_total_cost(), 2))\n",
        "print(\"Average cost: \", round(portfolio.get_avg_cost(), 2))\n"
      ],
      "metadata": {
        "id": "_henJDi45WQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import copy\n",
        "from typing import List\n",
        "\n",
        "import gym\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.set_printoptions(linewidth=np.inf)\n",
        "import pandas as pd\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# from stable_baselines3.common.logger import Logger, KVWriter, CSVOutputFormat\n",
        "\n",
        "\n",
        "class GokuEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "\n",
        "    metadata = {\"render.modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        stock_dim: int,\n",
        "        hmax: int,\n",
        "        initial_amount: int,\n",
        "        num_stock_shares: list[int],\n",
        "        buy_cost_pct: list[float],\n",
        "        sell_cost_pct: list[float],\n",
        "        reward_scaling: float,\n",
        "        state_space: int,\n",
        "        action_space: int,\n",
        "        tech_indicator_list: list[str],\n",
        "        turbulence_threshold=None,\n",
        "        risk_indicator_col=\"turbulence\",\n",
        "        make_plots: bool = False,\n",
        "        print_verbosity=10,\n",
        "        day=0,\n",
        "        initial=True,\n",
        "        previous_state=[],\n",
        "        model_name=\"\",\n",
        "        mode=\"\",\n",
        "        iteration=\"\",\n",
        "    ):\n",
        "        self.day = day\n",
        "        self.df = df\n",
        "        self.stock_dim = stock_dim\n",
        "        self.hmax = hmax\n",
        "        self.num_stock_shares = num_stock_shares\n",
        "        self.initial_amount = initial_amount  # get the initial cash\n",
        "        self.buy_cost_pct = buy_cost_pct\n",
        "        self.sell_cost_pct = sell_cost_pct\n",
        "        self.reward_scaling = reward_scaling\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.tech_indicator_list = tech_indicator_list\n",
        "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.action_space,))\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(self.state_space,)\n",
        "        )\n",
        "        self.data = self.df.loc[self.day, :]\n",
        "        self.terminal = False\n",
        "        self.make_plots = make_plots\n",
        "        self.print_verbosity = print_verbosity\n",
        "        self.turbulence_threshold = turbulence_threshold\n",
        "        self.risk_indicator_col = risk_indicator_col\n",
        "        self.initial = initial\n",
        "        self.previous_state = previous_state\n",
        "        self.model_name = model_name\n",
        "        self.mode = mode\n",
        "        self.iteration = iteration\n",
        "        # initalize state\n",
        "        self.state = self._initiate_state()\n",
        "\n",
        "        # initialize reward\n",
        "        self.reward = 0\n",
        "        self.turbulence = 0\n",
        "        self.cost = 0\n",
        "        self.trades = 0\n",
        "        self.episode = 0\n",
        "        # memorize all the total balance change\n",
        "        self.asset_memory = [\n",
        "            self.initial_amount\n",
        "            + np.sum(\n",
        "                np.array(self.num_stock_shares)\n",
        "                * np.array(self.state[1 : 1 + self.stock_dim])\n",
        "            )\n",
        "        ]  # the initial total asset is calculated by cash + sum (num_share_stock_i * price_stock_i)\n",
        "        self.rewards_memory = []\n",
        "        self.actions_memory = []\n",
        "        self.state_memory = (\n",
        "            []\n",
        "        )  # we need sometimes to preserve the state in the middle of trading process\n",
        "        self.date_memory = [self._get_date()]\n",
        "        #         self.logger = Logger('results',[CSVOutputFormat])\n",
        "        # self.reset()\n",
        "        self._seed()\n",
        "        self.total_price = np.array([0.0] * 29)\n",
        "        self.avg_price = np.array([0.0] * 29)\n",
        "        self.total_stockss = np.array([0.0] * 29)\n",
        "        self.frame  = pd.DataFrame()\n",
        "        self.portfolios = [Portfolio() for _ in range(29)]\n",
        "\n",
        "    def _sell_stock(self, index, action):\n",
        "        def _do_sell_normal():\n",
        "            if (\n",
        "                self.state[index + 2 * self.stock_dim + 1] != True\n",
        "            ):  # check if the stock is able to sell, for simlicity we just add it in techical index\n",
        "                # if self.state[index + 1] > 0: # if we use price<0 to denote a stock is unable to trade in that day, the total asset calculation may be wrong for the price is unreasonable\n",
        "                # Sell only if the price is > 0 (no missing data in this particular date)\n",
        "                # perform sell action based on the sign of the action\n",
        "                if self.state[index + self.stock_dim + 1] > 0:\n",
        "                    # Sell only if current asset is > 0\n",
        "                    sell_num_shares = min(\n",
        "                        abs(action), self.state[index + self.stock_dim + 1]\n",
        "                    )\n",
        "                    sell_amount = (\n",
        "                        self.state[index + 1]\n",
        "                        * sell_num_shares\n",
        "                        * (1 - self.sell_cost_pct[index])\n",
        "                    )\n",
        "                    # update balance\n",
        "                    self.state[0] += sell_amount\n",
        "\n",
        "                    self.state[index + self.stock_dim + 1] -= sell_num_shares\n",
        "                    self.cost += (\n",
        "                        self.state[index + 1]\n",
        "                        * sell_num_shares\n",
        "                        * self.sell_cost_pct[index]\n",
        "                    )\n",
        "                    self.trades += 1\n",
        "                    #if sell_num_shares >0:\n",
        "                    #  print(\"stocks Sold sell_num_shares\", sell_num_shares)\n",
        "                else:\n",
        "                    sell_num_shares = 0\n",
        "            else:\n",
        "                sell_num_shares = 0\n",
        "            return sell_num_shares\n",
        "\n",
        "        # perform sell action based on the sign of the action\n",
        "        if self.turbulence_threshold is not None:\n",
        "            if self.turbulence >= self.turbulence_threshold:\n",
        "                if self.state[index + 1] > 0:\n",
        "                    # Sell only if the price is > 0 (no missing data in this particular date)\n",
        "                    # if turbulence goes over threshold, just clear out all positions\n",
        "                    if self.state[index + self.stock_dim + 1] > 0:\n",
        "                        # Sell only if current asset is > 0\n",
        "                        sell_num_shares = self.state[index + self.stock_dim + 1]\n",
        "                        sell_amount = (\n",
        "                            self.state[index + 1]\n",
        "                            * sell_num_shares\n",
        "                            * (1 - self.sell_cost_pct[index])\n",
        "                        )\n",
        "                        # update balance\n",
        "                        self.state[0] += sell_amount\n",
        "                        self.state[index + self.stock_dim + 1] = 0\n",
        "                        self.cost += (\n",
        "                            self.state[index + 1]\n",
        "                            * sell_num_shares\n",
        "                            * self.sell_cost_pct[index]\n",
        "                        )\n",
        "                        self.trades += 1\n",
        "                    else:\n",
        "                        sell_num_shares = 0\n",
        "                else:\n",
        "                    sell_num_shares = 0\n",
        "            else:\n",
        "                sell_num_shares = _do_sell_normal()\n",
        "        else:\n",
        "            sell_num_shares = _do_sell_normal()\n",
        "        #print(\"stocks Sold sell_num_shares\", sell_num_shares)\n",
        "        return sell_num_shares\n",
        "\n",
        "    def _buy_stock(self, index, action):\n",
        "        def _do_buy():\n",
        "            if (self.state[index + 2 * self.stock_dim + 1] != True):  # check if the stock is able to buy\n",
        "                # if self.state[index + 1] >0:\n",
        "                # Buy only if the price is > 0 (no missing data in this particular date)\n",
        "                available_amount = self.state[0] // (self.state[index + 1] * (1 + self.buy_cost_pct[index]))\n",
        "                # when buying stocks, we should consider the cost of trading when calculating available_amount, or we may be have cash<0\n",
        "                # print('available_amount:{}'.format(available_amount))\n",
        "                # update balance\n",
        "                buy_num_shares = min(available_amount, action)\n",
        "                buy_amount = (self.state[index + 1] * buy_num_shares * (1 + self.buy_cost_pct[index]))\n",
        "                self.state[0] -= buy_amount\n",
        "                self.state[index + self.stock_dim + 1] += buy_num_shares\n",
        "                self.cost += (self.state[index + 1] * buy_num_shares * self.buy_cost_pct[index])\n",
        "                self.trades += 1\n",
        "            else:\n",
        "                buy_num_shares = 0\n",
        "\n",
        "            return buy_num_shares\n",
        "\n",
        "        # perform buy action based on the sign of the action\n",
        "        if self.turbulence_threshold is None:\n",
        "            buy_num_shares = _do_buy()\n",
        "        else:\n",
        "            if self.turbulence < self.turbulence_threshold:\n",
        "                buy_num_shares = _do_buy()\n",
        "            else:\n",
        "                buy_num_shares = 0\n",
        "                pass\n",
        "\n",
        "        return buy_num_shares\n",
        "\n",
        "    def _make_plot(self):\n",
        "        plt.plot(self.asset_memory, \"r\")\n",
        "        plt.savefig(f\"results/account_value_trade_{self.episode}.png\")\n",
        "        plt.close()\n",
        "\n",
        "    def step(self, actions):\n",
        "        self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
        "        if self.terminal:\n",
        "            # print(f\"Episode: {self.episode}\")\n",
        "            if self.make_plots:\n",
        "                self._make_plot()\n",
        "            end_total_asset = self.state[0] + sum(np.array(self.state[1 : (self.stock_dim + 1)]) * np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)]))\n",
        "            df_total_value = pd.DataFrame(self.asset_memory)\n",
        "            tot_reward = (self.state[0] + \n",
        "                          sum(np.array(self.state[1 : (self.stock_dim + 1)]) \n",
        "                          * np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])) \n",
        "                          - self.asset_memory[0])  # initial_amount is only cash part of our initial asset\n",
        "            df_total_value.columns = [\"account_value\"]\n",
        "            df_total_value[\"date\"] = self.date_memory\n",
        "            df_total_value[\"daily_return\"] = df_total_value[\"account_value\"].pct_change(1)\n",
        "            if df_total_value[\"daily_return\"].std() != 0:\n",
        "                sharpe = ((252**0.5) \n",
        "                * df_total_value[\"daily_return\"].mean() \n",
        "                / df_total_value[\"daily_return\"].std())\n",
        "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
        "            df_rewards.columns = [\"account_rewards\"]\n",
        "            df_rewards[\"date\"] = self.date_memory[:-1]\n",
        "            \n",
        "            if self.episode % self.print_verbosity == 0:\n",
        "                print(f\"day: {self.day}, episode: {self.episode}\")\n",
        "                print(f\"begin_total_asset: {self.asset_memory[0]:0.2f}\")\n",
        "                print(f\"end_total_asset: {end_total_asset:0.2f}\")\n",
        "                print(f\"total_reward: {tot_reward:0.2f}\")\n",
        "                print(f\"total_cost: {self.cost:0.2f}\")\n",
        "                print(f\"total_trades: {self.trades}\")\n",
        "                if df_total_value[\"daily_return\"].std() != 0:\n",
        "                    print(f\"Sharpe: {sharpe:0.3f}\")\n",
        "                print(\"=================================\")\n",
        "\n",
        "            if (self.model_name != \"\") and (self.mode != \"\"):\n",
        "                df_actions = self.save_action_memory()\n",
        "                df_actions.to_csv(\"results/actions_{}_{}_{}.csv\".format(self.mode, self.model_name, self.iteration))\n",
        "                df_total_value.to_csv(\"results/account_value_{}_{}_{}.csv\".format(self.mode, self.model_name, self.iteration),index=False,)\n",
        "                df_rewards.to_csv(\"results/account_rewards_{}_{}_{}.csv\".format(self.mode, self.model_name, self.iteration),index=False,)\n",
        "                plt.plot(self.asset_memory, \"r\")\n",
        "                plt.savefig(\"results/account_value_{}_{}_{}.png\".format(self.mode, self.model_name, self.iteration))\n",
        "                plt.close()\n",
        "            return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "        else:\n",
        "            actions = actions * self.hmax  # actions initially is scaled between 0 to 1\n",
        "            actions = actions.astype(int)  # convert into integer because we can't by fraction of shares\n",
        "\n",
        "\n",
        "            if self.turbulence_threshold is not None:\n",
        "                if self.turbulence >= self.turbulence_threshold:\n",
        "                    actions = np.array([-self.hmax] * self.stock_dim)\n",
        "            \n",
        "            current_price = np.array(self.state[1 : (self.stock_dim + 1)])\n",
        "            #actions = np.where(((current_price > ( self.avg_price * 0.6  + self.avg_price))& (self.avg_price >0.0)), self.total_stockss*-1,actions  )\n",
        "            #actions = np.where(((current_price < (self.avg_price - self.avg_price * .2))& (self.avg_price >0)), self.total_stockss*-1,actions  )\n",
        "            \n",
        "            \n",
        "            # # Mandatory Selling Profits and Losses\n",
        "            for i in range(0,len(actions)):\n",
        "              # self.total_price\n",
        "              # self.total_stockss\n",
        "              # self.avg_price\n",
        "              # print(\"Current Price\")\n",
        "              # print(np.array(self.state[1 : (self.stock_dim + 1)])[i])\n",
        "              current_price  = np.array(self.state[1 : (self.stock_dim + 1)])[i]\n",
        "              avg_price = self.avg_price[i]\n",
        "              #print(avg_price)\n",
        "              if (current_price > ( avg_price * 0.6  + avg_price))  and (avg_price > 0.0):\n",
        "                actions[i] = self.total_stockss[i] * -1\n",
        "                #print(\"Updated actions\")\n",
        "                #self.total_price[i] = 0.0\n",
        "                print(round(current_price) , \"|\", \" avg_price\", avg_price, \"Profit\", current_price -  avg_price)\n",
        "              elif current_price < (avg_price - avg_price * 0.2)  and avg_price > 0.0:\n",
        "                actions[i] = self.total_stockss[i] * -1\n",
        "                print(round(current_price) , \"|\", \" avg_price\", avg_price,  \"loss\", current_price -  avg_price )\n",
        "                #self.total_price[i] = 0.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            begin_total_asset = self.state[0] + sum(np.array(self.state[1 : (self.stock_dim + 1)])* np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)]))\n",
        "            # print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
        "\n",
        "\n",
        "            argsort_actions = np.argsort(actions)\n",
        "            sell_index = argsort_actions[: np.where(actions < 0)[0].shape[0]]\n",
        "            buy_index = argsort_actions[::-1][: np.where(actions > 0)[0].shape[0]]\n",
        "\n",
        "            for index in sell_index:\n",
        "              actions[index] = self._sell_stock(index, actions[index]) * (-1)\n",
        "\n",
        "            for index in buy_index:\n",
        "                actions[index] = self._buy_stock(index, actions[index])\n",
        "\n",
        "\n",
        "            self.actions_memory.append(actions)\n",
        "\n",
        "            # state: s -> s+1\n",
        "            self.day += 1\n",
        "            self.data = self.df.loc[self.day, :]\n",
        "            if self.turbulence_threshold is not None:\n",
        "                if len(self.df.tic.unique()) == 1:\n",
        "                    self.turbulence = self.data[self.risk_indicator_col]\n",
        "                elif len(self.df.tic.unique()) > 1:\n",
        "                    self.turbulence = self.data[self.risk_indicator_col].values[0]\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "            #self.frame.loc[self.frame.shape[0]+1] = recent_price\n",
        "            recent_price = copy.deepcopy(self.state[1 : (self.stock_dim + 1)])\n",
        "            recent_buy_sell = copy.deepcopy(actions)\n",
        "\n",
        "            recent_buy_sell_price = np.where(recent_buy_sell != 0.0, recent_buy_sell * recent_price, 0.0)\n",
        "            #print(recent_price)\n",
        "            #print(recent_buy_sell)\n",
        "            #print(recent_buy_sell_price)\n",
        "            #self.frame.append(pd.DataFrame( recent_price))\n",
        "            #self.frame.loc[self.frame.shape[0]+1] = recent_buy_sell\n",
        "            #self.frame.loc[self.frame.shape[0]+1] = recent_buy_sell_price\n",
        "\n",
        "            \n",
        "            self.total_stockss = copy.deepcopy(recent_buy_sell) + copy.deepcopy(self.total_stockss)\n",
        "            self.total_price = copy.deepcopy(recent_buy_sell_price)+ copy.deepcopy(self.total_price)\n",
        "            def _def_find_avg(x):\n",
        "                index, val = x\n",
        "                recent_action, recent_price = val\n",
        "                if recent_action > 0:\n",
        "                  self.portfolios[index].buy(recent_action, recent_price)\n",
        "                elif recent_action < 0:\n",
        "                  self.portfolios[index].sell(abs(recent_action))\n",
        "                return round(self.portfolios[index].get_avg_cost(), 2)\n",
        "              \n",
        "              \n",
        "            indexed = enumerate(zip(recent_buy_sell, recent_price))\n",
        "            \n",
        "            self.avg_price = np.fromiter(map(_def_find_avg, indexed, ), dtype=float)\n",
        "            \n",
        "            print(self.avg_price )\n",
        "            # self.avg_price = np.divide(self.total_price,\n",
        "            #                            self.total_stockss,\n",
        "            #                            out=np.zeros_like(self.total_price),\n",
        "            #                            #where=((recent_buy_sell!=0.0)) &(self.total_stockss >0.0) )\n",
        "            #                            where=recent_buy_sell!=0.0)\n",
        "\n",
        "            self.avg_price = np.where(~np.isfinite(self.avg_price), 0.0, self.avg_price)\n",
        "            self.avg_price = np.where(self.avg_price<0.0, 0.0, self.avg_price)\n",
        "\n",
        "            self.avg_price = np.where(self.total_stockss <=0.0,0.0,self.avg_price)\n",
        "\n",
        "\n",
        "            new_df = pd.DataFrame([recent_price])\n",
        "            self.frame = pd.concat([self.frame, pd.DataFrame(new_df)],ignore_index=True,axis = 0)\n",
        "            \n",
        "            new_df = pd.DataFrame([recent_buy_sell_price])\n",
        "            self.frame = pd.concat([self.frame, pd.DataFrame(new_df)],ignore_index=True,axis = 0)\n",
        "            \n",
        "            new_df = pd.DataFrame([self.total_stockss])\n",
        "            self.frame = pd.concat([self.frame, pd.DataFrame(new_df)],ignore_index=True,axis = 0)\n",
        "\n",
        "            new_df = pd.DataFrame([actions])\n",
        "            self.frame = pd.concat([self.frame, pd.DataFrame(new_df)],ignore_index=True,axis = 0)\n",
        "\n",
        "            new_df = pd.DataFrame([self.total_price])\n",
        "            self.frame = pd.concat([self.frame, pd.DataFrame(new_df)],ignore_index=True,axis = 0)\n",
        "\n",
        "            new_df = pd.DataFrame([self.avg_price])\n",
        "            self.frame = pd.concat([self.frame, pd.DataFrame(new_df)],ignore_index=True,axis = 0)\n",
        "\n",
        "            self.frame.to_csv(\"test.csv\")\n",
        "            \n",
        "            \n",
        "            \n",
        "            # dx = np.where(((self.avg_price < 0.00)))\n",
        "            # print(dx)\n",
        "            #if len(dx) > 0:\n",
        "            #print(self.total_stockss)\n",
        "            #print(self.total_price )\n",
        "            #print(self.avg_price)\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # self.total_price = np.add(recent_buy_sell_price,\n",
        "            #                           self.total_price,\n",
        "            #                           out=np.zeros_like(self.total_price),\n",
        "            #                                where=(recent_buy_sell!=0.0  )\n",
        "            \n",
        "            # #self.total_price = np.where(self.total_price<0.0, 0.0, self.total_price)\n",
        "            # self.total_stockss = actions + self.total_stockss\n",
        "            \n",
        "            # self.avg_price = np.divide(self.total_price,\n",
        "            #                            self.total_stockss,\n",
        "            #                            out=np.zeros_like(self.total_price),\n",
        "            #                            where=recent_buy_sell!=0.0)\n",
        "          \n",
        "            # print(\"Total Stocks\")\n",
        "            # print(self.total_stockss)\n",
        "            # print(\"Price\")\n",
        "            # print(recent_price)\n",
        "\n",
        "            # print(\"Avg Price\")\n",
        "            # print(self.avg_price)\n",
        "            # print(\"Total Price\")\n",
        "            # print(self.total_price)\n",
        "\n",
        "\n",
        "            self.state = self._update_state()\n",
        "\n",
        "            end_total_asset = self.state[0] + sum(\n",
        "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
        "                * np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
        "            )\n",
        "            self.asset_memory.append(end_total_asset)\n",
        "            self.date_memory.append(self._get_date())\n",
        "            self.reward = end_total_asset - begin_total_asset\n",
        "            self.rewards_memory.append(self.reward)\n",
        "            self.reward = self.reward * self.reward_scaling\n",
        "            self.state_memory.append(\n",
        "                self.state\n",
        "            )  # add current state in state_recorder for each step\n",
        "\n",
        "        return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "    def reset(self):\n",
        "        # initiate state\n",
        "        self.state = self._initiate_state()\n",
        "\n",
        "        if self.initial:\n",
        "            self.asset_memory = [\n",
        "                self.initial_amount\n",
        "                + np.sum(\n",
        "                    np.array(self.num_stock_shares)\n",
        "                    * np.array(self.state[1 : 1 + self.stock_dim])\n",
        "                )\n",
        "            ]\n",
        "        else:\n",
        "            previous_total_asset = self.previous_state[0] + sum(\n",
        "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
        "                * np.array(\n",
        "                    self.previous_state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)]\n",
        "                )\n",
        "            )\n",
        "            self.asset_memory = [previous_total_asset]\n",
        "\n",
        "        self.day = 0\n",
        "        self.data = self.df.loc[self.day, :]\n",
        "        self.turbulence = 0\n",
        "        self.cost = 0\n",
        "        self.trades = 0\n",
        "        self.terminal = False\n",
        "        # self.iteration=self.iteration\n",
        "        self.rewards_memory = []\n",
        "        self.actions_memory = []\n",
        "        self.date_memory = [self._get_date()]\n",
        "        if self.total_price[5] > 0.0:\n",
        "          dd = pd.DataFrame(data = self.avg_price)\n",
        "          dd.to_csv(\"avg\")\n",
        "          dd = pd.DataFrame(data = self.total_price)\n",
        "          dd.to_csv(\"tp\")\n",
        "        \n",
        "        self.episode += 1\n",
        "        self.total_price = np.array([0.0] * 29)\n",
        "        self.avg_price = np.array([0.0] * 29)\n",
        "        self.total_stockss = np.array([0.0] * 29)\n",
        "        #print(\"Resteting Account\")\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode=\"human\", close=False):\n",
        "        return self.state\n",
        "\n",
        "    def _initiate_state(self):\n",
        "        if self.initial:\n",
        "            # For Initial State\n",
        "            if len(self.df.tic.unique()) > 1:\n",
        "                # for multiple stock\n",
        "                state = (\n",
        "                    [self.initial_amount]\n",
        "                    + self.data.close.values.tolist()\n",
        "                    + self.num_stock_shares\n",
        "                    + sum(\n",
        "                        (\n",
        "                            self.data[tech].values.tolist()\n",
        "                            for tech in self.tech_indicator_list\n",
        "                        ),\n",
        "                        [],\n",
        "                    )\n",
        "                )  # append initial stocks_share to initial state, instead of all zero\n",
        "            else:\n",
        "                # for single stock\n",
        "                state = (\n",
        "                    [self.initial_amount]\n",
        "                    + [self.data.close]\n",
        "                    + [0] * self.stock_dim\n",
        "                    + sum(([self.data[tech]] for tech in self.tech_indicator_list), [])\n",
        "                )\n",
        "        else:\n",
        "            # Using Previous State\n",
        "            if len(self.df.tic.unique()) > 1:\n",
        "                # for multiple stock\n",
        "                state = (\n",
        "                    [self.previous_state[0]]\n",
        "                    + self.data.close.values.tolist()\n",
        "                    + self.previous_state[\n",
        "                        (self.stock_dim + 1) : (self.stock_dim * 2 + 1)\n",
        "                    ]\n",
        "                    + sum(\n",
        "                        (\n",
        "                            self.data[tech].values.tolist()\n",
        "                            for tech in self.tech_indicator_list\n",
        "                        ),\n",
        "                        [],\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                # for single stock\n",
        "                state = (\n",
        "                    [self.previous_state[0]]\n",
        "                    + [self.data.close]\n",
        "                    + self.previous_state[\n",
        "                        (self.stock_dim + 1) : (self.stock_dim * 2 + 1)\n",
        "                    ]\n",
        "                    + sum(([self.data[tech]] for tech in self.tech_indicator_list), [])\n",
        "                )\n",
        "        return state\n",
        "\n",
        "    def _update_state(self):\n",
        "        if len(self.df.tic.unique()) > 1:\n",
        "            # for multiple stock\n",
        "            state = (\n",
        "                [self.state[0]]\n",
        "                + self.data.close.values.tolist()\n",
        "                + list(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
        "                + sum((self.data[tech].values.tolist() for tech in self.tech_indicator_list),[],))\n",
        "\n",
        "        else:\n",
        "            # for single stock\n",
        "            state = ([self.state[0]] + [self.data.close] \n",
        "                     + list(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
        "                     + sum(([self.data[tech]] for tech in self.tech_indicator_list), []))\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _get_date(self):\n",
        "        if len(self.df.tic.unique()) > 1:\n",
        "            date = self.data.date.unique()[0]\n",
        "        else:\n",
        "            date = self.data.date\n",
        "        return date\n",
        "\n",
        "\n",
        "    def save_asset_memory(self):\n",
        "        date_list = self.date_memory\n",
        "        asset_list = self.asset_memory\n",
        "        # print(len(date_list))\n",
        "        # print(len(asset_list))\n",
        "        df_account_value = pd.DataFrame({\"date\": date_list, \"account_value\": asset_list})\n",
        "        return df_account_value\n",
        "\n",
        "    def save_action_memory(self):\n",
        "        if len(self.df.tic.unique()) > 1:\n",
        "            # date and close price length must match actions length\n",
        "            date_list = self.date_memory[:-1]\n",
        "            df_date = pd.DataFrame(date_list)\n",
        "            df_date.columns = [\"date\"]\n",
        "\n",
        "            action_list = self.actions_memory\n",
        "            df_actions = pd.DataFrame(action_list)\n",
        "            df_actions.columns = self.data.tic.values\n",
        "            df_actions.index = df_date.date\n",
        "            # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
        "        else:\n",
        "            date_list = self.date_memory[:-1]\n",
        "            action_list = self.actions_memory\n",
        "            df_actions = pd.DataFrame({\"date\": date_list, \"actions\": action_list})\n",
        "        return df_actions\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def get_sb_env(self):\n",
        "        e = DummyVecEnv([lambda: self])\n",
        "        obs = e.reset()\n",
        "        return e, obs\n"
      ],
      "metadata": {
        "id": "4p02b2cx_dj5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k5e9VO3mBLhm"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_dimension = len(init_train_data.tic.unique())\n",
        "state_space = 1 + 2 * stock_dimension + len(config.INDICATORS) * stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension"
      ],
      "metadata": {
        "id": "rg7DTZbVV10n",
        "outputId": "3e4f54f7-4e8e-4f62-c0c8-5a66188d84dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock Dimension: 29, State Space: 291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the environment kwargs\n",
        "\n",
        "initial_amount = 500000\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": initial_amount,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": config.INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}"
      ],
      "metadata": {
        "id": "zODPmc5hV42J"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate the training gym compatible environment\n",
        "e_train_gym = GokuEnv(df = init_train_data, **env_kwargs)\n",
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "agent = DRLAgent(env = env_train)"
      ],
      "metadata": {
        "id": "7n2QYGP5V58p"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate the trading environment\n",
        "e_trade_gym = GokuEnv(df = init_trade_data, turbulence_threshold = None, **env_kwargs)"
      ],
      "metadata": {
        "id": "xoBM74XEX1oI"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trade performance code\n",
        "The following code calculates trade performance metrics, which are then used as an objective for optimizing hyperparameter values.\n",
        "\n",
        "There are several available metrics. In this tutorial, the default choice is the ratio of average value of winning to losing trades."
      ],
      "metadata": {
        "id": "Dq6zJ14ZYsTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Main method\n",
        "# Calculates Trade Performance for Objective\n",
        "# Called from objective method\n",
        "# Returns selected trade perf metric(s)\n",
        "# Requires actions and associated prices\n",
        "\n",
        "def calc_trade_perf_metric(df_actions, \n",
        "                           df_prices_trade,\n",
        "                           tp_metric,\n",
        "                           dbg=False):\n",
        "  \n",
        "    df_actions_p, df_prices_p, tics = prep_data(df_actions.copy(),\n",
        "                                                df_prices_trade.copy())\n",
        "    # actions predicted by trained model on trade data\n",
        "    df_actions_p.to_csv('df_actions.csv') \n",
        "\n",
        "    \n",
        "    # Confirms that actions, prices and tics are consistent\n",
        "    df_actions_s, df_prices_s, tics_prtfl = \\\n",
        "        sync_tickers(df_actions_p.copy(),df_prices_p.copy(),tics)\n",
        "    \n",
        "    # copy to ensure that tics from portfolio remains unchanged\n",
        "    tics = tics_prtfl.copy()\n",
        "    \n",
        "    # Analysis is performed on each portfolio ticker\n",
        "    perf_data= collect_performance_data(df_actions_s, df_prices_s, tics)\n",
        "    # profit/loss for each ticker\n",
        "    pnl_all = calc_pnl_all(perf_data, tics)\n",
        "    # values for trade performance metrics\n",
        "    perf_results = calc_trade_perf(pnl_all)\n",
        "    df = pd.DataFrame.from_dict(perf_results, orient='index')\n",
        "    \n",
        "    # calculate and return trade metric value as objective\n",
        "    m = calc_trade_metric(df,tp_metric)\n",
        "    print(f'Ratio Avg Win/Avg Loss: {m}')\n",
        "    k = str(len(tpm_hist)+1)\n",
        "    # save metric value\n",
        "    tpm_hist[k] = m\n",
        "    return m\n",
        "\n",
        "\n",
        "# Supporting methods\n",
        "def calc_trade_metric(df,metric='avgwl'):\n",
        "    '''# trades', '# wins', '# losses', 'wins total value', 'wins avg value',\n",
        "       'losses total value', 'losses avg value'''\n",
        "    # For this tutorial, the only metric available is the ratio of \n",
        "    #  average values of winning to losing trades. Others are in development.\n",
        "    \n",
        "    # some test cases produce no losing trades.\n",
        "    # The code below assigns a value as a multiple of the highest value during\n",
        "    # previous hp optimization runs. If the first run experiences no losses,\n",
        "    # a fixed value is assigned for the ratio\n",
        "    tpm_mult = 1.0\n",
        "    avgwl_no_losses = 25\n",
        "    if metric == 'avgwl':\n",
        "        if sum(df['# losses']) == 0:\n",
        "          try:\n",
        "            return max(tpm_hist.values())*tpm_mult\n",
        "          except ValueError:\n",
        "            return avgwl_no_losses\n",
        "        avg_w = sum(df['wins total value'])/sum(df['# wins'])\n",
        "        avg_l = sum(df['losses total value'])/sum(df['# losses'])\n",
        "        m = abs(avg_w/avg_l)\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def prep_data(df_actions,\n",
        "              df_prices_trade):\n",
        "    \n",
        "    df=df_prices_trade[['date','close','tic']]\n",
        "    df['Date'] = pd.to_datetime(df['date'])\n",
        "    df = df.set_index('Date')\n",
        "    # set indices on both df to datetime\n",
        "    idx = pd.to_datetime(df_actions.index, infer_datetime_format=True)\n",
        "    df_actions.index=idx\n",
        "    tics = np.unique(df.tic)\n",
        "    n_tics = len(tics)\n",
        "    print(f'Number of tickers: {n_tics}')\n",
        "    print(f'Tickers: {tics}')\n",
        "    dategr = df.groupby('tic')\n",
        "    p_d={t:dategr.get_group(t).loc[:,'close'] for t in tics}\n",
        "    df_prices = pd.DataFrame.from_dict(p_d)\n",
        "    df_prices.index = df_prices.index.normalize()\n",
        "    return df_actions, df_prices, tics\n",
        "\n",
        "\n",
        "# prepares for integrating action and price files\n",
        "def link_prices_actions(df_a,\n",
        "                        df_p):\n",
        "    cols_a = [t + '_a' for t in df_a.columns]\n",
        "    df_a.columns = cols_a\n",
        "    cols_p = [t + '_p' for t in df_p.columns]\n",
        "    df_p.columns = cols_p\n",
        "    return df_a, df_p\n",
        "\n",
        "\n",
        "def sync_tickers(df_actions,df_tickers_p,tickers):\n",
        "    # Some DOW30 components may not be included in portfolio\n",
        "    # passed tickers includes all DOW30 components\n",
        "    # actions and ticker files may have different length indices\n",
        "    if len(df_actions) != len(df_tickers_p):\n",
        "      msng_dates = set(df_actions.index)^set(df_tickers_p.index)\n",
        "      try:\n",
        "        #assumption is prices has one additional timestamp (row)\n",
        "        df_tickers_p.drop(msng_dates,inplace=True)\n",
        "      except:\n",
        "        df_actions.drop(msng_dates,inplace=True)\n",
        "    df_actions, df_tickers_p = link_prices_actions(df_actions,df_tickers_p)\n",
        "    # identify any DOW components not in portfolio\n",
        "    t_not_in_a = [t for t in tickers if t + '_a' not in list(df_actions.columns)]\n",
        "  \n",
        "    # remove t_not_in_a from df_tickers_p\n",
        "    drop_cols = [t + '_p' for t in t_not_in_a]\n",
        "    df_tickers_p.drop(columns=drop_cols,inplace=True)\n",
        "    \n",
        "    # Tickers in portfolio\n",
        "    tickers_prtfl = [c.split('_')[0] for c in df_actions.columns]\n",
        "    return df_actions,df_tickers_p, tickers_prtfl\n",
        "\n",
        "def collect_performance_data(dfa,dfp,tics, dbg=False):\n",
        "    \n",
        "    perf_data = {}\n",
        "    # In current version, files columns include secondary identifier\n",
        "    for t in tics:\n",
        "        # actions: purchase/sale of DOW equities\n",
        "        acts = dfa['_'.join([t,'a'])].values\n",
        "        # ticker prices\n",
        "        prices = dfp['_'.join([t,'p'])].values\n",
        "        # market value of purchases/sales\n",
        "        tvals_init = np.multiply(acts,prices)\n",
        "        d={'actions':acts, 'prices':prices,'init_values':tvals_init}\n",
        "        perf_data[t]=d\n",
        "\n",
        "    return perf_data\n",
        "\n",
        "\n",
        "def calc_pnl_all(perf_dict, tics_all):\n",
        "    # calculate profit/loss for each ticker\n",
        "    print(f'Calculating profit/loss for each ticker')\n",
        "    pnl_all = {}\n",
        "    for tic in tics_all:\n",
        "        pnl_t = []\n",
        "        tic_data = perf_dict[tic]\n",
        "        init_values = tic_data['init_values']\n",
        "        acts = tic_data['actions']\n",
        "        prices = tic_data['prices']\n",
        "        cs = np.cumsum(acts)\n",
        "        args_s = [i + 1 for i in range(len(cs) - 1) if cs[i + 1] < cs[i]]\n",
        "        # tic actions with no sales\n",
        "        if not args_s:\n",
        "            pnl = complete_calc_buyonly(acts, prices, init_values)\n",
        "            pnl_all[tic] = pnl\n",
        "            continue\n",
        "        # copy acts: acts_rev will be revised based on closing/reducing init positions\n",
        "        pnl_all = execute_position_sales(tic,acts,prices,args_s,pnl_all)\n",
        "\n",
        "    return pnl_all\n",
        "\n",
        "\n",
        "def complete_calc_buyonly(actions, prices, init_values):\n",
        "    # calculate final pnl for each ticker assuming no sales\n",
        "    fnl_price = prices[-1]\n",
        "    final_values = np.multiply(fnl_price, actions)\n",
        "    pnl = np.subtract(final_values, init_values)\n",
        "    return pnl\n",
        "\n",
        "\n",
        "def execute_position_sales(tic,acts,prices,args_s,pnl_all):\n",
        "  # calculate final pnl for each ticker with sales\n",
        "    pnl_t = []\n",
        "    acts_rev = acts.copy()\n",
        "    # location of sales transactions\n",
        "    for s in args_s:  # s is scaler\n",
        "        # price_s = [prices[s]]\n",
        "        act_s = [acts_rev[s]]\n",
        "        args_b = [i for i in range(s) if acts_rev[i] > 0]\n",
        "        prcs_init_trades = prices[args_b]\n",
        "        acts_init_trades = acts_rev[args_b]\n",
        "  \n",
        "        # update actions for sales\n",
        "        # reduce/eliminate init values through trades\n",
        "        # always start with earliest purchase that has not been closed through sale\n",
        "        # selectors for purchase and sales trades\n",
        "        # find earliest remaining purchase\n",
        "        arg_sel = min(args_b)\n",
        "        # sel_s = len(acts_trades) - 1\n",
        "\n",
        "        # closing part/all of earliest init trade not yet closed\n",
        "        # sales actions are negative\n",
        "        # in this test case, abs_val of init and sales share counts are same\n",
        "        # zero-out sales actions\n",
        "        # market value of sale\n",
        "        # max number of shares to be closed: may be less than # originally purchased\n",
        "        acts_shares = min(abs(act_s.pop()), acts_rev[arg_sel])\n",
        "\n",
        "        # mv of shares when purchased\n",
        "        mv_p = abs(acts_shares * prices[arg_sel])\n",
        "        # mv of sold shares\n",
        "        mv_s = abs(acts_shares * prices[s])\n",
        "\n",
        "        # calc pnl\n",
        "        pnl = mv_s - mv_p\n",
        "        # reduce init share count\n",
        "        # close all/part of init purchase\n",
        "        acts_rev[arg_sel] -= acts_shares\n",
        "        acts_rev[s] += acts_shares\n",
        "        # calculate pnl for trade\n",
        "        # value of associated purchase\n",
        "        \n",
        "        # find earliest non-zero positive act in acts_revs\n",
        "        pnl_t.append(pnl)\n",
        "    \n",
        "    pnl_op = calc_pnl_for_open_positions(acts_rev, prices)\n",
        "    #pnl_op is list\n",
        "    # add pnl_op results (if any) to pnl_t (both lists)\n",
        "    pnl_t.extend(pnl_op)\n",
        "    #print(f'Total pnl for {tic}: {np.sum(pnl_t)}')\n",
        "    pnl_all[tic] = np.array(pnl_t)\n",
        "    return pnl_all\n",
        "\n",
        "\n",
        "def calc_pnl_for_open_positions(acts,prices):\n",
        "    # identify any positive share values after accounting for sales\n",
        "    pnl = []\n",
        "    fp = prices[-1] # last price\n",
        "    open_pos_arg = np.argwhere(acts>0)\n",
        "    if len(open_pos_arg)==0:return pnl # no open positions\n",
        "\n",
        "    mkt_vals_open = np.multiply(acts[open_pos_arg], prices[open_pos_arg])\n",
        "    # mkt val at end of testing period\n",
        "    # treat as trades for purposes of calculating pnl at end of testing period\n",
        "    mkt_vals_final = np.multiply(fp, acts[open_pos_arg])\n",
        "    pnl_a = np.subtract(mkt_vals_final, mkt_vals_open)\n",
        "    #convert to list\n",
        "    pnl = [i[0] for i in pnl_a.tolist()]\n",
        "    #print(f'Market value of open positions at end of testing {pnl}')\n",
        "    return pnl\n",
        "\n",
        "\n",
        "def calc_trade_perf(pnl_d):\n",
        "    # calculate trade performance metrics\n",
        "    perf_results = {}\n",
        "    for t,pnl in pnl_d.items():\n",
        "        wins = pnl[pnl>0]  # total val\n",
        "        losses = pnl[pnl<0]\n",
        "        n_wins = len(wins)\n",
        "        n_losses = len(losses)\n",
        "        n_trades = n_wins + n_losses\n",
        "        wins_val = np.sum(wins)\n",
        "        losses_val = np.sum(losses)\n",
        "        wins_avg = 0 if n_wins==0 else np.mean(wins)\n",
        "        #print(f'{t} n_wins: {n_wins} n_losses: {n_losses}')\n",
        "        losses_avg = 0 if n_losses==0 else np.mean(losses)\n",
        "        d = {'# trades':n_trades,'# wins':n_wins,'# losses':n_losses,\n",
        "             'wins total value':wins_val, 'wins avg value':wins_avg,\n",
        "             'losses total value':losses_val, 'losses avg value':losses_avg,}\n",
        "        perf_results[t] = d\n",
        "    return perf_results"
      ],
      "metadata": {
        "id": "QydaFexDX5BQ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning hyperparameters using Optuna"
      ],
      "metadata": {
        "id": "c_AT1dsuZApj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_ddpg_params(trial:optuna.Trial):\n",
        "  # Size of the replay buffer\n",
        "  buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(1e5), int(1e6)])\n",
        "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256, 512])\n",
        "  \n",
        "  return {\"buffer_size\": buffer_size,\n",
        "          \"learning_rate\":learning_rate,\n",
        "          \"batch_size\":batch_size}"
      ],
      "metadata": {
        "id": "fKGkKHV7Y9AA"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Variables\n",
        "## Fixed\n",
        "tpm_hist = {}  # record tp metric values for trials\n",
        "tp_metric = 'avgwl'  # specified trade_param_metric: ratio avg value win/loss\n",
        "## Settable by User\n",
        "n_trials = 100  # number of HP optimization runs\n",
        "total_timesteps = 2000 # per HP optimization run\n",
        "## Logging callback params\n",
        "lc_threshold=1e-5\n",
        "lc_patience=15\n",
        "lc_trial_number=5"
      ],
      "metadata": {
        "id": "N6blMWpz-pR0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPTIONAL CODE FOR SAMPLING HYPERPARAMETERS\n",
        "\n",
        "Replace current call in function objective with\n",
        "\n",
        "hyperparameters = sample_ddpg_params_all(trial)"
      ],
      "metadata": {
        "id": "fHET-odKZShg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_ddpg_params_all(trial:optuna.Trial,\n",
        "                           # fixed values from previous study\n",
        "                           learning_rate=0.0103,\n",
        "                           batch_size=128,\n",
        "                           buffer_size=int(1e6)):\n",
        "\n",
        "    gamma = trial.suggest_categorical(\"gamma\", [0.94, 0.96, 0.98])\n",
        "    # Polyak coeff\n",
        "    tau = trial.suggest_categorical(\"tau\", [0.08, 0.1, 0.12])\n",
        "\n",
        "    train_freq = trial.suggest_categorical(\"train_freq\", [512,768,1024])\n",
        "    gradient_steps = train_freq\n",
        "    \n",
        "    noise_type = trial.suggest_categorical(\"noise_type\", [\"ornstein-uhlenbeck\", \"normal\", None])\n",
        "    noise_std = trial.suggest_categorical(\"noise_std\", [.1,.2,.3] )\n",
        "\n",
        "    # NOTE: Add \"verybig\" to net_arch when tuning HER (see TD3)\n",
        "    net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"big\"])\n",
        "    # activation_fn = trial.suggest_categorical('activation_fn', [nn.Tanh, nn.ReLU, nn.ELU, nn.LeakyReLU])\n",
        "\n",
        "    net_arch = {\n",
        "        \"small\": [64, 64],\n",
        "        \"medium\": [256, 256],\n",
        "        \"big\": [512, 512],\n",
        "    }[net_arch]\n",
        "  \n",
        "    hyperparams = {\n",
        "        \"batch_size\": batch_size,\n",
        "        \"buffer_size\": buffer_size,\n",
        "        \"gamma\": gamma,\n",
        "        \"gradient_steps\": gradient_steps,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"tau\": tau,\n",
        "        \"train_freq\": train_freq,\n",
        "        #\"noise_std\": noise_std,\n",
        "        #\"noise_type\": noise_type,\n",
        "        \n",
        "        \"policy_kwargs\": dict(net_arch=net_arch)\n",
        "    }\n",
        "    return hyperparams"
      ],
      "metadata": {
        "id": "htLdZHKTZKPJ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_ddpg_params_all(trial:optuna.Trial,\n",
        "                           # fixed values from previous study\n",
        "                           learning_rate=0.0103,\n",
        "                           batch_size=128,\n",
        "                           buffer_size=int(1e6)):\n",
        "\n",
        "    gamma = trial.suggest_categorical(\"gamma\", [0.90, 0.92, 0.94, 0.96, 0.98])\n",
        "    # Polyak coeff\n",
        "    tau = trial.suggest_categorical(\"tau\", [0.02,0.04,0.06,0.08, 0.1, 0.12])\n",
        "\n",
        "    train_freq = trial.suggest_categorical(\"train_freq\", [512,768,1024])\n",
        "    gradient_steps = train_freq\n",
        "    \n",
        "    noise_type = trial.suggest_categorical(\"noise_type\", [\"ornstein-uhlenbeck\", \"normal\", None])\n",
        "    noise_std = trial.suggest_categorical(\"noise_std\", [.1,.2,.3,.4,.5] )\n",
        "\n",
        "    # NOTE: Add \"verybig\" to net_arch when tuning HER (see TD3)\n",
        "    net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"big\"])\n",
        "    # activation_fn = trial.suggest_categorical('activation_fn', [nn.Tanh, nn.ReLU, nn.ELU, nn.LeakyReLU])\n",
        "\n",
        "    net_arch = {\n",
        "        \"small\": [32, 32],\n",
        "        \"medium\": [64, 64],\n",
        "        \"big\": [256, 256],\n",
        "    }[net_arch]\n",
        "  \n",
        "    hyperparams = {\n",
        "        \"batch_size\": batch_size,\n",
        "        \"buffer_size\": buffer_size,\n",
        "        \"gamma\": gamma,\n",
        "        \"gradient_steps\": gradient_steps,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"tau\": tau,\n",
        "        \"train_freq\": train_freq,\n",
        "        #\"noise_std\": noise_std,\n",
        "        #\"noise_type\": noise_type,\n",
        "        \n",
        "        \"policy_kwargs\": dict(net_arch=net_arch)\n",
        "    }\n",
        "    return hyperparams"
      ],
      "metadata": {
        "id": "dJYdh-E4ZQKP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callbacks\n",
        "\n",
        "\n",
        "1. The callback will terminate if the improvement margin is below certain point\n",
        "2. It will terminate after certain number of trial_number are reached, not before that\n",
        "3. It will hold its patience to reach the threshold"
      ],
      "metadata": {
        "id": "mWEoJMPLZY4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoggingCallback:\n",
        "    def __init__(self,threshold,trial_number,patience):\n",
        "      '''\n",
        "      threshold:int tolerance for increase in objective\n",
        "      trial_number: int Prune after minimum number of trials\n",
        "      patience: int patience for the threshold\n",
        "      '''\n",
        "      self.threshold = threshold\n",
        "      self.trial_number  = trial_number\n",
        "      self.patience = patience\n",
        "      print(f'Callback threshold {self.threshold}, \\\n",
        "            trial_number {self.trial_number}, \\\n",
        "            patience {self.patience}')\n",
        "      self.cb_list = [] #Trials list for which threshold is reached\n",
        "    def __call__(self,study:optuna.study, frozen_trial:optuna.Trial):\n",
        "      #Setting the best value in the current trial\n",
        "      study.set_user_attr(\"previous_best_value\", study.best_value)\n",
        "      \n",
        "      #Checking if the minimum number of trials have pass\n",
        "      if frozen_trial.number >self.trial_number:\n",
        "          previous_best_value = study.user_attrs.get(\"previous_best_value\",None)\n",
        "          #Checking if the previous and current objective values have the same sign\n",
        "          if previous_best_value * study.best_value >=0:\n",
        "              #Checking for the threshold condition\n",
        "              if abs(previous_best_value-study.best_value) < self.threshold: \n",
        "                  self.cb_list.append(frozen_trial.number)\n",
        "                  #If threshold is achieved for the patience amount of time\n",
        "                  if len(self.cb_list)>self.patience:\n",
        "                      print('The study stops now...')\n",
        "                      print('With number',frozen_trial.number ,'and value ',frozen_trial.value)\n",
        "                      print('The previous and current best values are {} and {} respectively'\n",
        "                              .format(previous_best_value, study.best_value))\n",
        "                      study.stop()"
      ],
      "metadata": {
        "id": "He7GTZZUZWP8"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the Sharpe ratio\n",
        "#This is our objective for tuning\n",
        "def calculate_sharpe(df):\n",
        "  df['daily_return'] = df['account_value'].pct_change(1)\n",
        "  if df['daily_return'].std() !=0:\n",
        "    sharpe = (252**0.5)*df['daily_return'].mean()/ \\\n",
        "          df['daily_return'].std()\n",
        "    return sharpe\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "s9orU-WplQ70"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import sys   \n",
        "\n",
        "os.makedirs(\"models\",exist_ok=True)\n",
        "\n",
        "def objective(trial:optuna.Trial):\n",
        "  #Trial will suggest a set of hyperparamters from the specified range\n",
        "\n",
        "  # Optional to optimize larger set of parameters\n",
        "  # hyperparameters = sample_ddpg_params_all(trial)\n",
        "  \n",
        "  # Optimize buffer size, batch size, learning rate\n",
        "  hyperparameters = sample_ddpg_params_all(trial)\n",
        "  print(f'Hyperparameters from objective: {hyperparameters.keys()}')\n",
        "  policy_kwargs = None  # default\n",
        "  if 'policy_kwargs' in hyperparameters.keys():\n",
        "    policy_kwargs = hyperparameters['policy_kwargs']\n",
        "    del hyperparameters['policy_kwargs']\n",
        "    print(f'Policy keyword arguments {policy_kwargs}')\n",
        "  model_ddpg = agent.get_model(\"ddpg\",\n",
        "                               policy_kwargs = policy_kwargs,\n",
        "                               model_kwargs = hyperparameters )\n",
        "  \n",
        "  #You can increase it for better comparison\n",
        "  trained_ddpg = agent.train_model(model=model_ddpg,\n",
        "                                   tb_log_name=\"ddpg\",\n",
        "                                   total_timesteps=total_timesteps)\n",
        "  trained_ddpg.save('models/ddpg_{}.pth'.format(trial.number))\n",
        "  clear_output(wait=True)\n",
        "  \n",
        "  #For the given hyperparamters, determine the account value in the trading period\n",
        "  df_account_value, df_actions = DRLAgent.DRL_prediction(\n",
        "    model=trained_ddpg, \n",
        "    environment = e_trade_gym)\n",
        " \n",
        "  # Calculate trade performance metric\n",
        "  # Currently ratio of average win and loss market values\n",
        "  #tpm = calc_trade_perf_metric(df_actions,init_trade_data,tp_metric)\n",
        "  tpm = calculate_sharpe(df_account_value)\n",
        "  return tpm\n",
        "\n",
        "#Create a study object and specify the direction as 'maximize'\n",
        "#As you want to maximize sharpe\n",
        "#Pruner stops not promising iterations\n",
        "#Use a pruner, else you will get error related to divergence of model\n",
        "#You can also use Multivariate samplere\n",
        "#sampler = optuna.samplers.TPESampler(multivarite=True,seed=42)\n",
        "sampler = optuna.samplers.TPESampler()\n",
        "\n",
        "study = optuna.create_study(study_name=\"ddpg_study\",direction='maximize',\n",
        "                            sampler = sampler, pruner=optuna.pruners.HyperbandPruner())\n",
        "\n",
        "logging_callback = LoggingCallback(threshold=lc_threshold,\n",
        "                                   patience=lc_patience,\n",
        "                                   trial_number=lc_trial_number)\n",
        "#You can increase the n_trials for a better search space scanning\n",
        "study.optimize(objective, n_trials=n_trials,catch=(ValueError,),callbacks=[logging_callback])"
      ],
      "metadata": {
        "id": "b6EiwiiIZpRU",
        "outputId": "6ec745ec-d82c-4762-eab8-183b7cc3907d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-03 10:32:13,317]\u001b[0m A new study created in memory with name: ddpg_study\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Callback threshold 1e-05,             trial_number 5,             patience 15\n",
            "Hyperparameters from objective: dict_keys(['batch_size', 'buffer_size', 'gamma', 'gradient_steps', 'learning_rate', 'tau', 'train_freq', 'policy_kwargs'])\n",
            "Policy keyword arguments {'net_arch': [256, 256]}\n",
            "{'batch_size': 128, 'buffer_size': 1000000, 'gamma': 0.98, 'gradient_steps': 768, 'learning_rate': 0.0103, 'tau': 0.12, 'train_freq': 768}\n",
            "Using cuda device\n",
            "[ 73.45   0.     0.   331.35 138.44 166.99   0.   104.47 148.2    0.   203.57   0.   110.23   0.   134.1    0.    49.68 186.25 159.25  79.03   0.     0.   113.21 127.46 279.19 187.01   0.    51.41   0.  ]\n",
            "[ 73.45 216.29 119.01 331.35 138.44 166.23   0.   104.28 147.37 214.98 203.35 167.48 110.16   0.   132.96 125.13  49.68 186.25 158.9   78.7    0.     0.   113.2  127.46 279.19 187.01  50.83  51.41 111.91]\n",
            "[ 73.45 216.62 119.01 331.77 138.44 166.23   0.   104.06 146.87 215.7  203.35 167.48 109.91  54.16 132.79 125.06  49.68 187.68 158.72  78.7    0.    98.77 112.96 127.15 278.64 187.01  50.74  51.41 111.91]\n",
            "[ 73.45 216.12 117.88 331.77 134.62 169.32   0.   104.06 146.87 216.89 203.34 166.31 109.83  53.64 132.79 123.38  49.68 187.68 158.4   78.13   0.    98.77 112.64 125.78 277.61 184.63  50.74  51.59 111.91]\n",
            "[ 74.14 216.12 118.61 330.72 135.33 171.     0.   104.06 146.87 217.58 203.34 166.36 109.83  53.64 132.79 123.38  49.1  191.   158.4   77.95   0.    98.67 112.64 125.78 278.38 185.93  50.74  49.05 110.45]\n",
            "[ 74.14 216.36 119.72 330.72 135.33 173.54   0.   104.06 146.04 217.58 203.34 166.36 110.48  53.64 133.09 123.7   49.4  191.   158.4   77.78   0.    98.67 112.64 125.78 278.78 187.12  50.74  48.58 110.6 ]\n",
            "[ 74.14 216.36 119.72 330.72 135.33 174.09  42.57 104.06 145.7  217.58 207.8  166.36 110.48  53.48 133.09 123.69  49.4  192.26 159.37  77.6  156.46  98.67 112.64 125.78 278.97 187.74  50.74  48.58 110.6 ]\n",
            "[ 76.93 216.36 119.72 329.54 135.33 175.88  42.94 100.37 145.61 222.94 207.8  166.36 110.78  53.62 133.09 123.85  49.4  192.22 159.37  77.6  156.46  99.09 112.64 126.31 278.97 187.74  50.3   48.58 110.38]\n",
            "[ 76.75 216.42 119.72 329.54 135.33 175.88  43.04 100.11 145.54 223.72 207.8  167.19 110.78  53.62 133.09 123.91  49.4  192.22 159.37  77.6  156.46  99.33 112.64 126.31 278.08 187.74  50.23  48.58 110.38]\n",
            "[ 76.54 216.42 121.53 329.54 133.87 175.88  43.04 100.04 145.38 223.72 207.8  167.19 110.78  53.27 133.09 123.85  51.23 192.22 159.37  77.6  156.46  99.33 112.64 127.31 279.32 187.74  50.24  48.58 110.26]\n",
            "[ 76.54 216.42 121.53 329.54 133.87 175.88  43.04 100.04 145.38 223.72 212.06 167.19 112.32  53.27 133.09 123.85  51.28 192.22 159.37  77.79 156.46  99.54 115.65 127.31 279.32 187.74  50.24  47.35 110.26]\n",
            "[ 76.58 216.42 121.84 327.18 133.87 175.88  43.04 100.04 145.38 224.85 213.29 167.19 112.32  53.85 133.09 124.07  51.32 196.63 159.37  77.79 162.05  99.54 115.65 128.19 279.32 187.74  50.24  47.35 110.26]\n",
            "[ 76.58 216.42 121.84 319.47 133.87 175.88  44.08 100.04 145.38 224.85 213.29 169.41 113.21  53.85 133.09 124.03  51.38 195.96 159.37  77.79 161.8   99.54 115.65 128.79 279.32 192.77  51.29  47.35 110.26]\n",
            "[ 76.88 216.42 121.84 319.47 133.87 175.88  44.08 100.04 145.15 224.85 213.29 169.41 113.21  55.66 133.09 124.03  51.38 195.96 159.37  77.79 161.8   99.54 115.65 129.44 279.32 192.77  51.29  47.35 110.21]\n",
            "[ 76.88 216.42 121.84 319.47 133.87 175.88  44.11 100.04 145.15 224.85 214.88 169.41 113.21  55.66 133.09 124.03  51.38 195.96 157.25  77.79 161.76  99.54 115.65 129.44 279.32 192.77  51.29  47.35 110.21]\n",
            "[ 76.88 216.42 129.05 319.47 133.87 175.88  44.11 100.04 144.38 224.85 214.88 165.86 113.21  55.66 133.09 124.03  51.38 195.96 157.25  77.79 161.13  99.54 115.65 129.44 279.32 192.77  51.26  47.35 110.21]\n",
            "[ 76.5  202.22 129.05 319.47 133.87 179.45  44.11 100.04 144.38 224.85 214.64 165.86 112.84  55.66 133.09 124.03  51.38 194.18 157.25  74.82 161.13  99.54 115.65 129.44 279.32 192.77  51.13  45.45 110.21]\n",
            "[ 76.78 202.4  129.05 319.47 133.87 179.45  43.31 100.04 144.38 224.85 214.64 165.86 112.84  55.7  133.09 124.03  51.38 194.18 157.25  74.82 160.88  99.54 115.65 129.44 279.32 195.23  51.13  45.45 110.21]\n",
            "[ 76.78 202.4  127.72 319.47 133.87 179.45  43.31 100.04 144.38 224.85 214.64 164.49 112.84  57.49 133.09 123.65  51.38 194.18 157.25  74.82 160.88  99.54 115.65 129.44 279.32 196.88  51.08  45.49 110.21]\n",
            "[ 77.15 202.4  127.72 321.39 133.87 179.45  43.3   95.85 144.38 225.2  214.98 164.49 112.84  58.1  133.09 123.65  51.38 194.18 157.25  74.78 160.88  99.54 115.65 129.44 279.32 198.04  51.08  45.49 110.21]\n",
            "[ 77.01 202.4  125.58 321.39 121.64 179.45  43.3   95.85 143.58 225.2  214.98 164.49 115.93  58.1  133.09 123.65  51.38 198.47 152.46  74.78 160.88  93.41 115.   129.44 279.32 197.58  51.06  44.98 108.69]\n",
            "[ 77.01 202.4  125.58 321.39 121.64 179.45  43.3   91.44 143.31 225.2  214.98 160.42 115.93  58.1  137.96 123.65  51.39 199.16 144.37  74.94 160.88  93.41 115.   127.22 279.32 197.58  51.06  44.98 108.69]\n",
            "[ 77.01 202.4  125.58 321.39 121.64 179.45  43.28  91.44 143.31 225.2  214.98 162.36 115.93  58.1  138.57 123.65  51.39 199.16 144.37  74.94 160.88  93.41 115.   127.22 279.32 197.94  51.06  44.98 108.69]\n",
            "[ 77.01 202.4  126.67 321.39 127.28 185.49  43.28  91.44 143.31 225.65 214.98 162.36 115.93  58.1  138.57 123.65  51.39 199.06 144.37  73.77 160.88  97.52 115.   127.22 280.16 197.94  51.06  44.98 108.69]\n",
            "[ 77.58 202.4  126.67 321.39 127.28 185.84  43.28  91.44 143.31 225.65 215.78 162.82 126.54  58.1  138.57 123.75  51.53 199.06 144.34  73.77 160.88  97.52 115.   127.15 280.16 197.94  51.06  44.98 108.69]\n",
            "[ 77.68 202.4  126.62 321.39 123.51 185.77  43.33  93.73 143.14 225.65 215.78 163.12 126.54  58.1  139.4  123.83  51.53 199.06 144.34  73.77 178.33  97.52 115.   127.15 280.16 197.95  51.02  46.33 110.55]\n",
            "[ 77.68 206.27 126.62 321.39 123.51 185.77  43.74  94.24 143.07 225.65 215.78 163.12 127.02  58.1  139.4  123.83  51.53 199.06 144.34  73.77 178.33  97.52 115.   126.49 280.16 197.95  51.11  46.33 110.55]\n",
            "[ 77.79 206.27 126.62 342.38 123.51 187.19  43.74  94.24 143.07 225.65 216.3  164.8  126.4   58.1  139.47 123.83  51.53 199.06 143.72  73.77 178.33  97.01 115.   126.76 277.56 198.33  51.11  46.11 110.55]\n",
            "[ 77.79 206.27 126.62 342.38 129.27 188.01  44.41  94.24 142.91 225.65 220.05 164.8  126.4   58.1  139.47 123.91  52.64 199.06 143.72  73.77 178.33  99.16 115.   126.76 286.09 199.07  51.11  46.11 110.55]\n",
            "[ 77.79 206.27 128.43 342.38 129.27 188.01  44.41  94.24 142.91 222.08 220.05 164.8  126.4   58.1  139.47 123.91  52.64 201.67 143.72  73.77 178.33  99.16 115.   126.76 286.09 199.07  51.11  46.11 110.55]\n",
            "[ 77.79 206.27 128.43 342.38 129.27 188.01  43.84  94.24 142.51 222.08 220.05 164.8  123.95  58.1  138.7  124.05  52.65 201.52 143.73  73.77 178.33  99.73 115.   126.76 286.09 199.07  49.75  46.11 110.55]\n",
            "[ 77.79 206.27 128.43 342.38 129.27 188.01  43.47  94.24 142.51 222.08 220.05 164.8  123.95  58.1  138.64 123.74  52.65 201.52 143.73  73.77 178.33  99.54 115.   126.76 287.49 200.06  49.75  46.11 113.57]\n",
            "[ 77.79 206.27 128.43 342.38 126.75 188.01  43.47  94.24 142.51 221.56 220.05 164.8  123.95  60.95 137.97 123.74  52.65 201.52 143.73  73.77 178.33  99.54 115.   126.76 287.49 200.06  49.75  45.71 113.57]\n",
            "[ 77.79 206.27 128.43 342.38 126.75 188.01  43.47  94.24 142.51 221.56 220.05 164.8  123.95  60.95 137.97 123.74  53.22 201.52 143.73  73.77 178.33  99.54 115.   125.37 287.49 200.48  49.75  45.71 113.57]\n",
            "[ 77.79 206.27 128.85 342.38 126.75 188.01  43.47  94.89 142.51 221.56 220.05 164.8  123.24  60.95 137.97 123.57  53.22 201.52 143.73  73.77 178.33  99.54 115.   125.37 287.49 200.48  49.75  45.43 113.57]\n",
            "[ 77.79 206.27 128.85 342.38 126.75 188.01  43.47  94.89 142.51 221.56 220.05 164.8  123.24  60.95 137.97 123.57  53.22 201.52 143.73  73.77 178.33  99.54 113.78 125.37 279.94 200.48  49.31  43.09 113.57]\n",
            "[ 76.31 192.17 128.85 342.38 119.47 188.01  43.31  94.89 141.12 221.56 220.05 162.68 123.24  60.95 137.14 123.57  53.22 201.52 143.73  73.77 178.33  99.54 113.33 125.37 279.94 197.72  49.19  42.48 113.57]\n",
            "[ 74.82 192.17 113.19 305.59 119.47 188.01  43.31  94.89 141.12 221.56 220.05 162.68 123.24  54.17 136.95 122.92  53.22 201.52 143.73  73.77 165.47  99.54 113.33 120.89 279.94 197.72  48.98  42.48 111.19]\n",
            "[ 74.82 192.17 113.19 305.59 114.46 188.01  43.31  81.93 140.37 221.56 220.05 162.68 123.24  54.17 136.95 122.92  53.22 201.52 134.05  73.77 154.74  99.54 113.33 119.72 279.94 194.87  48.24  42.01 111.19]\n",
            "[ 74.82 192.17 113.19 305.59 114.46 188.01  43.31  81.44 138.21 221.56 220.05 162.68 123.24  54.17 136.95 122.92  50.81 201.52 134.05  73.77 154.74  99.54 104.49 119.67 279.94 193.64  48.24  42.01 111.19]\n",
            "[ 74.82 192.17 113.19 305.59 114.46 188.01  43.31  81.44 138.21 221.56 220.05 162.68 123.24  54.17 136.95 122.92  50.81 189.06 134.05  73.77 154.74  96.47 104.49 119.67 279.94 193.64  48.24  42.01 110.01]\n",
            "[ 74.82 192.17 105.31 305.59 114.46 169.37  43.22  81.83 138.21 221.56 211.25 162.68 108.39  54.17 136.95 122.92  50.81 189.06 134.05  67.95 154.74  96.47 104.49 118.57 279.94 193.64  47.98  42.01 110.01]\n",
            "[ 74.82 196.49 105.31 305.59 114.46 169.37  43.22  82.94 138.21 221.56 217.67 162.68 108.39  54.17 135.9  122.92  50.81 189.06 134.05  67.95 154.74  96.47 104.49 118.57 279.94 193.64  47.98  42.04 110.01]\n",
            "[ 74.82 196.49 105.31 305.59 112.96 169.37  43.22  82.94 138.21 221.56 217.67 162.68 108.39  54.17 135.9  122.92  50.81 189.06 134.05  69.39 154.74  96.47 104.49 118.57 279.94 193.64  47.99  42.04 110.04]\n",
            "98 |  avg_price 122.92 loss -25.157037658691408\n",
            "[ 74.82 194.53 103.39 305.59 112.96 169.37  43.22  82.94 135.95 221.56 217.67 160.09 108.39  54.17 135.9  122.92  50.43 189.06 137.17  69.81 154.74  96.47 104.49 118.57 279.94 193.64  47.99  42.04 110.04]\n",
            "34 |  avg_price 43.22 loss -8.931174011230468\n",
            "104 |  avg_price 135.95 loss -31.600001525878895\n",
            "[ 65.25 194.53  94.36 305.59 112.96 169.37  43.22  81.25 135.95 161.4  213.5  160.09 108.39  46.18 135.9   84.52  49.41 174.41 137.17  69.61 154.74  96.47 107.1  115.32 261.   190.9   47.93  42.52 110.04]\n",
            "[ 65.25 194.53  94.36 305.59 112.42 169.37  43.22  81.25 135.95 166.85 213.5  160.09 108.39  47.28 135.32  84.52  49.34 174.41 137.08  69.61 156.48  96.47 108.9  115.32 266.69 190.9   47.94  42.79 110.92]\n",
            "[ 65.25 194.53  94.36 189.08 112.42 169.37  43.22  81.25 135.95 166.85 213.5  160.09 108.39  47.28 135.32  84.52  49.34 175.28 137.08  69.28 153.37  96.47 108.9  115.32 266.22 190.9   47.94  41.99 110.92]\n",
            "85 |  avg_price 112.42 loss -26.97780029296875\n",
            "127 |  avg_price 160.09 loss -33.139026489257816\n",
            "[ 65.25 194.53  94.36 189.08 112.42 140.59  29.99  68.56 135.95 166.85 213.5  160.09 108.39  47.28 135.32  79.64  49.34 168.95 130.74  68.29 153.37  96.47 108.9  112.81 263.31 190.9   47.91  40.39 110.92]\n",
            "[ 68.14 194.53  95.13 189.08  92.28 140.59  30.75  68.56 135.95 166.54 213.5  140.66 108.39  47.86 135.32  79.64  47.93 168.95 130.74  68.18 153.91  96.47 108.9  112.81 263.04 190.9   47.91  40.41 108.9 ]\n",
            "154 |  avg_price 213.5 loss -59.59832763671875\n",
            "79 |  avg_price 112.81 loss -33.40531402587891\n",
            "149 |  avg_price 190.9 loss -41.94557800292969\n",
            "[ 63.52 194.53  82.84 129.61  90.22 124.3   30.64  62.34 135.95 162.04 213.5  136.45  81.49  47.86 128.06  79.64  47.93 168.95 130.74  68.18 152.73  64.96 100.12 112.81 249.28 190.9   47.91  40.41 107.7 ]\n",
            "[ 63.29 194.53  82.74 129.61  90.22 124.3   30.64  62.34 135.95 160.67 213.5  134.63  84.89  47.86 128.06  79.64  46.81 168.95 130.74  68.18 152.73  65.61 105.23 112.81 249.28 190.9   47.69  40.51 107.7 ]\n",
            "102 |  avg_price 129.61 loss -27.720000610351576\n",
            "[ 63.29 194.53  77.3  129.61  91.26 126.69  30.64  47.92 135.95 154.37 145.2  134.63  84.89  47.86 128.06  76.82  46.81 128.15 122.32  68.18 152.73  65.61 105.23 112.81 249.28 145.5   47.54  40.51 107.7 ]\n",
            "[ 62.17 194.53  76.34 129.61  91.26 126.69  34.06  47.92  94.93 154.37 145.2  134.63  84.89  47.86 124.46  76.82  45.57 128.15 122.46  68.18 152.73  67.41 103.7   80.31 229.35 148.4   47.16  40.51 107.7 ]\n",
            "106 |  avg_price 134.63 loss -28.719851379394527\n",
            "35 |  avg_price 45.57 loss -10.662670440673828\n",
            "[ 60.27 171.96  76.34  95.01  88.83 126.69  32.7   50.5   94.93 154.37 141.95 134.63  84.89  47.86 124.46  75.85  45.57 134.24 122.46  68.18 152.73  67.41  98.24  80.78 229.35 148.4   47.16  40.51 107.7 ]\n",
            "[ 60.27 170.31  76.34  98.86  88.83 140.52  32.36  50.5   85.76 148.95 141.95  97.78  84.89  47.86 115.25  71.83  34.23 132.59 121.61  65.96 152.73  63.43  95.1   81.43 229.35 143.98  47.16  39.82 108.97]\n",
            "[ 60.27 172.73  76.89 101.4   88.83 150.83  32.89  50.5   94.67 147.93 160.27  97.78  85.95  47.86 115.25  71.83  34.23 136.46 120.6   65.15 152.73  63.43  95.1   83.23 229.35 143.98  46.76  39.82 108.97]\n",
            "[ 60.23 172.73  76.89 101.4   88.83 150.83  32.89  55.01  94.67 147.8  162.77 112.4   85.95  46.55 114.47  77.59  37.92 136.46 120.6   64.32 152.73  63.43  94.52  83.63 225.97 143.98  46.18  39.67 108.97]\n",
            "181 |  avg_price 101.4 Profit 79.1500030517578\n",
            "[ 61.07 172.73  76.89 101.4  102.33 151.18  32.89  55.01  94.67 148.41 162.77 112.4   85.95  46.55 114.47  78.72  37.92 136.46 121.06  64.32 152.73  63.43  95.17  86.02 234.64 143.98  46.18  39.95 105.8 ]\n",
            "[ 61.07 175.67  76.89 162.   102.33 151.18  33.6   55.01  94.67 148.34 162.77 112.4   85.95  47.3  114.47  78.72  38.65 141.08 121.06  64.23 152.73  80.95  96.47  86.02 234.48 158.31  46.12  39.95 105.75]\n",
            "[ 61.07 175.67  76.89 162.   102.33 151.05  33.6   55.01  94.67 148.34 176.4  124.03  85.95  48.87 114.47  78.72  38.65 141.08 121.06  64.23 155.81  82.15  96.47  86.02 236.59 158.31  46.12  39.95 107.27]\n",
            "[ 61.49 175.67  76.89 162.   105.48 151.05  33.6   55.01  94.67 148.34 176.4  124.94  91.24  48.87 114.47  78.72  38.65 141.08 121.06  64.23 155.1   82.15  96.47  86.02 236.59 158.31  46.12  39.95 107.27]\n",
            "[ 61.49 175.67  74.35 148.52 104.86 151.05  33.77  55.01  94.67 148.34 172.26 124.94  90.7   48.87 118.74  76.89  38.39 141.08 121.06  64.23 153.7   82.15  97.18  86.02 236.59 158.31  46.07  39.95 107.89]\n",
            "[ 61.49 175.67  73.88 148.52 104.86 151.05  33.77  66.26  94.67 148.34 172.26 124.94  90.7   48.87 118.74  76.89  39.34 141.08 121.06  64.23 152.78  82.15  98.76  86.02 234.54 158.31  46.07  39.95 109.23]\n",
            "[ 61.49 175.67  73.88 134.61 105.03 141.06  33.77  66.26  93.88 148.34 172.26 124.94  90.7   48.99 118.74  76.89  39.34 141.08 121.06  64.61 152.78  82.15  98.76  86.02 234.54 158.31  46.15  35.74 109.23]\n",
            "[ 62.05 175.67  77.53 141.37 105.03 141.06  34.56  66.26  93.88 148.34 172.26 125.13  94.44  50.13 129.21  76.89  39.34 165.24 121.06  64.61 152.78  82.31  98.85  86.02 235.12 158.31  46.15  35.74 111.22]\n",
            "[ 62.05 175.67  77.53 141.46 105.03 141.06  34.56  66.26  93.88 148.34 172.26 125.13  94.44  50.13 129.21  76.89  39.34 165.24 121.06  64.61 152.78  82.31  98.85  86.02 235.12 158.31  46.15  36.59 111.22]\n",
            "[ 62.05 175.67  77.53 141.46 111.22 141.06  34.56  66.26  93.88 148.34 172.26 125.13  94.44  50.13 129.21  76.89  39.34 165.24 121.06  64.61 152.78  82.31  98.85  86.02 235.12 158.31  46.15  36.59 111.22]\n",
            "[ 62.05 199.32  79.52 141.46 111.22 141.06  35.36  73.39  93.88 148.34 172.26 125.13  94.44  50.13 129.21  76.89  39.34 165.24 121.06  64.61 152.78  84.42  98.85  86.02 238.4  170.2   46.67  36.59 111.22]\n",
            "[ 65.5  199.32  82.26 143.2  111.22 141.06  35.99  73.39  93.88 148.34 172.26 125.13  94.44  50.13 129.22  76.89  39.34 165.24 121.06  64.67 152.78  84.42  98.85  96.25 241.38 170.2   46.67  38.72 111.22]\n",
            "[ 65.5  199.32  82.26 143.2  111.22 149.44  36.55  73.39 106.03 148.65 172.26 126.25 101.92  51.97 129.22  76.89  39.34 171.73 134.03  64.67 152.78  84.42 101.01  96.25 241.38 170.8   46.67  38.72 111.22]\n",
            "[ 65.5  199.32  82.26 143.71 111.22 149.44  36.55  73.39 104.56 148.65 172.26 126.25  98.52  52.56 136.52  76.89  39.34 171.73 134.03  64.67 152.78  84.42 101.01  96.25 241.38 167.54  46.67  38.67 111.22]\n",
            "[ 69.64 210.91  82.26 143.71 111.22 149.44  36.55  73.39 104.56 148.65 186.45 126.25  98.52  52.56 136.52  76.89  39.34 171.73 134.03  64.67 172.15  84.42 101.01  96.25 241.38 167.54  46.67  38.67 111.22]\n",
            "[ 69.64 212.8   82.26 143.71 109.3  149.44  36.72  73.39 104.56 148.65 190.8  127.03  98.58  53.   136.52  86.99  39.34 173.7  132.02  64.67 172.15  84.42 101.01  96.25 241.38 167.54  46.67  38.88 111.22]\n",
            "[ 68.65 212.8   82.26 143.7  108.53 156.29  36.72  73.39 104.56 150.69 190.8  127.03  98.69  53.04 140.23  86.99  42.41 169.67 132.02  64.67 171.18  84.42 103.06  95.47 254.9  167.54  47.71  38.88 113.29]\n",
            "[ 67.73 212.8   79.66 143.7  107.05 155.98  36.72  73.39 104.56 153.68 190.8  125.97  98.69  52.7  140.23  86.99  42.41 169.67 132.02  64.67 171.18  84.42 103.92  95.2  254.9  167.54  47.71  37.35 113.29]\n",
            "[ 67.73 212.8   79.66 140.65 107.05 155.98  36.72  73.39 104.56 153.68 190.8  125.97  98.4   52.7  140.23  81.65  42.41 169.67 132.02  64.67 169.73  84.42 105.07  95.2  258.27 167.54  47.71  37.77 113.29]\n",
            "[ 67.59 212.37  79.5  140.65 107.05 154.14  36.72  75.55 104.56 153.68 190.8  126.13  98.4   52.7  140.23  81.65  42.41 169.67 132.02  70.02 169.16  84.42 105.07  94.72 258.27 167.54  47.71  37.93 113.29]\n",
            "[ 67.59 213.32  79.5  140.65 106.81 154.14  36.72  75.55 104.56 153.68 190.8  126.13  98.4   52.7  140.23  81.65  42.41 169.67 132.02  70.02 169.33  85.95 105.07  94.72 258.27 167.54  47.71  37.93 113.29]\n",
            "[ 68.23 213.32  79.52 140.65 106.81 155.06  36.72  75.55 104.56 153.68 201.83 126.43  98.4   52.89 140.23  81.65  42.64 169.67 137.17  72.71 169.31  85.95 105.17  94.72 258.27 167.54  47.71  37.93 114.46]\n",
            "[ 68.23 213.32  79.52 140.65 106.81 154.94  36.72  75.55 104.56 157.8  202.16 126.43 100.22  53.06 139.97  87.09  42.64 173.54 138.59  72.71 168.88  85.95 105.17  94.72 269.76 167.81  49.71  39.19 116.23]\n",
            "[ 68.23 213.32  79.52 140.65 111.36 154.94  36.72  82.36 104.56 157.8  202.16 126.43 100.22  53.06 139.97  87.09  42.64 173.54 138.59  72.71 168.88  85.95 105.71  97.15 269.76 167.81  49.71  39.19 116.23]\n",
            "[ 68.23 213.32  83.69 140.65 110.37 154.94  36.72  82.36 108.15 157.8  203.5  126.43 100.22  53.06 139.17  87.09  41.82 173.54 138.28  69.35 168.88  85.95 105.71  96.71 269.76 167.81  49.38  39.19 116.16]\n",
            "[ 68.29 216.29  83.69 140.65 110.37 158.21  37.45  82.36 108.15 162.66 203.5  134.87 102.05  53.07 136.57 114.43  41.82 173.54 138.28  69.35 167.71  85.95 105.71 100.97 269.76 167.81  49.38  51.41 116.06]\n",
            "[ 68.29 216.29  99.39 271.62 117.86 158.21  37.45  82.36 137.13 162.66 203.5  141.87 103.41  53.37 136.57 118.11  41.82 173.54 138.28  75.99 165.31  85.95 106.53 108.67 274.37 167.81  49.38  51.56 116.06]\n",
            "[ 68.29 216.12 104.82 297.67 119.03 162.53  39.04  99.99 138.16 173.33 203.07 146.37 106.31  53.34 136.37 119.37  49.01 179.55 138.28  76.32 165.31  90.36 106.53 112.35 275.08 174.87  50.16  51.56 115.15]\n",
            "[ 68.29 216.12 104.82 304.31 125.08 163.23  39.04  99.99 138.16 181.4  203.07 146.37 106.36  53.34 135.72 120.89  49.01 179.55 138.28  76.24 165.31  94.72 107.15 112.35 275.08 174.87  50.2   50.66 114.37]\n",
            "[ 69.81 216.52 110.62 310.35 126.06 163.23  39.04  99.99 138.16 181.92 203.07 150.31 107.55  53.34 135.72 121.23  49.42 188.27 138.28  76.24 164.77  95.46 107.15 112.35 275.08 181.73  50.2   50.66 114.07]\n",
            "[ 69.81 216.52 110.62 312.13 126.06 163.23  39.69 100.09 144.62 181.92 206.15 151.87 107.55  53.34 135.72 121.23  49.42 188.27 148.18  76.47 164.77  96.23 107.41 112.35 277.43 186.31  50.2   50.66 113.8 ]\n",
            "[ 71.45 216.04 110.62 312.13 126.06 163.23  39.69 100.09 144.03 181.92 206.47 151.87 107.55  53.34 135.58 122.12  49.98 188.27 148.49  76.53 164.77  96.23 107.42 114.37 277.43 186.31  50.23  50.66 113.8 ]\n",
            "[ 72.43 216.35 111.34 312.13 126.06 163.23  40.38 100.09 144.49 181.92 206.48 155.   108.7   53.34 135.58 122.12  49.98 188.78 151.58  76.53 163.29  96.23 107.42 114.37 276.94 188.95  50.22  47.53 113.8 ]\n",
            "[ 72.43 216.35 111.34 312.13 126.06 163.23  40.99 100.09 144.49 187.51 206.71 155.73 108.7   53.34 135.5  122.59  50.35 188.82 151.7   76.53 162.57  96.59 108.05 114.37 276.94 188.95  50.22  47.44 113.16]\n",
            "[ 74.61 216.35 111.34 312.13 130.57 163.23  40.99 100.09 144.49 197.34 206.98 155.73 109.86  53.56 135.68 122.59  50.35 188.82 153.77  76.53 162.57  96.59 108.05 114.37 276.94 188.95  50.22  47.44 113.16]\n",
            "[ 74.61 217.82 111.34 314.7  130.57 163.23  41.    99.52 144.49 197.34 206.98 155.73 109.86  53.62 135.68 122.59  50.35 188.82 153.77  76.53 162.57  96.59 108.05 115.7  276.94 197.25  50.22  47.44 113.16]\n",
            "[ 74.61 217.82 111.34 314.6  130.57 163.23  41.    99.52 143.56 197.34 206.98 157.77 110.78  53.62 135.68 122.59  50.35 188.82 154.62  76.53 162.57  96.59 108.05 115.7  276.94 197.25  50.22  47.44 112.6 ]\n",
            "[ 74.61 217.82 111.34 314.6  130.57 163.23  41.    97.37 143.56 198.71 206.98 158.53 110.78  54.59 135.68 122.85  50.35 188.82 154.92  76.53 162.57  96.59 108.05 115.7  280.55 197.25  50.22  47.44 112.59]\n",
            "[ 74.61 217.82 111.34 314.92 130.57 163.23  41.    97.37 143.56 205.51 206.98 158.53 110.78  54.59 135.78 122.85  50.35 188.82 154.92  76.53 162.57  96.59 108.05 115.7  280.55 197.25  50.22  46.38 112.59]\n",
            "[ 74.61 217.14 111.34 314.92 130.57 164.    41.3   96.98 143.56 205.51 206.98 158.53 111.76  54.59 135.86 122.85  51.01 188.82 154.92  76.53 162.57  96.59 108.05 119.41 280.55 198.83  50.22  46.38 112.16]\n",
            "[ 74.96 217.14 112.99 314.92 130.57 164.    41.3   96.56 143.56 205.51 207.2  158.53 111.89  54.59 135.86 121.98  51.01 191.82 154.92  76.53 162.57  97.04 108.05 120.25 275.53 198.37  50.95  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 314.85 128.73 164.    41.86  96.56 138.37 209.59 207.2  158.53 111.89  54.59 136.32 121.98  51.23 191.82 154.92  76.53 162.57  97.04 108.05 120.25 275.53 198.37  50.95  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 169.7   41.86  96.56 138.37 209.59 207.2  159.69 111.93  56.83 136.32 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.78  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 136.75 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.78  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 136.75 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.78  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 136.75 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.78  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 136.75 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.78  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 136.75 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.78  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 136.79 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.78  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 136.79 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.78  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 136.79 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.78  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 136.79 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.78  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.09 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.57  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.09 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.57  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.09 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.57  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.57  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.57  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.57  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  75.12 162.65  97.04 108.05 120.25 275.53 198.37  50.57  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  74.51 162.65  97.04 108.05 120.25 275.53 198.37  50.57  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.86  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  74.51 162.65  97.04 108.05 120.25 275.53 198.37  50.57  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 315.97 128.73 173.57  41.85  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  74.51 162.65  97.04 108.05 120.25 275.53 198.37  48.91  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 314.22 128.73 173.57  41.85  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  74.51 162.65  97.04 108.05 120.25 275.53 198.37  47.72  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 309.7  128.73 173.57  41.85  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  74.51 162.65  97.04 108.05 120.25 275.53 198.37  47.11  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 309.7  128.73 173.57  41.85  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  74.51 162.65  97.04 108.05 120.25 275.53 198.37  47.11  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 309.7  128.73 173.57  41.85  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  74.51 162.65  97.04 108.05 120.25 275.53 198.37  47.11  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 309.7  128.73 173.57  41.85  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  74.51 162.65  97.04 108.05 120.25 275.53 198.37  47.11  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 309.7  128.73 173.57  41.85  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  74.51 162.65  97.04 108.05 120.25 275.53 198.37  47.11  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 309.7  128.73 173.57  41.85  96.56 138.37 209.59 207.2  159.69 111.85  56.83 137.08 121.98  51.23 191.82 154.92  74.51 162.65  97.04 108.05 120.25 275.53 198.37  47.11  45.8  112.16]\n",
            "94 |  avg_price 119.55 loss -25.66666107177734\n",
            "227 |  avg_price 309.7 loss -82.53000183105468\n",
            "85 |  avg_price 121.98 loss -37.459553222656254\n",
            "[ 74.96 217.14 119.55 309.7  128.73 169.21  40.12  73.42 104.35 197.09 204.04 157.37 109.64  53.69 135.71 121.98  50.1  190.13 154.92  71.61 159.97  86.13 108.05 120.25 275.53 198.37  47.21  45.8  112.16]\n",
            "[ 74.96 217.14 119.55 279.52 128.73 169.21  40.08  73.42 104.35 197.09 204.04 157.37 109.64  53.69 135.18  95.8   50.1  190.13 154.92  71.61 159.97  86.13 108.05 120.25 275.53 198.37  47.21  45.8  112.16]\n",
            "189 |  avg_price 279.52 loss -90.4399981689453\n",
            "[ 74.96 217.14 119.55 279.52 128.73 169.21  40.08  73.42 104.35 197.09 204.04 157.37 109.64  53.69 135.18  92.73  50.1  190.13 154.92  71.61 159.97  86.13 108.05 120.25 275.53 198.37  47.21  45.8  112.16]\n",
            "30 |  avg_price 40.08 loss -10.090828018188475\n",
            "141 |  avg_price 197.09 loss -56.35530456542969\n",
            "85 |  avg_price 109.64 loss -25.078316345214844\n",
            "41 |  avg_price 53.69 loss -12.330216827392576\n",
            "[ 74.96 217.14  98.95 193.53 128.73 164.53  40.08  73.42 100.2  197.09 204.04 157.37 109.64  53.69 133.25  88.04  50.1  190.13 154.92  71.61 156.46  86.03 108.05 120.25 275.53 198.37  46.44  45.8  112.16]\n",
            "[ 74.96 217.14  98.95 193.53 128.73 164.53  40.08  73.42 100.2  197.09 204.04 157.37 109.64  53.69 133.25  88.04  50.1  190.13 154.92  71.61 156.46  86.03 108.05 120.25 275.53 198.37  46.44  45.8  112.16]\n",
            "130 |  avg_price 193.53 loss -63.91999938964844\n",
            "124 |  avg_price 164.53 loss -40.22999694824219\n",
            "154 |  avg_price 204.04 loss -50.13832763671874\n",
            "139 |  avg_price 190.13 loss -51.04787719726562\n",
            "65 |  avg_price 86.03 loss -21.067261962890626\n",
            "[ 74.96 217.14  93.17 193.53 128.73 164.53  37.21  73.42  98.11 183.45 204.04 149.57  92.48  48.18 131.82  85.9   47.71 190.13 154.92  66.79 153.38  86.03 108.05 120.25 275.53 198.37  45.9   45.8  112.16]\n",
            "[ 74.96 217.14  90.51 145.68 120.29 164.53  37.21  73.42  98.11 183.45 204.04 149.57  92.48  48.18 131.36  85.9   46.7  190.13 154.92  66.79 153.38  86.03 108.05 120.25 275.53 198.37  45.9   45.8  112.16]\n",
            "70 |  avg_price 90.51 loss -20.066465148925786\n",
            "102 |  avg_price 145.68 loss -43.79000061035157\n",
            "93 |  avg_price 120.29 loss -27.568633117675788\n",
            "131 |  avg_price 183.45 loss -52.671710205078114\n",
            "113 |  avg_price 149.57 loss -36.96632415771484\n",
            "[ 74.96 217.14  90.51 145.68 120.29 154.18  37.21  73.42  98.11 183.45 204.04 149.57  89.71  46.18 131.36  85.9   46.69 190.13 154.92  64.99 151.52  71.64 108.05 120.25 275.53 198.37  45.95  45.8  112.16]\n",
            "[ 74.96 217.14  90.51 145.68 120.29 154.18  37.21  73.42  98.11 183.45 204.04 149.57  89.71  46.18 131.36  85.9   46.69 190.13 154.92  64.9  151.52  71.64 108.05 120.25 275.53 198.37  45.95  45.8  112.16]\n",
            "35 |  avg_price 46.69 loss -11.782670440673826\n",
            "[ 74.96 217.14  83.02 145.68 120.29 154.18  37.21  73.42  98.11 183.45 204.04 149.57  86.62  46.18 131.36  85.9   46.69 190.13 154.92  64.9  151.52  71.64 108.05 120.25 275.53 198.37  45.67  45.8  112.16]\n",
            "66 |  avg_price 83.02 loss -17.151713867187496\n",
            "103 |  avg_price 131.36 loss -28.606421813964857\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 131.36  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.3   45.8  112.16]\n",
            "143 |  avg_price 180.34 loss -36.87799438476563\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 74.96 217.14  83.02 118.05 120.29 150.86  37.21  73.42  94.56 180.34 204.04 133.97  84.77  45.85 125.64  82.89  34.23 150.43 130.44  62.55 149.6   67.21 108.05 120.25 275.53 198.37  45.29  45.8  112.16]\n",
            "[ 73.37 217.14  85.41 127.43 114.91 150.86  37.64  73.42  97.59 177.97 204.74 133.97  88.05  45.85 133.15  83.69  38.03 150.43 130.44  62.55 151.83  67.21 108.05 109.63 275.53 180.35  45.76  45.8  112.16]\n",
            "331 |  avg_price 127.43 Profit 203.36190063476562\n",
            "[ 73.15 217.14  96.88 127.43 121.79 150.86  38.89  73.42 106.52 185.68 204.   133.97  91.27  45.85 132.93  89.79  41.82 150.43 130.44  62.55 152.    67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 150.86  39.7   73.42 112.56 191.11 203.96 133.97  93.61  45.85 132.79  94.32  43.72 150.43 130.44  62.55 152.    67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 150.86  39.7   73.42 113.11 191.11 203.96 133.97  95.42  47.51 132.87  97.57  43.72 165.14 130.44  62.6  152.06  67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 150.86  39.71  73.42 117.36 191.11 203.96 141.49  95.42  47.52 132.92  97.57  44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.53 133.02 100.3   44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.05 100.3   44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.05 100.3   44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.05 100.3   44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.05 100.3   44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.05 100.3   44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.05 100.3   44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.05 100.3   44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.05 100.3   44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.05 100.3   44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.28  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.42 101.48  44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.73 101.48  44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.73 101.48  44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.73 101.48  44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.73 101.48  44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.73 101.48  44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.73 101.48  44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.73 101.48  44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  47.56 133.73 101.48  44.79 165.14 130.44  62.6  152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "339 |  avg_price 210.83 Profit 128.58061401367186\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  49.98 134.42 103.07  44.79 165.14 130.44  65.25 152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  49.98 134.42 103.07  44.79 165.14 130.44  65.25 152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  49.98 134.42 103.07  44.79 165.14 130.44  65.25 152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  49.98 134.42 103.07  44.79 165.14 130.44  65.25 152.26  67.21 108.05 114.69 275.53 182.61  46.29  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.51 103.07  44.79 165.14 130.44  65.25 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.51 103.07  44.79 165.14 130.44  65.25 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.51 103.07  44.79 165.14 130.44  65.25 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.72 103.33  44.79 165.14 130.44  65.25 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  65.25 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  65.25 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  65.25 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.04 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.23 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.23 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.23 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.23 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.23 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.23 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.23 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.23 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.23 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.23 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.23 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "[ 73.18 217.14 102.38 210.83 125.32 156.48  39.71  73.42 118.67 191.11 203.96 141.49  95.42  50.   134.78 103.33  44.79 165.14 130.44  66.23 152.26  67.21 108.05 114.69 275.53 182.61  46.3   45.8  112.16]\n",
            "80 |  avg_price 102.38 loss -22.59493530273437\n",
            "30 |  avg_price 39.71 loss -9.720828018188477\n",
            "92 |  avg_price 118.67 loss -26.86000244140625\n",
            "80 |  avg_price 103.33 loss -23.685018615722655\n",
            "[ 73.18 217.14 102.38 187.98 125.32 153.88  39.71  73.42 118.67 177.85 186.7  138.84  94.29  48.68 133.36 103.33  44.43 165.14 130.44  65.86 151.12  67.21 108.05 114.69 275.53 182.61  46.05  45.8  112.16]\n",
            "[ 73.18 217.14 102.38 187.98 125.32 153.88  39.69  73.42 118.67 177.85 186.7  138.84  94.29  48.74 133.36 103.33  44.43 165.14 130.44  65.86 151.12  67.21 108.05 114.69 275.53 182.61  46.05  45.8  112.16]\n",
            "130 |  avg_price 187.98 loss -58.36999938964843\n",
            "30 |  avg_price 39.69 loss -9.240155639648435\n",
            "[ 73.18 217.14  91.83 187.98 125.32 152.81  39.69  73.42 118.61 177.85 186.7  138.84  93.09  48.74 133.36 103.33  44.43 165.14 130.44  65.86 151.12  67.21 108.05 114.69 275.53 182.61  45.82  45.8  112.16]\n",
            "94 |  avg_price 118.61 loss -25.080001220703124\n",
            "[ 73.18 217.14  91.83 187.98 125.32 152.81  39.69  73.42 118.61 177.85 186.7  138.84  93.09  48.74 133.36 100.13  44.43 165.14 130.44  65.86 151.12  67.21 108.05 114.69 275.53 182.61  45.83  45.8  112.16]\n",
            "70 |  avg_price 91.83 loss -21.38646514892578\n",
            "131 |  avg_price 177.85 loss -47.07171020507812\n",
            "145 |  avg_price 186.7 loss -41.502886962890614\n",
            "76 |  avg_price 100.13 loss -24.247927551269527\n",
            "[ 73.18 217.14  91.83 152.84 125.32 150.01  39.69  73.42 118.61 177.85 186.7  138.84  92.41  48.74 133.36 100.13  44.43 165.14 130.44  65.86 151.12  66.9  108.05 114.69 275.53 182.61  45.86  45.8  112.16]\n",
            "98 |  avg_price 152.84 loss -55.13000091552735\n",
            "[ 73.18 217.14  91.83 152.84 125.32 150.01  39.69  73.42 118.61 177.85 186.7  138.84  92.41  48.74 133.36  96.16  44.43 165.14 130.44  65.86 151.12  66.9  108.05 114.69 275.53 182.61  45.85  45.8  112.16]\n",
            "76 |  avg_price 96.16 loss -20.630687866210934\n",
            "35 |  avg_price 44.43 loss -9.522670440673828\n",
            "[ 73.18 217.14  80.93 129.66 125.32 150.01  39.69  73.42 118.61 177.85 186.7  138.84  92.41  47.87 133.36  96.16  44.43 165.14 130.44  65.86 151.12  66.9  108.05 114.69 275.53 182.61  45.72  45.8  112.16]\n",
            "103 |  avg_price 133.36 loss -30.606421813964857\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.69  73.42 111.27 164.32 163.47 129.41  91.26  47.56 133.36  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.69  73.42 111.27 164.32 163.47 129.41  91.26  47.56 123.58  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.69  73.42 111.27 164.32 163.47 129.41  91.26  47.56 119.72  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.69  73.42 111.27 164.32 163.47 129.41  91.26  47.56 118.78  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.69  73.42 111.27 164.32 163.47 129.41  91.26  47.56 118.42  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.69  73.42 111.27 164.32 163.47 129.41  91.26  47.56 118.42  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.69  73.42 111.27 164.32 163.47 129.41  91.26  47.56 118.42  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.69  73.42 111.27 164.32 163.47 129.41  91.26  47.56 118.42  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.69  73.42 111.27 164.32 163.47 129.41  91.26  47.56 118.42  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.69  73.42 111.27 164.32 163.47 129.41  91.26  47.56 118.42  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.69  73.42 111.27 164.32 163.47 129.41  91.26  47.56 118.42  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.68  73.42 111.27 164.32 163.47 129.41  91.26  47.56 118.51  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.26  47.56 120.32  91.9   44.14 162.27 130.44  65.86 149.94  65.57 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.26  47.56 120.52  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.26  47.56 120.52  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.53  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.26  47.56 120.99  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.26  47.56 120.99  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.26  47.56 120.99  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.27  47.56 121.05  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.27  47.56 121.05  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.27  47.56 121.05  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.27  47.56 121.05  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.27  47.56 121.05  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.27  47.56 121.05  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.27  47.56 121.05  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.27  47.56 121.05  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 63.4  217.14  75.79 122.63 125.32 148.87  39.67  73.42 111.27 164.32 163.47 129.41  91.27  47.56 121.05  91.9   44.14 162.27 130.44  65.86 149.94  65.62 108.05 114.69 275.53 182.61  45.54  45.8  112.16]\n",
            "[ 68.06 217.14  78.81 126.79 117.59 148.87  39.36  73.42 110.7  165.91 180.12 129.41  92.15  47.56 124.83  91.26  42.98 162.27 130.44  65.86 151.37  65.62 108.05 106.36 275.53 176.81  45.77  45.8  112.16]\n",
            "331 |  avg_price 126.79 Profit 204.0019006347656\n",
            "[ 69.7  217.14  86.96 126.79 123.61 148.87  40.2   73.42 116.23 174.98 186.63 129.41  93.32  47.56 126.19  95.6   45.12 162.27 130.44  65.86 151.51  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 120.16 181.56 190.46 129.41  94.34  47.56 127.12  98.95  46.19 162.27 130.44  65.86 151.51  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "335 |  avg_price 173.17 Profit 162.11515625\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 123.17 181.56 190.46 136.36  94.94  48.16 127.91 101.4   46.19 171.85 130.44  68.46 151.58  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 141.13  94.94  48.16 128.52 103.48  46.19 171.85 130.44  68.46 151.6   65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  48.67 129.04 103.48  46.19 171.85 130.44  70.07 151.88  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  48.67 129.44 103.87  46.19 171.85 130.44  70.07 151.88  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  48.67 129.7  103.87  46.19 171.85 130.44  70.07 151.88  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  48.67 129.7  103.87  46.19 171.85 130.44  70.07 151.88  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  48.67 129.7  103.87  46.19 171.85 130.44  70.07 151.88  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  48.67 129.7  103.87  46.19 171.85 130.44  70.07 151.88  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  48.67 129.7  103.87  46.19 171.85 130.44  70.07 151.88  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  48.67 129.7  103.87  46.19 171.85 130.44  70.07 151.88  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  48.67 129.85 103.87  46.19 171.85 130.44  70.07 151.88  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  48.67 129.85 103.87  46.19 171.85 130.44  70.07 151.88  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  92.28 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  48.67 130.14 103.89  46.19 171.85 130.44  70.07 151.88  65.62 108.05 112.38 275.53 180.61  46.06  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.6  130.64 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.6  130.64 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.6  130.64 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.6  130.64 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 151.88  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  70.08 152.04  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  69.59 152.04  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  69.59 152.09  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  69.59 152.09  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  69.59 152.09  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  69.59 152.09  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  69.59 152.09  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  69.59 152.09  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  69.59 152.09  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  69.59 152.09  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "[ 70.34 217.14  94.24 173.17 126.71 148.87  40.76  73.42 125.52 181.56 190.46 144.05  94.94  49.61 130.68 103.89  46.19 171.85 130.44  69.59 152.09  65.62 108.05 112.38 275.53 180.61  46.33  45.8  112.16]\n",
            "30 |  avg_price 40.76 loss -10.770828018188475\n",
            "92 |  avg_price 125.52 loss -33.710002441406246\n",
            "80 |  avg_price 103.89 loss -24.245018615722657\n",
            "[ 66.41 217.14  92.26 169.02 126.71 147.99  40.76  73.42 125.52 172.31 190.46 144.05  94.34  48.96 130.68 103.89  45.36 171.85 130.44  68.8  151.31  65.62 108.05 112.38 275.53 180.61  46.18  45.8  112.16]\n",
            "[ 66.91 217.14  92.26 169.02 126.71 147.99  40.76  73.42 125.52 172.31 190.46 144.05  94.34  48.96 130.68 103.89  45.36 171.85 130.44  68.8  151.31  65.62 108.05 112.38 275.53 180.61  46.18  45.8  112.16]\n",
            "130 |  avg_price 169.02 loss -39.40999938964845\n",
            "[ 64.42 217.14  92.26 169.02 126.71 147.35  40.76  73.42 125.52 172.31 190.46 144.05  93.64  48.96 130.68 103.89  45.36 171.85 130.44  68.8  151.31  65.61 108.05 112.38 275.53 180.61  46.05  45.8  112.16]\n",
            "[ 64.23 217.14  92.26 169.02 126.71 147.35  40.76  73.42 125.52 172.31 190.46 144.05  93.64  48.96 130.68 103.89  45.36 171.85 130.44  68.8  151.31  65.61 108.05 112.38 275.53 180.61  46.05  45.8  112.16]\n",
            "70 |  avg_price 92.26 loss -21.816465148925786\n",
            "131 |  avg_price 172.31 loss -41.53171020507813\n",
            "113 |  avg_price 144.05 loss -31.446324157714855\n",
            "[ 63.35 217.14  92.26 153.83 126.71 145.92  40.76  73.42 118.82 172.31 190.46 144.05  93.64  48.96 130.68  99.76  45.36 171.85 130.44  68.79 151.04  65.72 108.05 112.38 275.53 180.61  46.05  45.8  112.16]\n",
            "98 |  avg_price 153.83 loss -56.120000915527356\n",
            "95 |  avg_price 118.82 loss -23.889999694824212\n",
            "77 |  avg_price 99.76 loss -22.602514038085943\n",
            "[ 63.35 217.14  87.52 153.83 126.71 145.37  40.76  73.42 118.82 172.31 190.46 144.05  93.57  48.96 130.68  99.76  45.36 171.85 130.44  68.79 151.04  65.72 108.05 112.38 275.53 180.61  46.03  45.8  112.16]\n",
            "35 |  avg_price 45.36 loss -10.452670440673828\n",
            "[ 63.35 217.14  87.52 153.83 126.71 145.37  40.76  73.42 118.82 172.31 190.46 144.05  93.57  48.95 130.68  99.76  45.36 171.85 130.44  68.79 150.38  65.72 108.05 112.38 275.53 180.61  46.03  45.8  112.16]\n",
            "66 |  avg_price 87.52 loss -21.651713867187496\n",
            "103 |  avg_price 130.68 loss -27.92642181396485\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.76  73.42 112.78 161.9  190.46 135.4   92.76  48.67 130.68  95.6   45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.9   45.8  112.16]\n",
            "36 |  avg_price 45.14 loss -9.18454803466797\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.76  73.42 112.78 161.9  190.46 135.4   92.76  48.67 128.05  95.6   45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.9   45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.76  73.42 112.78 161.9  190.46 135.4   92.76  48.67 124.77  95.6   45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.9   45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.76  73.42 112.78 161.9  190.46 135.4   92.76  48.67 123.51  95.6   45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.76  73.42 112.78 161.9  190.46 135.4   92.76  48.67 122.87  95.6   45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.76  73.42 112.78 161.9  190.46 135.4   92.76  48.67 122.87  95.6   45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.76  73.42 112.78 161.9  190.46 135.4   92.76  48.67 122.87  95.6   45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "76 |  avg_price 95.6 loss -19.29278717041015\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.76  73.42 112.78 161.9  190.46 135.4   92.76  48.67 122.48  95.6   45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.76  73.42 112.78 161.9  190.46 135.4   92.76  48.67 122.48  95.6   45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.76  73.42 112.78 161.9  190.46 135.4   92.76  48.67 122.48  95.6   45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.76  73.42 112.78 161.9  190.46 135.4   92.76  48.67 122.6   95.58  45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.75  73.42 112.78 161.9  190.46 135.4   92.76  48.67 123.02  95.58  45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.75  73.42 112.78 161.9  190.46 135.4   92.76  48.67 123.91  95.58  45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.75  73.42 112.78 161.9  190.46 135.4   92.76  48.67 124.49  95.58  45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.75  73.42 112.78 161.9  190.46 135.4   92.76  48.67 124.88  95.58  45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.74  73.42 112.78 161.9  190.46 135.4   92.76  48.67 125.5   95.58  45.14 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.74  73.42 112.78 161.9  190.46 135.4   92.76  48.67 125.51  95.58  45.12 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.74  73.42 112.78 161.9  190.46 135.4   92.76  48.67 125.51  95.58  45.12 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.74  73.42 112.78 161.9  190.46 135.4   92.76  48.67 125.51  95.58  45.12 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.74  73.42 112.78 161.9  190.46 135.4   92.76  48.67 125.51  95.58  45.12 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.74  73.42 112.78 161.9  190.46 135.4   92.76  48.67 125.51  95.58  45.12 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.74  73.42 112.78 161.9  190.46 135.4   92.76  48.67 125.51  95.58  45.12 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.74  73.42 112.78 161.9  190.46 135.4   92.76  48.67 125.51  95.58  45.12 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.74  73.42 112.78 161.9  190.46 135.4   92.76  48.67 125.51  95.58  45.12 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.74  73.42 112.78 161.9  190.46 135.4   92.76  48.67 125.51  95.58  45.12 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.74  73.42 112.78 161.9  190.46 135.4   92.76  48.67 125.51  95.58  45.12 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "[ 61.78 217.14  87.52 142.92 126.71 144.98  40.74  73.42 112.78 161.9  190.46 135.4   92.76  48.67 125.51  95.58  45.12 169.86 130.44  68.79 149.61  65.   108.05 112.38 275.53 180.61  45.89  45.8  112.16]\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 27          |\n",
            "|    time_elapsed    | 12          |\n",
            "|    total_timesteps | 332         |\n",
            "| train/             |             |\n",
            "|    reward          | -0.12571773 |\n",
            "------------------------------------\n",
            "[ 67.32 217.14  87.55 142.57 118.33 144.98  40.12  73.42 112.07 163.97 196.31 135.4   93.28  48.67 126.55  94.39  43.48 169.86 130.44  68.79 150.61  65.   108.05 105.   275.53 176.36  46.03  45.8  112.16]\n",
            "331 |  avg_price 142.57 Profit 188.22190063476563\n",
            "[ 69.22 217.14  93.93 142.57 124.12 144.98  40.78  73.42 116.67 173.18 198.19 135.4   94.03  48.67 126.99  98.33  45.45 169.86 130.44  68.79 150.73  65.   108.05 111.41 275.53 180.36  46.21  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 120.09 179.91 199.45 135.4   94.7   48.67 127.36 101.37  46.44 169.86 130.44  68.79 150.73  65.   108.05 111.41 275.53 180.36  46.21  45.8  112.16]\n",
            "335 |  avg_price 177.48 Profit 157.80515625\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 122.79 179.91 199.45 140.31  95.12  49.14 127.73 103.56  46.44 176.41 130.44  70.68 150.81  65.   108.05 111.41 275.53 180.36  46.21  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 143.89  95.12  49.14 128.06 105.44  46.44 176.41 130.44  70.68 150.83  65.   108.05 111.41 275.53 180.36  46.21  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  49.56 128.37 105.44  46.44 176.41 130.44  71.86 151.06  65.   108.05 111.41 275.53 180.36  46.21  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  49.56 128.63 105.8   46.44 176.41 130.44  71.86 151.06  65.   108.05 111.41 275.53 180.36  46.21  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  49.57 128.82 105.8   46.44 176.41 130.44  71.86 151.06  65.   108.05 111.41 275.53 180.36  46.21  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  49.57 128.82 105.8   46.44 176.41 130.44  71.86 151.06  65.   108.05 111.41 275.53 180.36  46.21  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  49.57 128.82 105.8   46.44 176.41 130.44  71.86 151.06  65.   108.05 111.41 275.53 180.36  46.21  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  49.57 128.82 105.8   46.44 176.41 130.44  71.86 151.06  65.   108.05 111.41 275.53 180.36  46.21  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  49.57 128.82 105.8   46.44 176.41 130.44  71.86 151.06  65.   108.05 111.41 275.53 180.36  46.21  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  49.57 128.82 105.8   46.44 176.41 130.44  71.86 151.06  65.   108.05 111.41 275.53 180.36  46.21  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  49.57 128.92 105.8   46.44 176.41 130.44  71.86 151.06  65.   108.05 111.41 275.53 180.36  46.22  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  49.57 128.92 105.8   46.44 176.41 130.44  71.86 151.06  65.   108.05 111.41 275.53 180.36  46.22  45.8  112.16]\n",
            "[ 69.95 217.14  98.07 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  49.57 129.12 105.81  46.44 176.41 130.44  71.86 151.06  65.   108.05 111.41 275.53 180.36  46.22  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.41 129.48 105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.41 129.48 105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.41 129.48 105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.41 129.48 105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.06  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.87 151.18  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.09 151.18  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.09 151.22  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.09 151.22  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.09 151.22  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.09 151.22  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.09 151.22  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "[ 69.95 217.14  99.68 177.48 127.09 144.98  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.43 129.5  105.81  46.44 176.41 130.44  71.09 151.22  65.   108.05 111.41 275.53 180.36  46.39  45.8  112.16]\n",
            "85 |  avg_price 105.81 loss -21.289553222656252\n",
            "[ 69.95 217.14  98.89 186.65 127.09 145.07  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.09 129.5  105.81  46.44 176.41 130.44  71.09 151.22  65.03 108.05 111.41 275.53 180.36  46.43  45.8  112.16]\n",
            "[ 69.95 217.14  98.89 186.65 127.09 145.07  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.09 129.5  105.81  46.44 176.41 130.44  71.09 151.22  65.03 108.05 111.41 275.53 180.36  46.43  45.8  112.16]\n",
            "[ 68.78 217.14  98.89 186.65 127.09 145.07  41.23  73.42 124.95 179.91 199.45 146.19  95.12  50.09 129.5  105.81  46.44 176.41 130.44  71.09 151.22  65.03 108.05 111.41 275.53 180.36  46.43  45.8  112.16]\n",
            "30 |  avg_price 41.23 loss -11.240828018188473\n",
            "92 |  avg_price 124.95 loss -33.14000244140625\n",
            "[ 68.78 217.14  98.89 186.65 127.09 145.07  41.23  73.42 124.95 171.28 190.8  146.19  95.12  49.45 129.5  101.96  46.44 176.41 130.44  70.93 150.68  65.99 108.05 111.41 275.53 180.36  46.43  45.8  112.16]\n",
            "[ 68.78 217.14  98.89 186.65 127.09 145.07  41.23  73.42 124.95 171.28 190.8  146.19  95.12  49.45 129.5  101.96  46.44 176.41 130.44  70.93 150.68  65.99 108.05 111.41 275.53 180.36  46.43  45.8  112.16]\n",
            "130 |  avg_price 186.65 loss -57.03999938964844\n",
            "80 |  avg_price 101.96 loss -22.03463073730468\n",
            "[ 65.21 217.14  98.89 186.65 127.09 144.56  41.23  73.42 124.95 171.28 190.8  146.19  94.55  49.45 129.5  101.96  46.44 176.41 130.44  70.93 150.68  65.99 108.05 111.41 275.53 180.36  46.32  45.8  112.16]\n",
            "[ 64.51 217.14  98.89 186.65 127.09 144.56  41.23  73.42 124.95 171.28 190.8  146.19  94.55  49.45 129.5  101.96  46.44 176.41 130.44  70.93 150.68  65.99 108.05 111.41 275.53 180.36  46.32  45.8  112.16]\n",
            "70 |  avg_price 98.89 loss -28.446465148925782\n",
            "131 |  avg_price 171.28 loss -40.501710205078126\n",
            "145 |  avg_price 190.8 loss -45.602886962890636\n",
            "113 |  avg_price 146.19 loss -33.58632415771484\n",
            "[ 63.59 217.14  98.89 171.01 127.09 143.68  41.14  73.42 119.37 171.28 190.8  146.19  94.18  49.02 129.5   98.13  46.44 170.62 130.44  69.72 150.21  66.01 108.05 111.41 275.53 180.36  46.32  45.8  112.16]\n",
            "98 |  avg_price 171.01 loss -73.30000091552733\n",
            "95 |  avg_price 119.37 loss -24.439999694824223\n",
            "77 |  avg_price 98.13 loss -20.972514038085933\n",
            "[ 63.59 217.14  92.46 171.01 127.09 143.4   41.14  73.42 119.37 171.28 190.8  146.19  94.18  49.02 129.5   98.13  46.44 170.62 130.44  69.72 150.21  66.05 108.05 111.41 275.53 180.36  46.29  45.8  112.16]\n",
            "71 |  avg_price 92.46 loss -21.66304107666015\n",
            "32 |  avg_price 41.14 loss -8.982937469482422\n",
            "35 |  avg_price 46.44 loss -11.532670440673826\n",
            "[ 63.59 217.14  92.46 164.13 127.09 143.4   41.14  73.42 119.37 171.28 190.8  146.19  94.18  49.02 129.5   98.13  46.44 170.62 130.44  69.72 150.21  66.05 108.05 111.41 275.53 180.36  46.23  45.8  112.16]\n",
            "106 |  avg_price 164.13 loss -58.509997253417964\n",
            "103 |  avg_price 129.5 loss -26.746421813964844\n",
            "128 |  avg_price 170.62 loss -42.654370422363286\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.14  73.42 114.18 171.28 190.8  138.57  93.56  48.77 129.5   94.21  46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.14  73.42 114.18 171.28 190.8  138.57  93.56  48.77 128.66  94.21  46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.14  73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.9   94.21  46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.14  73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.47  94.2   46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.11  73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.08  94.2   46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.11  73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.08  94.2   46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.11  73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.08  94.2   46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.11  73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.08  94.2   46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.11  73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.08  94.2   46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.11  73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.08  94.2   46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.11  73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.08  94.2   46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.11  73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.08  94.2   46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.11  73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.08  94.2   46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.1   73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.16  94.2   46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.1   73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.18  94.19  46.44 170.62 125.03  69.72 149.64  65.53 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.1   73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.5   94.19  46.44 170.62 125.03  69.72 149.64  65.55 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.1   73.42 114.18 171.28 190.8  138.57  93.56  48.77 127.82  94.19  46.44 170.62 125.03  69.72 149.64  65.55 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.1   73.42 114.18 171.28 190.8  138.57  93.56  48.77 128.15  94.19  46.44 170.62 125.03  69.72 149.64  65.55 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.1   73.42 114.18 171.28 190.8  138.57  93.56  48.77 128.78  94.19  46.44 170.62 125.03  69.72 149.64  65.55 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.1   73.42 114.18 171.28 190.8  138.57  93.56  48.77 129.31  94.19  46.44 170.62 125.03  69.72 149.64  65.55 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.08  73.42 114.18 171.28 190.8  138.57  93.56  48.77 129.66  94.19  46.44 170.62 125.03  69.72 149.64  65.55 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.07  73.42 114.18 171.28 190.8  138.57  93.56  48.77 130.01  94.19  46.44 170.62 125.03  69.72 149.64  65.58 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.07  73.42 114.18 171.28 190.8  138.57  93.56  48.77 130.01  94.19  46.44 170.62 125.03  69.72 149.64  65.58 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.07  73.42 114.18 171.28 190.8  138.57  93.56  48.77 130.01  94.19  46.44 170.62 125.03  69.72 149.64  65.58 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.07  73.42 114.18 171.28 190.8  138.57  93.56  48.77 130.01  94.19  46.44 170.62 125.03  69.72 149.64  65.58 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.07  73.42 114.18 171.28 190.8  138.57  93.56  48.77 130.01  94.19  46.44 170.62 125.03  69.72 149.64  65.58 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 62.   217.14  85.69 164.13 127.09 143.23  41.07  73.42 114.18 171.28 190.8  138.57  93.56  48.77 130.01  94.19  46.44 170.62 125.03  69.72 149.64  65.58 108.05 111.41 275.53 180.36  46.12  45.8  112.16]\n",
            "[ 67.42 217.14  86.09 159.87 118.53 143.23  40.35  73.42 113.38 171.29 196.51 138.57  93.91  48.77 130.45  93.34  44.14 170.62 125.03  69.72 150.4   65.58 108.05 104.43 275.53 176.3   46.22  45.8  112.16]\n",
            "331 |  avg_price 159.87 Profit 170.92190063476562\n",
            "[ 69.28 217.14  91.64 159.87 124.26 143.23  40.96  73.42 117.28 179.17 198.34 138.57  94.46  48.77 130.55  96.95  45.89 170.62 125.03  69.72 150.5   65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 120.28 184.99 199.56 138.57  94.97  48.77 130.64  99.81  46.77 170.62 125.03  69.72 150.5   65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "335 |  avg_price 191.58 Profit 143.70515625\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 122.7  184.99 199.56 142.37  95.29  49.23 130.75 101.94  46.77 176.9  125.03  71.38 150.57  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 145.28  95.29  49.23 130.85 103.8   46.77 176.9  125.03  71.38 150.59  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  49.65 130.96 103.8   46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  49.65 131.05 104.15  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  49.65 131.13 104.15  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  49.65 131.13 104.15  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  49.65 131.13 104.15  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  49.65 131.13 104.15  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  49.65 131.13 104.15  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  49.65 131.13 104.15  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  49.65 131.18 104.15  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  49.65 131.18 104.15  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  95.52 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  49.65 131.28 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.35  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.49 131.47 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.49 131.47 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.49 131.47 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.49 131.47 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.77  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  72.43 150.87  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  71.57 150.87  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  71.57 150.91  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  71.57 150.91  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  71.57 150.91  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  71.57 150.91  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  71.57 150.91  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  71.57 150.91  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  71.57 150.91  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  71.57 150.91  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "[ 70.   217.14  97.04 191.58 127.19 143.23  41.37  73.42 124.68 184.99 199.56 147.2   95.29  50.5  131.49 104.16  46.77 176.9  125.03  71.57 150.91  65.58 108.05 111.01 275.53 180.33  46.48  45.8  112.16]\n",
            "30 |  avg_price 41.37 loss -11.380828018188474\n",
            "92 |  avg_price 124.68 loss -32.87000244140626\n",
            "80 |  avg_price 104.16 loss -24.515018615722653\n",
            "[ 66.21 217.14  94.96 184.8  127.19 143.07  41.37  73.42 124.68 175.24 199.56 147.2   94.94  49.78 131.49 104.16  45.8  176.9  125.03  70.49 150.48  65.58 108.05 111.01 275.53 180.33  46.4   45.8  112.16]\n",
            "[ 66.76 217.14  94.96 184.8  127.19 143.07  41.37  73.42 124.68 175.24 199.56 147.2   94.94  49.78 131.49 104.16  45.8  176.9  125.03  70.49 150.48  65.58 108.05 111.01 275.53 180.33  46.4   45.8  112.16]\n",
            "130 |  avg_price 184.8 loss -55.18999938964845\n",
            "[ 64.32 217.14  94.96 184.8  127.19 142.79  41.37  73.42 124.68 175.24 199.56 147.2   94.51  49.78 131.49 104.16  45.8  176.9  125.03  70.49 150.48  65.58 108.05 111.01 275.53 180.33  46.32  45.8  112.16]\n",
            "[ 64.14 217.14  94.96 184.8  127.19 142.79  41.37  73.42 124.68 175.24 199.56 147.2   94.51  49.78 131.49 104.16  45.8  176.9  125.03  70.49 150.48  65.58 108.05 111.01 275.53 180.33  46.32  45.8  112.16]\n",
            "70 |  avg_price 94.96 loss -24.516465148925775\n",
            "131 |  avg_price 175.24 loss -44.461710205078134\n",
            "113 |  avg_price 147.2 loss -34.59632415771483\n",
            "[ 63.28 217.14  94.96 169.5  127.19 142.22  41.37  73.42 119.88 175.24 199.56 147.2   94.51  49.77 131.49 100.55  45.8  176.9  125.03  70.48 150.33  65.63 108.05 111.01 275.53 180.33  46.32  45.8  112.16]\n",
            "98 |  avg_price 169.5 loss -71.79000091552734\n",
            "95 |  avg_price 119.88 loss -24.949999694824214\n",
            "77 |  avg_price 100.55 loss -23.392514038085935\n",
            "[ 63.28 217.14  90.63 169.5  127.19 142.07  41.37  73.42 119.88 175.24 199.56 147.2   94.47  49.77 131.49 100.55  45.8  176.9  125.03  70.48 150.33  65.63 108.05 111.01 275.53 180.33  46.3   45.8  112.16]\n",
            "71 |  avg_price 90.63 loss -19.83304107666015\n",
            "35 |  avg_price 45.8 loss -10.892670440673825\n",
            "[ 63.28 217.14  90.63 159.83 127.19 142.07  41.37  73.42 119.88 175.24 199.56 147.2   93.98  49.77 131.49 100.55  45.8  176.9  125.03  70.48 150.33  65.63 108.05 111.01 275.53 180.33  46.24  45.8  112.16]\n",
            "106 |  avg_price 159.83 loss -54.20999725341798\n",
            "103 |  avg_price 131.49 loss -28.736421813964853\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.33  73.42 115.32 175.24 180.46 140.48  93.51  49.47 131.49  96.83  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.33  73.42 115.32 175.24 180.46 140.48  93.51  49.46 130.9   96.83  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.33  73.42 115.32 175.24 180.46 140.48  93.51  49.46 130.34  96.83  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.33  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.99  96.83  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.33  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.58  96.81  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.31  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.57  96.81  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.31  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.57  96.81  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "76 |  avg_price 96.81 loss -20.50278717041016\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.31  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.26  96.81  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.31  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.26  96.81  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.31  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.26  96.81  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.31  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.26  96.81  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.31  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.22  96.81  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.3   73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.29  96.81  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.28  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.32  96.81  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.28  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.32  96.81  40.05 176.9  125.03  70.48 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.28  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.45  96.81  40.05 176.9  125.03  70.49 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.28  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.46  96.81  40.05 176.9  125.03  70.49 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.27  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.46  96.81  40.05 176.9  125.03  70.49 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.27  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.57  96.81  40.05 176.9  125.03  70.49 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.27  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.63  96.81  40.05 176.9  125.03  70.49 149.86  65.24 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.25  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.64  96.81  40.05 176.9  125.03  70.49 149.86  65.26 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.25  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.64  96.81  40.05 176.9  125.03  70.49 149.86  65.26 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.25  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.64  96.81  40.05 176.9  125.03  70.49 149.86  65.26 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.25  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.64  96.81  40.05 176.9  125.03  70.49 149.86  65.26 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.25  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.64  96.81  40.05 176.9  125.03  70.49 149.86  65.26 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.25  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.7   96.81  40.05 176.9  125.03  70.49 149.86  65.26 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 61.73 217.14  85.61 159.83 127.19 142.    41.25  73.42 115.32 175.24 180.46 140.48  93.51  49.46 129.73  96.81  40.05 176.9  125.03  70.49 149.86  65.26 108.05 111.01 275.53 180.33  46.16  45.8  112.16]\n",
            "[ 67.29 217.14  85.95 156.36 118.58 142.    40.5   73.42 114.47 174.37 187.51 140.48  93.79  49.46 130.08  95.62  40.64 176.9  125.03  70.49 150.48  65.26 108.05 104.19 275.53 176.3   46.24  45.8  112.16]\n",
            "331 |  avg_price 156.36 Profit 174.4319006347656\n",
            "[ 69.2  217.14  90.72 156.36 124.29 142.    41.06  73.42 117.85 181.7  190.94 140.48  94.22  49.46 130.18  98.97  42.83 176.9  125.03  70.49 150.56  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 120.51 187.13 193.29 140.48  94.63  49.46 130.26 101.62  44.14 176.9  125.03  70.49 150.56  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "335 |  avg_price 188.72 Profit 146.56515625\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 122.7  187.13 193.29 143.6   94.89  49.77 130.36 103.59  44.14 180.91 125.03  71.96 150.62  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 146.06  94.89  49.77 130.45 105.31  44.14 180.91 125.03  71.96 150.63  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.06 130.56 105.31  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.06 130.64 105.63  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.07 130.71 105.63  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.07 130.71 105.63  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.07 130.71 105.63  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.07 130.71 105.63  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.07 130.71 105.63  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.07 130.71 105.63  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.07 130.75 105.63  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.07 130.75 105.63  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  94.22 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.07 130.85 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.34  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.72 131.02 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.72 131.02 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.72 131.02 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.72 131.02 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.79  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  72.89 150.87  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  71.96 150.87  65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  71.96 150.9   65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  71.96 150.9   65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  71.96 150.9   65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  71.96 150.9   65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  71.96 150.9   65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  71.96 150.9   65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  71.96 150.9   65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  71.96 150.9   65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 69.94 217.14  95.62 188.72 127.22 142.    41.45  73.42 124.52 187.13 193.29 147.74  94.89  50.73 131.03 105.64  44.14 180.91 125.03  71.96 150.9   65.26 108.05 110.84 275.53 180.32  46.45  45.8  112.16]\n",
            "30 |  avg_price 41.45 loss -11.46082801818848\n",
            "92 |  avg_price 124.52 loss -32.710002441406246\n",
            "80 |  avg_price 105.64 loss -25.995018615722657\n",
            "[ 66.18 217.14  93.92 182.47 127.22 141.93  41.45  73.42 124.52 176.91 193.29 147.74  94.62  50.12 131.03 105.64  43.85 180.91 125.03  70.83 150.54  65.26 108.05 110.84 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 66.74 217.14  93.92 182.47 127.22 141.93  41.45  73.42 124.52 176.91 193.29 147.74  94.62  50.12 131.03 105.64  43.85 180.91 125.03  70.83 150.54  65.26 108.05 110.84 275.53 180.32  46.38  45.8  112.16]\n",
            "130 |  avg_price 182.47 loss -52.859999389648436\n",
            "[ 64.31 217.14  93.92 182.47 127.22 141.72  41.45  73.42 124.52 176.91 193.29 147.74  94.29  50.12 131.03 105.64  43.85 180.91 125.03  70.83 150.54  65.26 108.05 110.84 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 64.12 217.14  93.92 182.47 127.22 141.72  41.45  73.42 124.52 176.91 193.29 147.74  94.29  50.12 131.03 105.64  43.85 180.91 125.03  70.83 150.54  65.26 108.05 110.84 275.53 180.32  46.32  45.8  112.16]\n",
            "70 |  avg_price 93.92 loss -23.476465148925783\n",
            "131 |  avg_price 176.91 loss -46.13171020507812\n",
            "113 |  avg_price 147.74 loss -35.13632415771485\n",
            "[ 63.27 217.14  93.92 167.6  127.22 141.3   41.45  73.42 120.31 176.91 193.29 147.74  94.29  50.11 131.03 101.84  43.85 180.91 125.03  70.82 150.41  65.33 108.05 110.84 275.53 180.32  46.32  45.8  112.16]\n",
            "98 |  avg_price 167.6 loss -69.89000091552734\n",
            "95 |  avg_price 120.31 loss -25.37999969482422\n",
            "77 |  avg_price 101.84 loss -24.68251403808594\n",
            "[ 63.27 217.14  90.5  167.6  127.22 141.22  41.45  73.42 120.31 176.91 193.29 147.74  94.25  50.11 131.03 101.84  43.85 180.91 125.03  70.82 150.41  65.33 108.05 110.84 275.53 180.32  46.3   45.8  112.16]\n",
            "71 |  avg_price 90.5 loss -19.703041076660156\n",
            "35 |  avg_price 43.85 loss -8.94267044067383\n",
            "[ 63.27 217.14  90.5  158.17 127.22 141.22  41.45  73.42 120.31 176.91 193.29 147.74  93.86  50.11 131.03 101.84  43.85 180.91 125.03  70.82 150.41  65.33 108.05 110.84 275.53 180.32  46.26  45.8  112.16]\n",
            "106 |  avg_price 158.17 loss -52.549997253417956\n",
            "103 |  avg_price 131.03 loss -28.276421813964845\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.41  73.42 116.23 176.91 181.4  141.76  93.48  49.84 131.03  97.96  40.65 180.91 125.03  70.82 150.01  65.02 108.05 110.84 275.53 180.32  46.19  45.8  112.16]\n",
            "113 |  avg_price 141.76 loss -29.25047851562499\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.41  73.42 116.23 176.91 181.4  141.76  93.48  49.71 130.21  97.96  40.65 180.91 125.03  70.82 150.01  65.02 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.41  73.42 116.23 176.91 181.4  141.76  93.48  49.71 129.81  97.96  40.65 180.91 125.03  70.82 150.01  65.02 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.39  73.42 116.23 176.91 181.4  141.76  93.48  49.71 129.56  97.96  40.65 180.91 125.03  70.82 150.01  65.02 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.39  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.25  97.96  40.65 180.91 125.03  70.82 150.01  65.02 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.39  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.24  97.96  40.65 180.91 125.03  70.82 150.01  65.02 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.39  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.24  97.96  40.65 180.91 125.03  70.82 150.01  65.02 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "76 |  avg_price 97.96 loss -21.65278717041015\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.39  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.01  97.96  40.65 180.91 125.03  70.82 150.01  65.02 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.39  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.01  97.96  40.65 180.91 125.03  70.82 150.01  65.02 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.39  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.01  97.96  40.65 180.91 125.03  70.82 150.01  65.02 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.39  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.01  97.96  40.65 180.91 125.03  70.82 150.01  65.02 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.39  73.42 116.23 176.91 181.4  141.76  93.48  49.7  128.98  97.96  40.65 180.91 125.03  70.82 150.01  65.04 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.39  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.04  97.96  40.65 180.91 125.03  70.82 150.01  65.04 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.39  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.06  97.96  40.65 180.91 125.03  70.82 150.01  65.05 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.38  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.07  97.96  40.65 180.91 125.03  70.82 150.01  65.05 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.37  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.13  97.96  40.65 180.91 125.03  70.82 150.01  65.05 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.37  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.13  97.96  40.65 180.91 125.03  70.82 150.01  65.05 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.37  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.13  97.96  40.65 180.91 125.03  70.82 150.01  65.05 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.36  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.14  97.96  40.65 180.91 125.03  70.82 150.01  65.05 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.35  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.16  97.96  40.65 180.91 125.03  70.82 150.01  65.06 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.33  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.16  97.96  40.65 180.91 125.03  70.82 150.01  65.06 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.33  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.18  97.96  40.65 180.91 125.03  70.82 150.01  65.06 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.33  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.19  97.96  40.65 180.91 125.03  70.82 150.01  65.08 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.33  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.22  97.96  40.65 180.91 125.03  70.82 150.01  65.08 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.33  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.24  97.96  40.65 180.91 125.03  70.82 150.01  65.09 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.33  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.27  97.94  40.65 180.91 125.03  70.82 150.01  65.09 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 61.72 217.14  86.34 158.17 127.22 141.19  41.33  73.42 116.23 176.91 181.4  141.76  93.48  49.7  129.27  97.94  40.65 180.91 125.03  70.82 150.01  65.09 108.05 110.84 275.53 180.32  46.17  45.8  112.16]\n",
            "[ 67.29 217.14  86.53 155.01 118.6  141.19  40.57  73.42 115.38 175.68 186.69 141.76  93.71  49.7  129.57  96.61  41.04 180.91 125.03  70.82 150.54  65.09 108.05 104.09 275.53 176.3   46.23  45.8  112.16]\n",
            "331 |  avg_price 155.01 Profit 175.78190063476563\n",
            "[ 69.2  217.14  90.63 155.01 124.3  141.19  41.11  73.42 118.35 182.77 189.65 141.76  94.07  49.7  129.67  99.84  43.13 180.91 125.03  70.82 150.6   65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.48  73.42 120.73 188.03 191.84 141.76  94.41  49.7  129.75 102.4   44.38 180.91 125.03  70.82 150.6   65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "335 |  avg_price 187.62 Profit 147.66515625\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.48  73.42 122.73 188.03 191.84 144.72  94.62  49.9  129.85 104.3   44.38 183.46 125.03  72.21 150.65  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.48  73.42 124.41 188.03 191.84 147.06  94.62  49.9  129.94 105.95  44.38 183.46 125.03  72.21 150.66  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.48  73.42 124.41 188.03 191.84 148.66  94.62  50.1  130.04 105.95  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.1  130.13 106.26  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.1  130.2  106.26  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.1  130.2  106.26  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.1  130.2  106.26  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.1  130.2  106.26  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.1  130.2  106.26  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.1  130.2  106.26  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.1  130.24 106.26  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.1  130.24 106.26  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  93.75 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.1  130.32 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.56 130.48 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.56 130.48 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.56 130.48 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.56 130.48 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.79  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  73.09 150.87  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  72.13 150.87  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  72.13 150.89  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  72.13 150.89  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  72.13 150.89  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  72.13 150.89  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  72.13 150.89  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "[ 69.93 217.14  95.02 187.62 127.23 141.19  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.57 130.49 106.28  44.38 183.46 125.03  72.13 150.89  65.09 108.05 110.77 275.53 180.32  46.4   45.8  112.16]\n",
            "85 |  avg_price 106.28 loss -21.75955322265625\n",
            "[ 69.93 217.14  94.91 194.92 127.23 141.26  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.36 130.49 106.28  44.38 183.46 125.03  72.13 150.89  65.11 108.05 110.77 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 69.93 217.14  94.91 194.92 127.23 141.26  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.36 130.49 106.28  44.38 183.46 125.03  72.13 150.89  65.11 108.05 110.77 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 68.77 217.14  94.91 194.92 127.23 141.26  41.49  73.42 124.41 188.03 191.84 148.66  94.62  50.36 130.49 106.28  44.38 183.46 125.03  72.13 150.89  65.11 108.05 110.77 275.53 180.32  46.42  45.8  112.16]\n",
            "155 |  avg_price 194.92 loss -40.08000366210936\n",
            "30 |  avg_price 41.49 loss -11.500828018188479\n",
            "92 |  avg_price 124.41 loss -32.60000244140625\n",
            "[ 68.77 217.14  93.57 194.92 127.23 141.24  41.49  73.42 124.41 177.61 190.8  148.66  94.4   50.36 130.49 106.28  44.38 183.46 125.03  70.98 150.58  65.59 108.05 110.77 275.53 180.32  46.37  45.8  112.16]\n",
            "[ 68.77 217.14  93.57 194.92 127.23 141.24  41.46  73.42 124.41 177.61 190.8  148.66  94.4   50.36 130.49 106.28  44.38 183.46 125.03  70.98 150.58  65.59 108.05 110.77 275.53 180.32  46.37  45.8  112.16]\n",
            "30 |  avg_price 41.46 loss -11.010155639648438\n",
            "[ 68.77 217.14  93.57 194.92 127.23 141.24  41.46  73.42 124.41 177.61 190.8  148.66  94.4   50.36 130.49 106.28  44.38 183.46 125.03  70.98 150.58  65.59 108.05 110.77 275.53 180.32  46.37  45.8  112.16]\n",
            "[ 66.64 217.14  93.57 194.92 127.23 141.24  41.46  73.42 124.41 177.61 190.8  148.66  94.4   50.36 130.49 106.28  44.38 183.46 125.03  70.98 150.58  65.59 108.05 110.77 275.53 180.32  46.37  45.8  112.16]\n",
            "70 |  avg_price 93.57 loss -23.126465148925774\n",
            "131 |  avg_price 177.61 loss -46.83171020507814\n",
            "145 |  avg_price 190.8 loss -45.602886962890636\n",
            "113 |  avg_price 148.66 loss -36.05632415771484\n",
            "[ 64.82 217.14  93.57 177.75 127.23 140.9   41.46  73.42 120.65 177.61 190.8  148.66  94.21  50.04 130.49 102.39  44.38 183.46 125.03  70.13 150.31  65.62 108.05 110.77 275.53 180.32  46.37  45.8  112.16]\n",
            "98 |  avg_price 177.75 loss -80.04000091552734\n",
            "95 |  avg_price 120.65 loss -25.719999694824224\n",
            "77 |  avg_price 102.39 loss -25.232514038085938\n",
            "[ 64.82 217.14  90.69 177.75 127.23 140.85  41.46  73.42 120.65 177.61 190.8  148.66  94.21  50.04 130.49 102.39  44.38 183.46 125.03  70.13 150.31  65.65 108.05 110.77 275.53 180.32  46.35  45.8  112.16]\n",
            "71 |  avg_price 90.69 loss -19.893041076660154\n",
            "35 |  avg_price 44.38 loss -9.47267044067383\n",
            "[ 64.82 217.14  90.69 168.89 127.23 140.85  41.46  73.42 120.61 177.61 190.8  148.66  94.21  50.04 130.49 102.39  44.38 183.46 125.03  70.13 150.31  65.65 108.05 110.77 275.53 180.32  46.32  45.8  112.16]\n",
            "106 |  avg_price 168.89 loss -63.269997253417955\n",
            "86 |  avg_price 120.61 loss -34.84999786376953\n",
            "103 |  avg_price 130.49 loss -27.736421813964853\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.04 130.49  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.26  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.04 130.15  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.26  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.01 129.53  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.26  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.01 129.14  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.26  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.92  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.26  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.92  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.26  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.92  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.26  45.8  112.16]\n",
            "76 |  avg_price 98.45 loss -22.14278717041016\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.63  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.63  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.63  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.63  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.63  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.46  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.63  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.44  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.64  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.44  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.65  98.45  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.44  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.74  98.43  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.44  73.42 120.61 177.61 182.3  148.66  93.87  50.01 128.86  98.43  41.03 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.44  73.42 120.61 177.61 182.3  148.66  93.87  50.01 129.01  98.43  41.04 163.35 125.03  70.13 149.97  65.38 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.43  73.42 120.61 177.61 182.3  148.66  93.87  50.01 129.18  98.43  41.04 163.35 125.03  70.13 149.97  65.39 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.43  73.42 120.61 177.61 182.3  148.66  93.87  50.01 129.28  98.43  41.04 163.35 125.03  70.13 149.97  65.39 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.43  73.42 120.61 177.61 182.3  148.66  93.87  50.01 129.34  98.43  41.04 163.35 125.03  70.13 149.97  65.39 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.43  73.42 120.61 177.61 182.3  148.66  93.87  50.01 129.38  98.43  41.04 163.35 125.03  70.13 149.97  65.39 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.43  73.42 120.61 177.61 182.3  148.66  93.87  50.01 129.38  98.43  41.04 163.35 125.03  70.13 149.97  65.39 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.43  73.42 120.61 177.61 182.3  148.66  93.87  50.01 129.38  98.43  41.04 163.35 125.03  70.13 149.97  65.39 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.43  73.42 120.61 177.61 182.3  148.66  93.87  50.01 129.38  98.43  41.04 163.35 125.03  70.13 149.97  65.39 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.43  73.42 120.61 177.61 182.3  148.66  93.87  50.01 129.38  98.43  41.04 163.35 125.03  70.13 149.97  65.39 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "[ 64.82 217.14  87.11 168.89 127.23 140.84  41.43  73.42 120.61 177.61 182.3  148.66  93.87  50.01 129.38  98.43  41.04 163.35 125.03  70.13 149.97  65.39 108.05 110.77 275.53 180.32  46.25  45.8  112.16]\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 8           |\n",
            "|    fps             | 15          |\n",
            "|    time_elapsed    | 42          |\n",
            "|    total_timesteps | 664         |\n",
            "| train/             |             |\n",
            "|    reward          | -0.13885188 |\n",
            "------------------------------------\n",
            "[ 68.71 217.14  87.17 163.75 118.6  140.84  40.65  73.42 119.3  176.23 187.07 148.66  94.06  50.01 129.63  97.04  41.23 163.35 125.03  70.13 150.42  65.39 108.05 104.05 275.53 176.3   46.3   45.8  112.16]\n",
            "331 |  avg_price 163.75 Profit 167.04190063476562\n",
            "[ 70.12 217.14  90.74 163.75 124.31 140.84  41.17  73.42 121.89 183.22 189.81 148.66  94.37  50.01 129.7  100.22  42.87 163.35 125.03  70.13 150.48  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 123.96 188.41 191.88 148.66  94.66  50.01 129.77 102.74  43.95 163.35 125.03  70.13 150.48  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "335 |  avg_price 194.75 Profit 140.53515625\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 125.71 188.41 191.88 150.79  94.84  50.19 129.85 104.6   43.95 169.9  125.03  71.69 150.52  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 152.48  94.84  50.19 129.93 106.23  43.95 169.9  125.03  71.69 150.53  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.37 130.01 106.23  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.37 130.08 106.53  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.37 130.14 106.53  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.37 130.14 106.53  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.37 130.14 106.53  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.37 130.14 106.53  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.37 130.14 106.53  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.37 130.14 106.53  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.37 130.17 106.53  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.37 130.17 106.53  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  93.54 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.37 130.25 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.38  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.38 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.38 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.38 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.38 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.65  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  72.67 150.71  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  71.77 150.71  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  71.77 150.74  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  71.77 150.74  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  71.77 150.74  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  71.77 150.74  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  71.77 150.74  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "[ 70.69 217.14  94.69 194.75 127.23 140.84  41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.83 130.39 106.55  43.95 169.9  125.03  71.77 150.74  65.39 108.05 110.74 275.53 180.32  46.45  45.8  112.16]\n",
            "85 |  avg_price 106.55 loss -22.029553222656247\n",
            "[ 70.69 217.14  94.62 200.73 127.23 140.9   41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.61 130.39 106.55  43.95 169.9  125.03  71.77 150.74  65.4  108.05 110.74 275.53 180.32  46.46  45.8  112.16]\n",
            "[ 70.69 217.14  94.62 200.73 127.23 140.9   41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.61 130.39 106.55  43.95 169.9  125.03  71.77 150.74  65.4  108.05 110.74 275.53 180.32  46.46  45.8  112.16]\n",
            "[ 69.16 217.14  94.62 200.73 127.23 140.9   41.53  73.42 127.17 188.41 191.88 153.66  94.84  50.61 130.39 106.55  43.95 169.9  125.03  71.77 150.74  65.4  108.05 110.74 275.53 180.32  46.46  45.8  112.16]\n",
            "155 |  avg_price 200.73 loss -45.890003662109365\n",
            "30 |  avg_price 41.53 loss -11.540828018188478\n",
            "92 |  avg_price 127.17 loss -35.36000244140625\n",
            "[ 69.16 217.14  93.41 200.73 127.23 140.89  41.53  73.42 127.17 177.91 190.91 153.66  94.65  50.61 130.39 106.55  43.95 169.9  125.03  70.67 150.47  65.78 108.05 110.74 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 69.16 217.14  93.41 200.73 127.23 140.89  41.5   73.42 127.17 177.91 190.91 153.66  94.65  50.61 130.39 106.55  43.95 169.9  125.03  70.67 150.47  65.78 108.05 110.74 275.53 180.32  46.42  45.8  112.16]\n",
            "30 |  avg_price 41.5 loss -11.050155639648438\n",
            "[ 69.16 217.14  93.41 200.73 127.23 140.89  41.5   73.42 127.17 177.91 190.91 153.66  94.65  50.61 130.39 106.55  43.95 169.9  125.03  70.67 150.47  65.78 108.05 110.74 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 66.91 217.14  93.41 200.73 127.23 140.89  41.5   73.42 127.17 177.91 190.91 153.66  94.65  50.61 130.39 106.55  43.95 169.9  125.03  70.67 150.47  65.78 108.05 110.74 275.53 180.32  46.42  45.8  112.16]\n",
            "70 |  avg_price 93.41 loss -22.966465148925778\n",
            "131 |  avg_price 177.91 loss -47.13171020507812\n",
            "145 |  avg_price 190.91 loss -45.71288696289062\n",
            "113 |  avg_price 153.66 loss -41.05632415771484\n",
            "[ 65.01 217.14  93.41 182.49 127.23 140.61  41.5   73.42 123.12 177.91 190.91 153.66  94.47  50.28 130.39 102.63  43.95 169.9  125.03  69.85 150.23  65.8  108.05 110.74 275.53 180.32  46.42  45.8  112.16]\n",
            "98 |  avg_price 182.49 loss -84.78000091552735\n",
            "95 |  avg_price 123.12 loss -28.189999694824223\n",
            "77 |  avg_price 102.63 loss -25.472514038085933\n",
            "[ 65.01 217.14  90.92 182.49 127.23 140.57  41.5   73.42 123.12 177.91 190.91 153.66  94.47  50.28 130.39 102.63  43.95 169.9  125.03  69.85 150.23  65.82 108.05 110.74 275.53 180.32  46.4   45.8  112.16]\n",
            "71 |  avg_price 90.92 loss -20.123041076660158\n",
            "35 |  avg_price 43.95 loss -9.042670440673831\n",
            "[ 65.01 217.14  90.92 173.13 127.23 140.57  41.5   73.42 123.08 177.91 190.91 153.66  94.47  50.28 130.39 102.63  43.95 169.9  125.03  69.85 150.23  65.82 108.05 110.74 275.53 180.32  46.37  45.8  112.16]\n",
            "106 |  avg_price 173.13 loss -67.50999725341796\n",
            "86 |  avg_price 123.08 loss -37.31999786376953\n",
            "103 |  avg_price 130.39 loss -27.63642181396483\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.27 130.39  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.27 130.11  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.61  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.3   98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.11  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.11  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.32  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.11  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.32  45.8  112.16]\n",
            "76 |  avg_price 98.66 loss -22.352787170410153\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.25 128.87  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.25 128.87  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.25 128.87  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.25 128.87  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.25 128.87  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.5   73.42 123.08 177.91 182.93 153.66  94.17  50.25 128.87  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.49  73.42 123.08 177.91 182.93 153.66  94.17  50.25 128.88  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.49  73.42 123.08 177.91 182.93 153.66  94.17  50.25 128.88  98.66  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.49  73.42 123.08 177.91 182.93 153.66  94.17  50.25 128.95  98.65  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.49  73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.05  98.65  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.49  73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.17  98.65  41.55 158.74 125.03  69.85 149.94  65.59 108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.48  73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.31  98.65  41.55 158.74 125.03  69.85 149.94  65.6  108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.48  73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.39  98.65  41.55 158.74 125.03  69.85 149.94  65.6  108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.48  73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.45  98.65  41.55 158.74 125.03  69.85 149.94  65.6  108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.48  73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.48  98.65  41.55 158.74 125.03  69.85 149.94  65.6  108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.48  73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.48  98.65  41.55 158.74 125.03  69.85 149.94  65.6  108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.48  73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.48  98.65  41.55 158.74 125.03  69.85 149.94  65.6  108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.48  73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.48  98.65  41.55 158.74 125.03  69.85 149.94  65.6  108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.48  73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.48  98.65  41.55 158.74 125.03  69.85 149.94  65.6  108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "[ 65.01 217.14  87.76 173.13 127.23 140.57  41.48  73.42 123.08 177.91 182.93 153.66  94.17  50.25 129.48  98.65  41.55 158.74 125.03  69.85 149.94  65.6  108.05 110.74 275.53 180.32  46.31  45.8  112.16]\n",
            "day: 82, episode: 10\n",
            "begin_total_asset: 500000.00\n",
            "end_total_asset: 479556.93\n",
            "total_reward: -20443.07\n",
            "total_cost: 2067.28\n",
            "total_trades: 1569\n",
            "Sharpe: 0.040\n",
            "=================================\n",
            "[ 68.8  217.14  87.74 167.2  118.61 140.57  40.68  73.42 121.5  176.46 187.29 153.66  94.33  50.25 129.68  97.23  41.61 158.74 125.03  69.85 150.33  65.6  108.05 104.03 275.53 176.3   46.36  45.8  112.16]\n",
            "331 |  avg_price 167.2 Profit 163.59190063476564\n",
            "[ 70.18 217.14  90.89 167.2  124.31 140.57  41.2   73.42 123.89 183.41 189.86 153.66  94.6   50.25 129.74 100.38  42.9  158.74 125.03  69.85 150.39  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 125.78 188.58 191.83 153.66  94.85  50.25 129.8  102.88  43.83 158.74 125.03  69.85 150.39  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "335 |  avg_price 197.57 Profit 137.71515625\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 127.38 188.58 191.83 155.18  95.01  50.42 129.87 104.73  43.83 164.88 125.03  71.48 150.43  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 156.4   95.01  50.42 129.93 106.34  43.83 164.88 125.03  71.48 150.44  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  50.58 130.01 106.34  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  50.58 130.07 106.65  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  50.59 130.12 106.65  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  50.59 130.12 106.65  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  50.59 130.12 106.65  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  50.59 130.12 106.65  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  50.59 130.12 106.65  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  50.59 130.12 106.65  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  50.59 130.15 106.65  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  50.59 130.15 106.65  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  93.42 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  50.59 130.21 106.66  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.42  45.8  112.16]\n",
            "[ 70.74 217.14  94.47 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.33 106.66  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  94.47 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.33 106.66  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  94.47 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.33 106.66  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  94.47 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.33 106.66  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  94.47 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.34 106.66  43.83 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  95.39 197.57 127.23 140.57  41.55  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.48 106.66  45.07 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.63 106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.33 197.57 127.23 140.57  41.56  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.8  106.66  46.05 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "85 |  avg_price 106.66 loss -22.139553222656247\n",
            "[ 70.74 217.14  96.14 197.57 112.82 140.73  41.53  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.72 106.66  46.14 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.14 197.57 112.82 140.73  41.53  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.72 106.66  46.14 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  96.14 197.57 112.82 140.73  41.53  73.42 128.72 188.58 191.83 157.27  95.01  51.03 130.72 106.66  46.14 164.88 125.03  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "85 |  avg_price 112.82 loss -27.377800292968743\n",
            "30 |  avg_price 41.53 loss -11.540828018188478\n",
            "92 |  avg_price 128.72 loss -36.91000244140625\n",
            "[ 70.74 217.14  94.98 197.57 112.82 140.73  41.53  73.42 128.72 178.04 191.15 157.27  95.01  51.03 130.48 106.66  46.14 164.88 123.4   72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  94.98 197.57 112.82 140.73  41.53  73.42 128.72 178.04 191.15 157.27  95.01  51.03 130.48 106.66  46.14 164.88 123.4   72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "41 |  avg_price 51.03 loss -10.514840393066407\n",
            "[ 70.74 217.14  94.98 197.57 112.82 140.73  41.53  73.42 128.72 178.04 191.15 157.27  95.01  51.03 130.26 106.66  46.07 164.88 123.4   72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "[ 70.74 217.14  94.98 197.57 112.82 140.73  41.53  73.42 128.72 178.04 191.15 157.27  95.01  51.03 130.26 106.66  46.07 164.88 123.4   72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "70 |  avg_price 94.98 loss -24.536465148925785\n",
            "131 |  avg_price 178.04 loss -47.26171020507812\n",
            "145 |  avg_price 191.15 loss -45.95288696289063\n",
            "[ 70.74 217.14  94.98 197.57 103.43 140.49  39.29  73.42 124.69 178.04 191.15 157.27  95.01  51.03 130.17 106.66  46.07 164.88 123.17  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "95 |  avg_price 124.69 loss -29.759999694824216\n",
            "[ 70.74 217.14  94.98 197.57 103.43 140.49  39.29  73.42 124.69 178.04 191.15 157.27  95.01  51.03 130.02 106.66  46.06 164.88 123.17  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "56 |  avg_price 70.74 loss -14.547228088378901\n",
            "35 |  avg_price 46.06 loss -11.15267044067383\n",
            "[ 70.74 217.14  94.98 197.57 103.43 140.49  39.27  73.42 124.69 177.23 181.46 157.27  95.01  50.5  129.71 106.66  46.06 164.88 123.17  72.5  150.54  65.6  108.05 110.73 275.53 180.32  46.48  45.8  112.16]\n",
            "31 |  avg_price 39.27 loss -8.016229171752933\n",
            "126 |  avg_price 177.23 loss -51.168392639160146\n",
            "103 |  avg_price 129.71 loss -26.95642181396485\n",
            "[ 62.23 217.14  91.72 197.57  97.58 140.49  39.27  73.42 120.59 177.23 176.54 157.27  94.72  50.21 129.71 102.18  43.7  164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "80 |  avg_price 102.18 loss -22.191299133300788\n",
            "[ 62.23 217.14  90.57 197.57  96.68 140.84  38.04  73.42 118.44 169.79 176.02 157.27  94.72  50.21 129.29 102.18  42.41 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  96.68 140.84  38.04  73.42 118.44 169.79 176.02 157.27  94.72  50.21 129.29 102.18  42.41 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  96.68 140.84  38.04  73.42 118.44 169.79 176.02 157.27  94.72  50.21 129.29 102.18  42.41 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  96.68 140.84  38.04  73.42 118.44 169.79 176.02 157.27  94.72  50.21 129.29 102.18  42.41 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  96.68 140.84  38.04  73.42 118.44 169.79 176.02 157.27  94.72  50.21 129.29 102.18  42.41 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  96.68 140.84  38.04  73.42 118.44 169.79 176.02 157.27  94.72  50.21 129.29 102.18  42.41 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "136 |  avg_price 169.79 loss -34.08953002929687\n",
            "[ 62.23 217.14  90.57 197.57  96.68 140.84  38.04  73.42 118.44 169.79 176.02 157.27  94.72  50.21 129.07 102.18  42.15 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  96.68 140.84  38.04  73.42 118.44 169.79 176.02 157.27  94.72  50.21 129.07 102.18  42.15 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "71 |  avg_price 90.57 loss -19.879226989746087\n",
            "94 |  avg_price 118.44 loss -24.56000274658203\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 62.23 217.14  90.57 197.57  98.53 140.77  38.03  73.42 118.44 169.79 176.02 157.27  94.72  50.21 128.97 102.18  41.85 164.88 119.94  72.5  150.26  65.6  108.05 110.73 275.53 180.32  46.43  45.8  112.16]\n",
            "[ 65.67 217.14  90.24 197.57 100.2  141.31  38.16  73.42 117.36 170.12 179.68 157.27  94.86  50.43 129.17 100.31  41.85 164.88 122.33  72.5  150.62  65.6  108.05 110.73 275.53 180.32  46.47  45.8  112.16]\n",
            "[ 67.5  217.14  93.14 197.57 105.28 141.93  39.03  73.42 120.14 178.22 182.27 157.27  95.1   50.61 129.23 103.12  42.65 164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.5  217.14  95.46 197.57 109.11 142.71  39.66  73.42 122.36 184.18 184.45 157.27  95.31  50.62 129.3  103.12  43.3  164.88 127.03  72.5  150.67  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "85 |  avg_price 109.11 loss -23.66780029296875\n",
            "30 |  avg_price 39.66 loss -9.670828018188473\n",
            "92 |  avg_price 122.36 loss -30.55000244140625\n",
            "141 |  avg_price 184.18 loss -43.445304565429694\n",
            "80 |  avg_price 103.12 loss -23.47501861572266\n",
            "[ 66.13 217.14  94.14 197.57 109.11 142.65  39.66  73.42 122.36 184.18 183.84 157.27  95.14  50.19 129.04 103.12  43.24 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 66.13 217.14  94.14 197.57 109.11 142.65  39.66  73.42 122.36 184.18 183.84 157.27  95.14  50.19 128.95 103.12  43.3  164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 66.13 217.14  94.14 197.57 109.11 142.65  39.66  73.42 122.36 184.18 183.84 157.27  95.14  50.19 128.95 103.12  43.3  164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 66.13 217.14  94.14 197.57 109.11 142.65  39.66  73.42 122.36 184.18 183.84 157.27  95.14  50.19 128.95 103.12  43.3  164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "70 |  avg_price 94.14 loss -23.696465148925782\n",
            "145 |  avg_price 183.84 loss -38.64288696289063\n",
            "[ 65.89 217.14  94.14 197.57 106.44 142.41  38.32  73.42 118.82 172.42 183.84 157.27  94.98  49.89 128.87  99.65  43.11 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "95 |  avg_price 118.82 loss -23.889999694824212\n",
            "77 |  avg_price 99.65 loss -22.492514038085943\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.32  73.42 118.82 172.42 183.84 157.27  94.98  49.89 128.66  99.65  42.76 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "71 |  avg_price 93.59 loss -22.79304107666016\n",
            "129 |  avg_price 172.42 loss -43.145448608398425\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 128.34  99.65  42.27 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "85 |  avg_price 106.44 loss -21.377499999999998\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 65.89 217.14  93.59 197.57 106.44 142.41  38.31  73.42 118.82 172.42 183.84 157.27  94.98  49.89 127.97  99.65  42.26 164.88 126.12  72.5  150.44  65.6  108.05 110.73 275.53 180.32  46.49  45.8  112.16]\n",
            "[ 66.91 217.14  92.92 197.57 106.82 142.86  38.37  73.42 117.69 172.17 186.52 157.27  95.11  50.08 128.16  98.1   42.23 164.88 127.12  72.5  150.79  65.6  108.05 110.73 275.53 180.32  46.53  45.8  112.16]\n",
            "[ 67.73 217.14  95.55 197.57 110.98 143.38  39.08  73.42 120.44 179.9  188.35 157.27  95.32  50.25 128.23 101.16  42.65 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.73 217.14  97.65 197.57 114.1  144.03  39.61  73.42 122.64 185.6  189.91 157.27  95.51  50.25 128.3  101.16  43.02 164.88 130.03  72.5  150.83  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "85 |  avg_price 114.1 loss -28.657800292968744\n",
            "30 |  avg_price 39.61 loss -9.620828018188476\n",
            "92 |  avg_price 122.64 loss -30.83000244140625\n",
            "141 |  avg_price 185.6 loss -44.86530456542968\n",
            "80 |  avg_price 101.16 loss -21.515018615722653\n",
            "[ 66.88 217.14  96.15 197.57 114.1  143.96  39.61  73.42 122.64 185.6  188.8  157.27  95.35  49.91 128.1  101.16  43.   164.88 129.1   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.88 217.14  96.15 197.57 114.1  143.96  39.61  73.42 122.64 185.6  188.8  157.27  95.35  49.91 128.04 101.16  43.05 164.88 129.1   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.88 217.14  96.15 197.57 114.1  143.96  39.61  73.42 122.64 185.6  188.8  157.27  95.35  49.91 128.04 101.16  43.05 164.88 129.1   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.88 217.14  96.15 197.57 114.1  143.96  39.61  73.42 122.64 185.6  188.8  157.27  95.35  49.91 128.04 101.16  43.05 164.88 129.1   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "70 |  avg_price 96.15 loss -25.706465148925787\n",
            "145 |  avg_price 188.8 loss -43.602886962890636\n",
            "[ 66.71 217.14  96.15 197.57 110.62 143.71  38.53  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.98  97.94  42.94 164.88 129.1   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "95 |  avg_price 119.07 loss -24.139999694824212\n",
            "77 |  avg_price 97.94 loss -20.782514038085935\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.53  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.82  97.94  42.73 164.88 129.1   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "71 |  avg_price 95.54 loss -24.743041076660163\n",
            "88 |  avg_price 110.62 loss -22.17722045898438\n",
            "129 |  avg_price 173.53 loss -44.25544860839844\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "[ 66.71 217.14  95.54 197.57 110.62 143.71  38.51  73.42 119.07 173.53 188.8  157.27  95.2   49.66 127.56  97.94  42.73 164.88 127.7   72.5  150.61  65.6  108.05 110.73 275.53 180.32  46.54  45.8  112.16]\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 12          |\n",
            "|    fps             | 10          |\n",
            "|    time_elapsed    | 91          |\n",
            "|    total_timesteps | 996         |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -7.01       |\n",
            "|    critic_loss     | 1.12e+05    |\n",
            "|    learning_rate   | 0.0103      |\n",
            "|    n_updates       | 768         |\n",
            "|    reward          | -0.28840208 |\n",
            "------------------------------------\n",
            "[ 67.29 217.14  94.66 197.57 110.32 144.08  38.53  73.42 117.92 173.04 190.85 157.27  95.32  49.83 127.73  96.61  42.69 164.88 128.28  72.5  150.94  65.6  108.05 110.73 275.53 180.32  46.58  45.8  112.16]\n",
            "[ 67.82 217.14  97.11 197.57 113.99 144.52  39.12  73.42 120.65 180.61 192.2  157.27  95.52  49.99 127.8   99.83  42.97 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.82 217.14  99.07 197.57 116.74 145.09  39.59  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.87  99.83  43.22 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "93 |  avg_price 116.74 loss -23.49076293945312\n",
            "[ 67.82 217.14  98.4  197.57 116.74 145.11  39.57  73.42 122.82 186.2  193.37 157.27  95.69  49.99 127.78  99.83  43.38 164.88 130.31  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "30 |  avg_price 39.57 loss -9.580828018188477\n",
            "92 |  avg_price 122.82 loss -31.010002441406243\n",
            "141 |  avg_price 186.2 loss -45.465304565429676\n",
            "80 |  avg_price 99.83 loss -20.185018615722655\n",
            "[ 67.23 217.14  96.96 197.57 111.65 145.02  39.57  73.42 122.82 186.2  191.95 157.27  95.54  49.7  127.61  99.83  43.35 164.88 129.6   72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.23 217.14  96.96 197.57 111.65 145.02  39.57  73.42 122.82 186.2  191.95 157.27  95.54  49.7  127.61  99.83  43.35 164.88 129.6   72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "87 |  avg_price 111.65 loss -25.142774963378912\n",
            "[ 67.23 217.14  96.96 197.57 111.65 145.02  39.57  73.42 122.82 186.2  191.95 157.27  95.54  49.7  127.51  99.83  43.35 164.88 129.6   72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.23 217.14  96.96 197.57 111.65 145.02  39.57  73.42 122.82 186.2  191.95 157.27  95.54  49.7  127.51  99.83  43.35 164.88 129.6   72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "70 |  avg_price 96.96 loss -26.516465148925775\n",
            "145 |  avg_price 191.95 loss -46.752886962890614\n",
            "[ 66.67 217.14  96.96 197.57 108.56 144.78  38.66  73.42 119.23 173.99 191.95 157.27  95.4   49.49 127.47  96.78  43.26 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "95 |  avg_price 119.23 loss -24.299999694824223\n",
            "77 |  avg_price 96.78 loss -19.62251403808594\n",
            "[ 66.67 217.14  96.33 197.57 108.56 144.78  38.65  73.42 119.23 173.99 191.95 157.27  95.4   49.49 127.33  96.78  43.08 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "71 |  avg_price 96.33 loss -25.533041076660155\n",
            "129 |  avg_price 173.99 loss -44.71544860839845\n",
            "[ 66.67 217.14  96.33 197.57 108.56 144.78  38.65  73.42 119.23 173.99 191.95 157.27  95.4   49.49 127.1   96.78  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "85 |  avg_price 108.56 loss -23.497500000000002\n",
            "34 |  avg_price 42.82 loss -8.587124176025391\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.78  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.78  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.78  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.78  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.78  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.78  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.78  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.78  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "74 |  avg_price 92.92 loss -19.29017822265625\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 66.67 217.14  92.92 197.57 108.56 144.72  38.63  73.42 119.23 173.99 191.95 157.27  95.4   49.49 126.75  93.56  42.82 164.88 129.52  72.5  150.98  65.6  108.05 110.73 275.53 180.32  46.63  45.8  112.16]\n",
            "[ 67.08 217.14  92.33 197.57 108.6  145.03  38.63  73.42 118.06 173.4  193.61 157.27  95.5   49.65 126.91  92.88  42.77 164.88 129.88  72.5  151.29  65.6  108.05 110.73 275.53 180.32  46.66  45.8  112.16]\n",
            "[ 67.48 217.14  95.02 197.57 112.51 145.41  39.14  73.42 120.78 180.91 194.65 157.27  95.68  49.79 126.98  96.15  43.05 164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.48 217.14  97.16 197.57 115.44 145.9   39.56  73.42 122.94 186.45 195.57 157.27  95.84  49.79 127.05  96.15  43.3  164.88 131.46  72.5  151.32  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "85 |  avg_price 115.44 loss -29.997800292968748\n",
            "30 |  avg_price 39.56 loss -9.570828018188479\n",
            "92 |  avg_price 122.94 loss -31.130002441406248\n",
            "141 |  avg_price 186.45 loss -45.715304565429676\n",
            "[ 67.04 217.14  95.71 197.57 115.44 145.81  39.56  73.42 122.94 186.45 193.95 157.27  95.7   49.54 126.91  94.63  43.28 164.88 130.81  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  95.71 197.57 115.44 145.81  39.56  73.42 122.94 186.45 193.95 157.27  95.7   49.54 126.91  94.63  43.28 164.88 130.81  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "154 |  avg_price 193.95 loss -40.04832763671874\n",
            "[ 67.04 217.14  94.65 197.57 110.73 145.45  38.37  73.42 120.   183.68 193.95 157.27  95.7   49.54 126.79  94.63  43.2  164.88 130.81  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "94 |  avg_price 120.0 loss -26.470001220703125\n",
            "[ 67.04 217.14  94.65 197.57 110.73 145.45  38.36  73.42 120.   183.68 193.95 157.27  95.7   49.54 126.79  94.63  43.2  164.88 130.81  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "70 |  avg_price 94.65 loss -24.206465148925787\n",
            "131 |  avg_price 183.68 loss -52.90171020507813\n",
            "[ 67.04 217.14  94.65 197.57 108.21 145.24  38.36  73.42 120.   183.68 193.95 157.27  95.7   49.54 126.76  94.63  43.12 164.88 130.81  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  94.65 197.57 108.21 145.24  38.36  73.42 120.   183.68 193.95 157.27  95.7   49.54 126.76  94.63  43.12 164.88 130.81  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "76 |  avg_price 94.63 loss -19.100687866210933\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.24  38.36  73.42 120.   183.68 193.95 157.27  95.7   49.54 126.57  94.63  42.84 164.88 130.81  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "66 |  avg_price 91.98 loss -26.111713867187504\n",
            "85 |  avg_price 107.91 loss -22.847499999999997\n",
            "34 |  avg_price 42.84 loss -8.607124176025394\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.29  94.63  42.84 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "143 |  avg_price 181.34 loss -37.87799438476563\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.04 217.14  91.98 197.57 107.91 145.16  37.54  73.42 116.39 181.34 193.95 157.27  95.7   49.54 126.25  94.63  42.83 164.88 129.51  72.5  151.17  65.6  108.05 110.73 275.53 180.32  46.71  45.8  112.16]\n",
            "[ 67.35 217.14  91.49 197.57 108.05 145.43  37.65  73.42 115.6  179.13 195.35 157.27  95.79  49.68 126.4   93.83  42.79 164.88 129.81  72.5  151.47  65.6  108.05 110.73 275.53 180.32  46.74  45.8  112.16]\n",
            "[ 67.66 217.14  94.26 197.57 112.04 145.76  38.16  73.42 118.29 185.6  196.2  157.27  95.96  49.81 126.47  97.01  43.06 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.66 217.14  96.48 197.57 115.03 146.19  38.59  73.42 120.48 190.43 196.97 157.27  96.11  49.81 126.54  97.01  43.31 164.88 131.1   72.5  151.5   65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "85 |  avg_price 115.03 loss -29.58780029296875\n",
            "30 |  avg_price 38.59 loss -8.60082801818848\n",
            "92 |  avg_price 120.48 loss -28.670002441406254\n",
            "141 |  avg_price 190.43 loss -49.695304565429694\n",
            "[ 67.29 217.14  95.08 197.57 115.03 146.11  38.59  73.42 120.48 190.43 195.22 157.27  95.97  49.58 126.42  95.41  43.29 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  95.08 197.57 115.03 146.11  38.59  73.42 120.48 190.43 195.22 157.27  95.97  49.58 126.42  95.41  43.29 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "154 |  avg_price 195.22 loss -41.31832763671875\n",
            "[ 67.29 217.14  94.07 197.57 110.39 145.78  37.75  73.42 118.05 187.4  195.22 157.27  95.97  49.58 126.32  95.41  43.21 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "94 |  avg_price 118.05 loss -24.520001220703122\n",
            "148 |  avg_price 187.4 loss -39.202612304687506\n",
            "[ 67.29 217.14  94.07 197.57 110.39 145.78  37.75  73.42 118.05 187.4  195.22 157.27  95.97  49.58 126.32  95.41  43.21 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "70 |  avg_price 94.07 loss -23.626465148925774\n",
            "76 |  avg_price 95.41 loss -19.527927551269528\n",
            "[ 67.29 217.14  94.07 197.57 107.91 145.57  37.35  73.42 115.26 180.39 195.22 157.27  95.97  49.58 126.3   95.41  43.13 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "140 |  avg_price 180.39 loss -40.76672424316405\n",
            "[ 67.29 217.14  94.07 197.57 107.91 145.57  37.35  73.42 115.26 180.39 195.22 157.27  95.97  49.58 126.24  95.41  43.13 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "86 |  avg_price 115.26 loss -29.27999664306641\n",
            "[ 67.29 217.14  94.07 197.57 107.91 145.57  37.35  73.42 115.26 180.39 195.22 157.27  95.97  49.58 126.11  95.41  43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "85 |  avg_price 107.91 loss -22.847499999999997\n",
            "34 |  avg_price 43.12 loss -8.887124176025388\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.86  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.86  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.86  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.86  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.86  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.86  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.86  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.86  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.86  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "71 |  avg_price 90.91 loss -20.21922698974609\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.29 217.14  90.91 197.57 107.91 145.5   37.33  73.42 115.26 180.39 195.22 157.27  95.97  49.58 125.85  92.7   43.12 164.88 130.58  72.5  151.35  65.6  108.05 110.73 275.53 180.32  46.79  45.8  112.16]\n",
            "[ 67.53 217.14  90.55 197.57 108.05 145.74  37.45  73.42 114.58 178.39 196.46 157.27  96.06  49.71 125.99  92.18  43.06 164.88 130.79  72.5  151.64  65.6  108.05 110.73 275.53 180.32  46.82  45.8  112.16]\n",
            "[ 67.79 217.14  93.41 197.57 112.04 146.03  37.88  73.42 117.36 185.   197.18 157.27  96.21  49.83 126.06  95.21  43.33 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.79 217.14  95.71 197.57 115.03 146.42  38.27  73.42 119.63 189.92 197.85 157.27  96.35  49.83 126.12  95.21  43.57 164.88 131.89  72.5  151.66  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "85 |  avg_price 115.03 loss -29.58780029296875\n",
            "30 |  avg_price 38.27 loss -8.28082801818848\n",
            "92 |  avg_price 119.63 loss -27.820002441406245\n",
            "141 |  avg_price 189.92 loss -49.185304565429675\n",
            "[ 67.46 217.14  94.37 197.57 115.03 146.34  38.27  73.42 119.63 189.92 196.02 157.27  96.22  49.62 126.02  93.9   43.53 164.88 131.4   72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  94.37 197.57 115.03 146.34  38.27  73.42 119.63 189.92 196.02 157.27  96.22  49.62 126.02  93.9   43.53 164.88 131.4   72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "154 |  avg_price 196.02 loss -42.11832763671876\n",
            "[ 67.46 217.14  93.42 197.57 110.39 146.04  37.6   73.42 117.28 186.92 196.02 157.27  96.22  49.61 125.93  93.9   43.45 164.88 131.4   72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "94 |  avg_price 117.28 loss -23.750001220703126\n",
            "148 |  avg_price 186.92 loss -38.72261230468749\n",
            "[ 67.46 217.14  93.42 197.57 110.39 146.04  37.6   73.42 117.28 186.92 196.02 157.27  96.22  49.61 125.94  93.9   43.45 164.88 131.4   72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "70 |  avg_price 93.42 loss -22.976465148925783\n",
            "[ 67.46 217.14  93.42 197.57 107.91 145.9   37.6   73.42 117.28 186.92 196.02 157.27  96.22  49.61 125.92  93.9   43.36 164.88 131.4   72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  93.42 197.57 107.91 145.9   37.6   73.42 117.28 186.92 196.02 157.27  96.22  49.61 125.92  93.9   43.36 164.88 131.4   72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  93.42 197.57 107.91 145.9   37.6   73.42 117.28 186.92 196.02 157.27  96.22  49.61 125.92  93.9   43.36 164.88 131.4   72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "85 |  avg_price 107.91 loss -22.847499999999997\n",
            "71 |  avg_price 93.9 loss -22.41400756835938\n",
            "34 |  avg_price 43.36 loss -9.12712417602539\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.7   93.9   43.36 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "143 |  avg_price 179.65 loss -36.18799438476563\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.6   93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.6   93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.6   93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.6   93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.6   93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.6   93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.6   93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.6   93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "71 |  avg_price 90.33 loss -19.639226989746092\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "[ 67.46 217.14  90.33 197.57 107.91 145.83  37.1   73.42 114.27 179.65 196.02 157.27  96.22  49.61 125.59  93.9   43.35 164.88 130.42  72.5  151.52  65.6  108.05 110.73 275.53 180.32  46.86  45.8  112.16]\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 16          |\n",
            "|    fps             | 8           |\n",
            "|    time_elapsed    | 151         |\n",
            "|    total_timesteps | 1328        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.19976467 |\n",
            "------------------------------------\n",
            "[ 67.67 217.14  90.03 197.57 108.05 146.05  37.21  73.42 113.74 177.82 197.16 157.27  96.3   49.73 125.72  93.25  43.29 164.88 130.6   72.5  151.79  65.6  108.05 110.73 275.53 180.32  46.89  45.8  112.16]\n",
            "[ 67.89 217.14  92.95 197.57 112.04 146.31  37.6   73.42 116.36 184.52 197.8  157.27  96.45  49.84 125.78  96.19  43.54 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.89 217.14  95.29 197.57 115.03 146.66  37.96  73.42 118.54 189.52 198.41 157.27  96.58  49.84 125.85  96.19  43.77 164.88 131.55  72.5  151.82  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "85 |  avg_price 115.03 loss -29.58780029296875\n",
            "30 |  avg_price 37.96 loss -7.970828018188477\n",
            "92 |  avg_price 118.54 loss -26.730002441406256\n",
            "141 |  avg_price 189.52 loss -48.7853045654297\n",
            "[ 67.6  217.14  93.99 197.57 115.03 146.58  37.96  73.42 118.54 189.52 196.53 157.27  96.44  49.65 125.75  94.79  43.73 164.88 131.14  72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  93.99 197.57 115.03 146.58  37.96  73.42 118.54 189.52 196.53 157.27  96.44  49.65 125.75  94.79  43.73 164.88 131.14  72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "154 |  avg_price 196.53 loss -42.62832763671875\n",
            "[ 67.6  217.14  93.06 197.57 110.39 146.3   37.41  73.42 116.49 186.54 196.53 157.27  96.44  49.64 125.68  94.79  43.64 164.88 131.14  72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "148 |  avg_price 186.54 loss -38.34261230468749\n",
            "[ 67.6  217.14  93.06 197.57 110.39 146.3   37.41  73.42 116.49 186.54 196.53 157.27  96.44  49.64 125.68  94.79  43.64 164.88 131.14  72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "70 |  avg_price 93.06 loss -22.616465148925784\n",
            "89 |  avg_price 116.49 loss -27.689996948242182\n",
            "[ 67.6  217.14  93.06 197.57 107.91 146.13  37.14  73.42 116.49 184.72 196.53 157.27  96.44  49.64 125.67  94.79  43.54 164.88 131.14  72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "140 |  avg_price 184.72 loss -45.09672424316406\n",
            "[ 67.6  217.14  93.06 197.57 107.91 146.13  37.14  73.42 116.49 184.72 196.53 157.27  96.44  49.64 125.66  94.79  43.54 164.88 131.14  72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "76 |  avg_price 94.79 loss -19.260687866210944\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.13  37.14  73.42 116.49 184.72 196.53 157.27  96.44  49.64 125.53  94.79  43.24 164.88 131.14  72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "66 |  avg_price 90.57 loss -24.701713867187493\n",
            "85 |  avg_price 107.62 loss -22.557500000000005\n",
            "34 |  avg_price 43.24 loss -9.007124176025393\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.32  94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "143 |  avg_price 182.33 loss -38.86799438476564\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.6  217.14  90.57 197.57 107.62 146.06  36.77  73.42 113.81 182.33 196.53 157.27  96.44  49.64 125.3   94.79  43.24 164.88 130.3   72.5  151.67  65.6  108.05 110.73 275.53 180.32  46.93  45.8  112.16]\n",
            "[ 67.77 217.14  90.24 197.57 107.81 146.25  36.88  73.42 113.36 179.91 197.61 157.27  96.52  49.75 125.42  94.06  43.18 164.88 130.46  72.5  151.94  65.6  108.05 110.73 275.53 180.32  46.96  45.8  112.16]\n",
            "[ 67.96 217.14  93.14 197.57 111.83 146.49  37.23  73.42 115.82 186.24 198.2  157.27  96.66  49.85 125.48  96.92  43.44 164.88 131.3   72.5  151.96  65.6  108.05 110.73 275.53 180.32  47.    45.8  112.16]\n",
            "[ 67.96 217.14  95.46 197.57 114.85 146.81  37.55  73.42 117.88 190.97 198.77 157.27  96.78  49.86 125.54  96.92  43.67 164.88 131.3   72.5  151.96  65.6  108.05 110.73 275.53 180.32  47.    45.8  112.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2023-05-03 10:35:02,299]\u001b[0m Trial 0 failed with parameters: {'gamma': 0.98, 'tau': 0.12, 'train_freq': 768, 'noise_type': 'ornstein-uhlenbeck', 'noise_std': 0.1, 'net_arch': 'big'} because of the following error: KeyboardInterrupt().\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-53-26bf33718e81>\", line 25, in objective\n",
            "    trained_ddpg = agent.train_model(model=model_ddpg,\n",
            "  File \"/usr/local/lib/python3.10/site-packages/finrl/agents/stablebaselines3/models.py\", line 103, in train_model\n",
            "    model = model.learn(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/stable_baselines3/ddpg/ddpg.py\", line 123, in learn\n",
            "    return super().learn(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/stable_baselines3/td3/td3.py\", line 216, in learn\n",
            "    return super().learn(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 311, in learn\n",
            "    rollout = self.collect_rollouts(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 543, in collect_rollouts\n",
            "    new_obs, rewards, dones, infos = env.step(actions)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\", line 163, in step\n",
            "    return self.step_wait()\n",
            "  File \"/usr/local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\", line 54, in step_wait\n",
            "    obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n",
            "  File \"<ipython-input-42-0d744a4105d2>\", line 386, in step\n",
            "    self.frame.to_csv(\"test.csv\")\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pandas/core/generic.py\", line 3720, in to_csv\n",
            "    return DataFrameRenderer(formatter).to_csv(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pandas/io/formats/format.py\", line 1189, in to_csv\n",
            "    csv_formatter.save()\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pandas/io/formats/csvs.py\", line 261, in save\n",
            "    self._save()\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pandas/io/formats/csvs.py\", line 266, in _save\n",
            "    self._save_body()\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pandas/io/formats/csvs.py\", line 304, in _save_body\n",
            "    self._save_chunk(start_i, end_i)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pandas/io/formats/csvs.py\", line 311, in _save_chunk\n",
            "    res = df._mgr.to_native_types(**self._number_format)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pandas/core/internals/managers.py\", line 502, in to_native_types\n",
            "    return self.apply(\"to_native_types\", **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pandas/core/internals/managers.py\", line 352, in apply\n",
            "    applied = getattr(b, f)(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pandas/core/internals/blocks.py\", line 541, in to_native_types\n",
            "    result = to_native_types(self.values, na_rep=na_rep, quoting=quoting, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/pandas/core/internals/blocks.py\", line 2328, in to_native_types\n",
            "    values = values.astype(object, copy=False)\n",
            "KeyboardInterrupt\n",
            "\u001b[33m[W 2023-05-03 10:35:02,301]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 67.96 217.14  95.46 197.57 114.85 146.81  37.55  73.42 117.88 190.97 198.77 157.27  96.78  49.86 125.54  96.92  43.67 164.88 131.3   72.5  151.96  65.6  108.05 110.73 275.53 180.32  47.    45.8  112.16]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-26bf33718e81>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m                                    trial_number=lc_trial_number)\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#You can increase the n_trials for a better search space scanning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogging_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-26bf33718e81>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;31m#You can increase it for better comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   trained_ddpg = agent.train_model(model=model_ddpg,\n\u001b[0m\u001b[1;32m     26\u001b[0m                                    \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ddpg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                    total_timesteps=total_timesteps)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/finrl/agents/stablebaselines3/models.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         model = model.learn(\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/ddpg/ddpg.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     ) -> SelfDDPG:\n\u001b[0;32m--> 123\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     ) -> SelfTD3:\n\u001b[0;32m--> 216\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             rollout = self.collect_rollouts(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mtrain_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;31m# Rescale and perform action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n",
            "\u001b[0;32m<ipython-input-42-0d744a4105d2>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3718\u001b[0m         )\n\u001b[1;32m   3719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3720\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3721\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3722\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         )\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m             )\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mto_native_types\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0mformatting\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \"\"\"\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"to_native_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mto_native_types\u001b[0;34m(self, na_rep, quoting, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;34m\"\"\"convert to our native types format\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquoting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mto_native_types\u001b[0;34m(values, na_rep, quoting, float_format, decimal, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2329\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(study, \"final_ddpg_study__.pkl\")"
      ],
      "metadata": {
        "id": "1yIXC6W2ZqPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the best hyperparamters\n",
        "print('Hyperparameters after tuning',study.best_params)\n",
        "print('Hyperparameters before tuning',config.DDPG_PARAMS)"
      ],
      "metadata": {
        "id": "gXzfenQTdqq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad76d6e8-c0ff-4d10-d0f2-efd5b0972e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters after tuning {'gamma': 0.98, 'tau': 0.04, 'train_freq': 1024, 'noise_type': 'normal', 'noise_std': 0.4, 'net_arch': 'big'}\n",
            "Hyperparameters before tuning {'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_trial"
      ],
      "metadata": {
        "id": "VQcuj-yidu0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0496257d-abd8-4566-b571-8cecf333c4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenTrial(number=4, state=TrialState.COMPLETE, values=[1.3926463459776786], datetime_start=datetime.datetime(2023, 5, 2, 14, 40, 26, 156017), datetime_complete=datetime.datetime(2023, 5, 2, 14, 40, 42, 543469), params={'gamma': 0.98, 'tau': 0.04, 'train_freq': 1024, 'noise_type': 'normal', 'noise_std': 0.4, 'net_arch': 'big'}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'gamma': CategoricalDistribution(choices=(0.9, 0.92, 0.94, 0.96, 0.98)), 'tau': CategoricalDistribution(choices=(0.02, 0.04, 0.06, 0.08, 0.1, 0.12)), 'train_freq': CategoricalDistribution(choices=(512, 768, 1024)), 'noise_type': CategoricalDistribution(choices=('ornstein-uhlenbeck', 'normal', None)), 'noise_std': CategoricalDistribution(choices=(0.1, 0.2, 0.3, 0.4, 0.5)), 'net_arch': CategoricalDistribution(choices=('small', 'big'))}, trial_id=4, value=None)"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import DDPG\n",
        "tuned_model_ddpg = DDPG.load('models/ddpg_{}.pth'.format(study.best_trial.number),env=env_train)"
      ],
      "metadata": {
        "id": "ZjYe9lOIdw3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trading period account value with tuned model\n",
        "df_account_value_tuned, df_actions_tuned = DRLAgent.DRL_prediction(\n",
        "    model=tuned_model_ddpg, \n",
        "    environment = e_trade_gym)"
      ],
      "metadata": {
        "id": "3bnbx87vdzHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84cd21e-3154-43e3-ba2e-db3271bb3de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hit end!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_account_value_tuned[105:106][\"account_value\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1Y1jjMLf0qF",
        "outputId": "6e205e9b-9567-445f-d6d9-63f680806e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105    533043.809441\n",
              "Name: account_value, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Backtesting with our pruned model\n",
        "print(\"==============Get Backtest Results===========\")\n",
        "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
        "\n",
        "perf_stats_all_tuned = backtest_stats(account_value=df_account_value_tuned)\n",
        "perf_stats_all_tuned = pd.DataFrame(perf_stats_all_tuned)\n",
        "perf_stats_all_tuned.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_tuned_\"+now+'.csv')"
      ],
      "metadata": {
        "id": "FJrv144od3G_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd8aee3-2ce7-4267-ca58-96a66364eba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============Get Backtest Results===========\n",
            "Annual return          0.164324\n",
            "Cumulative returns     0.066088\n",
            "Annual volatility      0.115015\n",
            "Sharpe ratio           1.392646\n",
            "Calmar ratio           5.177199\n",
            "Stability              0.778060\n",
            "Max drawdown          -0.031740\n",
            "Omega ratio            1.269228\n",
            "Sortino ratio          2.187981\n",
            "Skew                        NaN\n",
            "Kurtosis                    NaN\n",
            "Tail ratio             1.171742\n",
            "Daily value at risk   -0.013855\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now train with not tuned hyperaparameters\n",
        "#Default config.ddpg_PARAMS\n",
        "non_tuned_model_ddpg = agent.get_model(\"ddpg\",model_kwargs = config.DDPG_PARAMS )\n",
        "trained_ddpg = agent.train_model(model=non_tuned_model_ddpg, \n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=50000)"
      ],
      "metadata": {
        "id": "asWsWzemd5dk",
        "outputId": "78a3262d-e665-4d79-a47d-a1ccfd892d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cuda device\n",
            "day: 82, episode: 570\n",
            "begin_total_asset: 500000.00\n",
            "end_total_asset: 434938.17\n",
            "total_reward: -65061.83\n",
            "total_cost: 25569.22\n",
            "total_trades: 1611\n",
            "Sharpe: -0.514\n",
            "=================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-5018ead22eec>:354: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  self.avg_price = np.divide(self.total_price,\n",
            "<ipython-input-64-5018ead22eec>:268: RuntimeWarning: invalid value encountered in subtract\n",
            "  actions = np.where(((current_price < (self.avg_price - self.avg_price * .1))& (self.avg_price >0)), self.total_stockss*-1,actions  )\n",
            "<ipython-input-64-5018ead22eec>:354: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  self.avg_price = np.divide(self.total_price,\n",
            "<ipython-input-64-5018ead22eec>:268: RuntimeWarning: invalid value encountered in subtract\n",
            "  actions = np.where(((current_price < (self.avg_price - self.avg_price * .1))& (self.avg_price >0)), self.total_stockss*-1,actions  )\n",
            "<ipython-input-64-5018ead22eec>:354: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  self.avg_price = np.divide(self.total_price,\n",
            "<ipython-input-64-5018ead22eec>:268: RuntimeWarning: invalid value encountered in subtract\n",
            "  actions = np.where(((current_price < (self.avg_price - self.avg_price * .1))& (self.avg_price >0)), self.total_stockss*-1,actions  )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 233        |\n",
            "|    time_elapsed    | 1          |\n",
            "|    total_timesteps | 332        |\n",
            "| train/             |            |\n",
            "|    actor_loss      | -21.1      |\n",
            "|    critic_loss     | 2.07e+03   |\n",
            "|    learning_rate   | 0.001      |\n",
            "|    n_updates       | 166        |\n",
            "|    reward          | -0.8715689 |\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-5018ead22eec>:354: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  self.avg_price = np.divide(self.total_price,\n",
            "<ipython-input-64-5018ead22eec>:268: RuntimeWarning: invalid value encountered in subtract\n",
            "  actions = np.where(((current_price < (self.avg_price - self.avg_price * .1))& (self.avg_price >0)), self.total_stockss*-1,actions  )\n",
            "<ipython-input-64-5018ead22eec>:354: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  self.avg_price = np.divide(self.total_price,\n",
            "<ipython-input-64-5018ead22eec>:268: RuntimeWarning: invalid value encountered in subtract\n",
            "  actions = np.where(((current_price < (self.avg_price - self.avg_price * .1))& (self.avg_price >0)), self.total_stockss*-1,actions  )\n",
            "<ipython-input-64-5018ead22eec>:354: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  self.avg_price = np.divide(self.total_price,\n",
            "<ipython-input-64-5018ead22eec>:268: RuntimeWarning: invalid value encountered in subtract\n",
            "  actions = np.where(((current_price < (self.avg_price - self.avg_price * .1))& (self.avg_price >0)), self.total_stockss*-1,actions  )\n",
            "<ipython-input-64-5018ead22eec>:354: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  self.avg_price = np.divide(self.total_price,\n",
            "<ipython-input-64-5018ead22eec>:268: RuntimeWarning: invalid value encountered in subtract\n",
            "  actions = np.where(((current_price < (self.avg_price - self.avg_price * .1))& (self.avg_price >0)), self.total_stockss*-1,actions  )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 8          |\n",
            "|    fps             | 176        |\n",
            "|    time_elapsed    | 3          |\n",
            "|    total_timesteps | 664        |\n",
            "| train/             |            |\n",
            "|    actor_loss      | -15.6      |\n",
            "|    critic_loss     | 3.43e+03   |\n",
            "|    learning_rate   | 0.001      |\n",
            "|    n_updates       | 498        |\n",
            "|    reward          | -0.8715689 |\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-5018ead22eec>:354: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  self.avg_price = np.divide(self.total_price,\n",
            "<ipython-input-64-5018ead22eec>:268: RuntimeWarning: invalid value encountered in subtract\n",
            "  actions = np.where(((current_price < (self.avg_price - self.avg_price * .1))& (self.avg_price >0)), self.total_stockss*-1,actions  )\n",
            "<ipython-input-64-5018ead22eec>:354: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  self.avg_price = np.divide(self.total_price,\n",
            "<ipython-input-64-5018ead22eec>:268: RuntimeWarning: invalid value encountered in subtract\n",
            "  actions = np.where(((current_price < (self.avg_price - self.avg_price * .1))& (self.avg_price >0)), self.total_stockss*-1,actions  )\n",
            "<ipython-input-64-5018ead22eec>:354: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  self.avg_price = np.divide(self.total_price,\n",
            "<ipython-input-64-5018ead22eec>:268: RuntimeWarning: invalid value encountered in subtract\n",
            "  actions = np.where(((current_price < (self.avg_price - self.avg_price * .1))& (self.avg_price >0)), self.total_stockss*-1,actions  )\n",
            "<ipython-input-64-5018ead22eec>:354: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  self.avg_price = np.divide(self.total_price,\n",
            "<ipython-input-64-5018ead22eec>:268: RuntimeWarning: invalid value encountered in subtract\n",
            "  actions = np.where(((current_price < (self.avg_price - self.avg_price * .1))& (self.avg_price >0)), self.total_stockss*-1,actions  )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day: 82, episode: 580\n",
            "begin_total_asset: 500000.00\n",
            "end_total_asset: 498030.54\n",
            "total_reward: -1969.46\n",
            "total_cost: 703.88\n",
            "total_trades: 1231\n",
            "Sharpe: 0.281\n",
            "=================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-3b1b0264cc27>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Default config.ddpg_PARAMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnon_tuned_model_ddpg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ddpg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDDPG_PARAMS\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m trained_ddpg = agent.train_model(model=non_tuned_model_ddpg, \n\u001b[0m\u001b[1;32m      5\u001b[0m                              \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ddpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                              total_timesteps=50000)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/finrl/agents/stablebaselines3/models.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         model = model.learn(\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/ddpg/ddpg.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     ) -> SelfDDPG:\n\u001b[0;32m--> 123\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     ) -> SelfTD3:\n\u001b[0;32m--> 216\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0;31m# Special case when the user passes `gradient_steps=0`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mpolyak_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mpolyak_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m                 \u001b[0;31m# Copy running stats, see GH issue #996\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mpolyak_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_batch_norm_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_batch_norm_stats_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/common/utils.py\u001b[0m in \u001b[0;36mpolyak_update\u001b[0;34m(params, target_params, tau)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;31m# zip does not raise an exception if length of parameters does not match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_param\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/common/utils.py\u001b[0m in \u001b[0;36mzip_strict\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0msentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcombo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip_longest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iterables have different lengths\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mcombo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_account_value, df_actions = DRLAgent.DRL_prediction(\n",
        "    model=trained_ddpg, \n",
        "    environment = e_trade_gym)"
      ],
      "metadata": {
        "id": "mv1zgwOgd9FY",
        "outputId": "3fd9a94a-98d4-41c0-8870-74b8271b6494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-38368194789e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df_account_value, df_actions = DRLAgent.DRL_prediction(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrained_ddpg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     environment = e_trade_gym)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trained_ddpg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Backtesting for not tuned hyperparamters\n",
        "print(\"==============Get Backtest Results===========\")\n",
        "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
        "\n",
        "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
        "perf_stats_all = pd.DataFrame(perf_stats_all)\n",
        "# perf_stats_all.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_\"+now+'.csv')"
      ],
      "metadata": {
        "id": "DuZL52qDeEz6",
        "outputId": "f4e9cd19-27cd-4dba-956e-353df0dabffa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============Get Backtest Results===========\n",
            "Annual return          0.247826\n",
            "Cumulative returns     0.368416\n",
            "Annual volatility      0.167226\n",
            "Sharpe ratio           1.412093\n",
            "Calmar ratio           2.531845\n",
            "Stability              0.897149\n",
            "Max drawdown          -0.097883\n",
            "Omega ratio            1.280406\n",
            "Sortino ratio          2.010344\n",
            "Skew                        NaN\n",
            "Kurtosis                    NaN\n",
            "Tail ratio             1.033938\n",
            "Daily value at risk   -0.020131\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#You can see with trial, our sharpe ratio is increasing\n",
        "#Certainly you can afford more number of trials for further optimization\n",
        "from optuna.visualization import plot_optimization_history\n",
        "plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "5aSsEmpZeI5C",
        "outputId": "cba8ec8c-7654-44d7-88cb-bd6e1bad4a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.20.0.min.js\"></script>                <div id=\"565b1505-44ed-4588-989d-e42e52b774ca\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"565b1505-44ed-4588-989d-e42e52b774ca\")) {                    Plotly.newPlot(                        \"565b1505-44ed-4588-989d-e42e52b774ca\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\"y\":[-0.49080299778360714,-0.3950000677893763,1.3011854021414007,-0.8267805987990207,1.3926463459776786,0.26111723817257965,-0.9640129324129495,0.2819918104525999,1.0018988775173625,-0.2701595920840011,1.1243951293107357,-0.40659941768702645,0.25085630521807395,0.2654094534716406,1.3315265826000702,-0.7288319825005803,0.25514408327043286,-1.0674379403572838,-0.6986206079122625,0.3810367303898103,-0.471019724578347,0.748178567824751],\"type\":\"scatter\"},{\"name\":\"Best Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\"y\":[-0.49080299778360714,-0.3950000677893763,1.3011854021414007,1.3011854021414007,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786,1.3926463459776786],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"Trial\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('565b1505-44ed-4588-989d-e42e52b774ca');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from optuna.visualization import plot_contour\n",
        "from optuna.visualization import plot_edf\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_parallel_coordinate\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_slice"
      ],
      "metadata": {
        "id": "K9zcey_BeJMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparamters importance\n",
        "#Ent_coef is the most important\n",
        "plot_param_importances(study)"
      ],
      "metadata": {
        "id": "J6WmB5DneOgh",
        "outputId": "2d916588-ae89-4970-f430-249ef6b5c81a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.20.0.min.js\"></script>                <div id=\"65abb362-3b70-4093-bbd4-fd5b77123c9e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"65abb362-3b70-4093-bbd4-fd5b77123c9e\")) {                    Plotly.newPlot(                        \"65abb362-3b70-4093-bbd4-fd5b77123c9e\",                        [{\"cliponaxis\":false,\"hovertemplate\":[\"net_arch (CategoricalDistribution): 0.029499975293024668<extra></extra>\",\"train_freq (CategoricalDistribution): 0.0539951123903899<extra></extra>\",\"noise_type (CategoricalDistribution): 0.1655824179588198<extra></extra>\",\"tau (CategoricalDistribution): 0.1773850893223912<extra></extra>\",\"noise_std (CategoricalDistribution): 0.19437196725757405<extra></extra>\",\"gamma (CategoricalDistribution): 0.37916543777780054<extra></extra>\"],\"marker\":{\"color\":\"rgb(66,146,198)\"},\"orientation\":\"h\",\"text\":[\"0.03\",\"0.05\",\"0.17\",\"0.18\",\"0.19\",\"0.38\"],\"textposition\":\"outside\",\"x\":[0.029499975293024668,0.0539951123903899,0.1655824179588198,0.1773850893223912,0.19437196725757405,0.37916543777780054],\"y\":[\"net_arch\",\"train_freq\",\"noise_type\",\"tau\",\"noise_std\",\"gamma\"],\"type\":\"bar\"}],                        {\"showlegend\":false,\"title\":{\"text\":\"Hyperparameter Importances\"},\"xaxis\":{\"title\":{\"text\":\"Importance for Objective Value\"}},\"yaxis\":{\"title\":{\"text\":\"Hyperparameter\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('65abb362-3b70-4093-bbd4-fd5b77123c9e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "    # matplotlib.use('Agg')\n",
        "    import datetime\n",
        "\n",
        "\n",
        "    from finrl.config_tickers import DOW_30_TICKER\n",
        "    from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "    from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "    from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "    from finrl.agents.stablebaselines3.models import DRLAgent, DRLEnsembleAgent\n",
        "    from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
        "\n",
        "    from pprint import pprint\n",
        "\n",
        "    import sys\n",
        "    sys.path.append(\"../FinRL-Library\")\n",
        "\n",
        "    import itertools\n",
        "\n",
        "    import os\n",
        "    from finrl.main import check_and_make_directories\n",
        "    from finrl.config import (\n",
        "        DATA_SAVE_DIR,\n",
        "        TRAINED_MODEL_DIR,\n",
        "        TENSORBOARD_LOG_DIR,\n",
        "        RESULTS_DIR,\n",
        "        INDICATORS,\n",
        "        TRAIN_START_DATE,\n",
        "        TRAIN_END_DATE,\n",
        "        TEST_START_DATE,\n",
        "        TEST_END_DATE,\n",
        "        TRADE_START_DATE,\n",
        "        TRADE_END_DATE,\n",
        "    )\n",
        "\n",
        "    check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])\n",
        "    print(DOW_30_TICKER)\n",
        "    TRAIN_START_DATE = '2009-04-01'\n",
        "    TRAIN_END_DATE = '2021-01-01'\n",
        "    TEST_START_DATE = '2021-01-01'\n",
        "    TEST_END_DATE = '2022-06-01'\n",
        "\n",
        "    df = YahooDownloader(start_date=TRAIN_START_DATE,\n",
        "                         end_date=TEST_END_DATE,\n",
        "                         ticker_list=DOW_30_TICKER).fetch_data()\n",
        "\n",
        "    df.sort_values(['date', 'tic']).head()\n",
        "\n",
        "    fe = FeatureEngineer(use_technical_indicator=True,\n",
        "                         tech_indicator_list=INDICATORS,\n",
        "                         use_turbulence=True,\n",
        "                         user_defined_feature=False)\n",
        "\n",
        "    processed = fe.preprocess_data(df)\n",
        "    processed = processed.copy()\n",
        "    processed = processed.fillna(0)\n",
        "    processed = processed.replace(np.inf, 0)\n",
        "\n",
        "    stock_dimension = len(processed.tic.unique())\n",
        "    state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension\n",
        "    print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
        "\n",
        "    env_kwargs = {\n",
        "        \"hmax\": 100,\n",
        "        \"initial_amount\": 1000000,\n",
        "        \"buy_cost_pct\": 0.001,\n",
        "        \"sell_cost_pct\": 0.001,\n",
        "        \"state_space\": state_space,\n",
        "        \"stock_dim\": stock_dimension,\n",
        "        \"tech_indicator_list\": INDICATORS,\n",
        "        \"action_space\": stock_dimension,\n",
        "        \"reward_scaling\": 1e-4,\n",
        "        \"print_verbosity\": 5\n",
        "\n",
        "    }\n",
        "\n",
        "    rebalance_window = 63  # rebalance_window is the number of days to retrain the model\n",
        "    validation_window = 63  # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
        "\n",
        "    ensemble_agent = DRLEnsembleAgent(df=processed,\n",
        "                                      train_period=(TRAIN_START_DATE, TRAIN_END_DATE),\n",
        "                                      val_test_period=(TEST_START_DATE, TEST_END_DATE),\n",
        "                                      rebalance_window=rebalance_window,\n",
        "                                      validation_window=validation_window,\n",
        "                                      **env_kwargs)\n",
        "\n",
        "    A2C_model_kwargs = {\n",
        "        'n_steps': 5,\n",
        "        'ent_coef': 0.005,\n",
        "        'learning_rate': 0.0007\n",
        "    }\n",
        "\n",
        "    PPO_model_kwargs = {\n",
        "        \"ent_coef\": 0.01,\n",
        "        \"n_steps\": 2048,\n",
        "        \"learning_rate\": 0.00025,\n",
        "        \"batch_size\": 128\n",
        "    }\n",
        "\n",
        "    DDPG_model_kwargs = {\n",
        "        # \"action_noise\":\"ornstein_uhlenbeck\",\n",
        "        \"buffer_size\": 10_000,\n",
        "        \"learning_rate\": 0.0005,\n",
        "        \"batch_size\": 64\n",
        "    }\n",
        "\n",
        "    timesteps_dict = {'a2c': 10_000,\n",
        "                      'ppo': 10_000,\n",
        "                      'ddpg': 10_000\n",
        "                      }\n",
        "    df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
        "                                                      PPO_model_kwargs,\n",
        "                                                      DDPG_model_kwargs,\n",
        "                                                      timesteps_dict)\n",
        "\n",
        "    unique_trade_date = processed[(processed.date > TEST_START_DATE) & (processed.date <= TEST_END_DATE)].date.unique()\n",
        "\n",
        "    df_trade_date = pd.DataFrame({'datadate': unique_trade_date})\n",
        "\n",
        "    df_account_value = pd.DataFrame()\n",
        "    for i in range(rebalance_window + validation_window, len(unique_trade_date) + 1, rebalance_window):\n",
        "        temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble', i))\n",
        "        df_account_value = df_account_value.append(temp, ignore_index=True)\n",
        "    sharpe = (252 ** 0.5) * df_account_value.account_value.pct_change(\n",
        "        1).mean() / df_account_value.account_value.pct_change(1).std()\n",
        "    print('Sharpe Ratio: ', sharpe)\n",
        "    df_account_value = df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))\n",
        "\n",
        "    df_account_value.account_value.plot()\n",
        "\n",
        "    print(\"==============Get Backtest Results===========\")\n",
        "    now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
        "\n",
        "    perf_stats_all = backtest_stats(account_value=df_account_value)\n",
        "    perf_stats_all = pd.DataFrame(perf_stats_all)\n",
        "\n",
        "    # baseline stats\n",
        "    print(\"==============Get Baseline Stats===========\")\n",
        "    baseline_df = get_baseline(\n",
        "        ticker=\"^DJI\",\n",
        "        start=df_account_value.loc[0, 'date'],\n",
        "        end=df_account_value.loc[len(df_account_value) - 1, 'date'])\n",
        "\n",
        "    stats = backtest_stats(baseline_df, value_col_name='close')\n",
        "\n",
        "    print(\"==============Compare to DJIA===========\")\n",
        "\n",
        "    # S&P 500: ^GSPC\n",
        "    # Dow Jones Index: ^DJI\n",
        "    # NASDAQ 100: ^NDX\n",
        "    backtest_plot(df_account_value,\n",
        "                  baseline_ticker='^DJI',\n",
        "                  baseline_start=df_account_value.loc[0, 'date'],\n",
        "                  baseline_end=df_account_value.loc[len(df_account_value) - 1, 'date'])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "s_xcVpBSeOyE",
        "outputId": "52b3d82a-1f8d-4af4-a405-6b5b9778781d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Shape of DataFrame:  (96942, 8)\n",
            "Successfully added technical indicators\n",
            "Successfully added turbulence index\n",
            "Stock Dimension: 29, State Space: 291\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  203.40117769841157\n",
            "======Model training from:  2009-04-01 to  2021-01-04\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_126_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 100        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0.00295    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -71.2      |\n",
            "|    reward             | 0.78650117 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 12.2       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 101        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0.00111    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 6.59       |\n",
            "|    reward             | 0.66569686 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 1.03       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 102       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -123      |\n",
            "|    reward             | 2.9206326 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 15.8      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 102        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | -6.35      |\n",
            "|    reward             | -1.8242201 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 3.27       |\n",
            "--------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 102           |\n",
            "|    iterations         | 500           |\n",
            "|    time_elapsed       | 24            |\n",
            "|    total_timesteps    | 2500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -41.2         |\n",
            "|    explained_variance | -1.19e-07     |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 499           |\n",
            "|    policy_loss        | 16            |\n",
            "|    reward             | -0.0061751665 |\n",
            "|    std                | 1             |\n",
            "|    value_loss         | 0.657         |\n",
            "-----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 102       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -33.6     |\n",
            "|    reward             | 0.5337091 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.837     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 102        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 34         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -1.97      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -76.4      |\n",
            "|    reward             | -1.3246601 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.85       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 101        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.0632     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 44.6       |\n",
            "|    reward             | -1.6959082 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.53       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 100      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 44       |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.2    |\n",
            "|    explained_variance | 0.0211   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -33.2    |\n",
            "|    reward             | 3.994608 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 3.53     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 101         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 49          |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 1.79e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 36.3        |\n",
            "|    reward             | -0.20298508 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.68        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 101         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 54          |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | 0.0114      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | 393         |\n",
            "|    reward             | -0.92543656 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 120         |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 101          |\n",
            "|    iterations         | 1200         |\n",
            "|    time_elapsed       | 59           |\n",
            "|    total_timesteps    | 6000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.2        |\n",
            "|    explained_variance | 0.0236       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1199         |\n",
            "|    policy_loss        | 151          |\n",
            "|    reward             | -0.099366836 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 19.4         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 101         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 64          |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | -0.000106   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | -47.6       |\n",
            "|    reward             | -0.39146426 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 3.15        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 101         |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 69          |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | 31.8        |\n",
            "|    reward             | -0.25985184 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.84        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 101       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 74        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -30.8     |\n",
            "|    reward             | 3.1766434 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 4.93      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 101        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 78         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.00673    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 133        |\n",
            "|    reward             | -2.0720153 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 10.9       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 101       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 83        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -84.2     |\n",
            "|    reward             | 3.8356218 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 7         |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 101        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 88         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.063      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 42         |\n",
            "|    reward             | 0.97460705 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.23       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 101       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 93        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.0122    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -84.7     |\n",
            "|    reward             | 2.2624621 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 8.34      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 101        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 98         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 45.9       |\n",
            "|    reward             | -1.0189797 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.77       |\n",
            "--------------------------------------\n",
            "======A2C Validation from:  2021-01-04 to  2021-04-06\n",
            "A2C Sharpe Ratio:  0.26168708794170054\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_126_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 111       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 18        |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1.8877938 |\n",
            "----------------------------------\n",
            "day: 2959, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3881161.61\n",
            "total_reward: 2881161.61\n",
            "total_cost: 376867.37\n",
            "total_trades: 82873\n",
            "Sharpe: 0.776\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 37          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016203282 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0104     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4           |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0191     |\n",
            "|    reward               | 1.0358981   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 11.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 56          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012528071 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00217    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 57          |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0176     |\n",
            "|    reward               | 0.07821791  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 55.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 107         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 76          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012139827 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00454    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.7        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.018      |\n",
            "|    reward               | -1.5925834  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 43.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 107         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 94          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019309906 |\n",
            "|    clip_fraction        | 0.17        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.0161     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.07        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0225     |\n",
            "|    reward               | 0.0615912   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 11.3        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2021-01-04 to  2021-04-06\n",
            "PPO Sharpe Ratio:  0.3016310407987196\n",
            "======DDPG Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_1\n",
            "day: 2959, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4424085.63\n",
            "total_reward: 3424085.63\n",
            "total_cost: 1037.34\n",
            "total_trades: 50241\n",
            "Sharpe: 0.797\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 78        |\n",
            "|    time_elapsed    | 151       |\n",
            "|    total_timesteps | 11840     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -60.3     |\n",
            "|    critic_loss     | 441       |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8880      |\n",
            "|    reward          | 3.0541244 |\n",
            "----------------------------------\n",
            "======DDPG Validation from:  2021-01-04 to  2021-04-06\n",
            "======Best Model Retraining from:  2009-04-01 to  2021-04-06\n",
            "======Trading from:  2021-04-06 to  2021-07-06\n",
            "============================================\n",
            "turbulence_threshold:  203.40117769841157\n",
            "======Model training from:  2009-04-01 to  2021-04-06\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_189_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 99         |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -0.446     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -85.9      |\n",
            "|    reward             | -0.6058027 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 9.16       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 100       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | -19.4     |\n",
            "|    reward             | 1.8647146 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 0.253     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 100      |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41      |\n",
            "|    explained_variance | 0.0122   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | -179     |\n",
            "|    reward             | 3.291392 |\n",
            "|    std                | 0.996    |\n",
            "|    value_loss         | 22.1     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 100       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0.0507    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -11.1     |\n",
            "|    reward             | 0.3743614 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 1.09      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 100        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 24         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 169        |\n",
            "|    reward             | -0.3291204 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 16         |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 100        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 29         |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0.054      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 905        |\n",
            "|    reward             | -1.0711567 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 521        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 100        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 34         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -0.0433    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -32.9      |\n",
            "|    reward             | -2.9666545 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 0.946      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 100        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -3.25      |\n",
            "|    reward             | -0.5585839 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 1.05       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 100       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -40.9     |\n",
            "|    explained_variance | -0.66     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 306       |\n",
            "|    reward             | 2.3080251 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 70.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 100       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 49        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -108      |\n",
            "|    reward             | 1.1761196 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 10.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 100       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 54        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -117      |\n",
            "|    reward             | 3.2501192 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 10.7      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 100       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 59        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 362       |\n",
            "|    reward             | 6.932239  |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 84.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 100        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 64         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 44.8       |\n",
            "|    reward             | -1.5383602 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 2.08       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 100       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 69        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -143      |\n",
            "|    reward             | 0.7461217 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 17        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 100        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 74         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | -145       |\n",
            "|    reward             | -1.6722031 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 14.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 100       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 79        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 4.35      |\n",
            "|    reward             | 3.1992574 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 0.916     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 100       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 84        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -40.9     |\n",
            "|    explained_variance | -0.00566  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -1.16e+03 |\n",
            "|    reward             | 11.774156 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 936       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 100      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 89       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -40.9    |\n",
            "|    explained_variance | -0.00464 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | -185     |\n",
            "|    reward             | 4.000574 |\n",
            "|    std                | 0.993    |\n",
            "|    value_loss         | 41.6     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 100        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 94         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -40.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 12.7       |\n",
            "|    reward             | -1.3208354 |\n",
            "|    std                | 0.991      |\n",
            "|    value_loss         | 0.306      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 100       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 99        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -40.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -10.6     |\n",
            "|    reward             | 0.7043803 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 0.431     |\n",
            "-------------------------------------\n",
            "======A2C Validation from:  2021-04-06 to  2021-07-06\n",
            "A2C Sharpe Ratio:  0.23121777505990648\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 104       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 19        |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1.1126131 |\n",
            "----------------------------------\n",
            "day: 3022, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3636477.12\n",
            "total_reward: 2636477.12\n",
            "total_cost: 386575.15\n",
            "total_trades: 84625\n",
            "Sharpe: 0.731\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 105         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 38          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015871149 |\n",
            "|    clip_fraction        | 0.234       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0129     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.2         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0282     |\n",
            "|    reward               | 0.88015693  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 10.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 105         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 58          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019049045 |\n",
            "|    clip_fraction        | 0.166       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.00262     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.1        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.023      |\n",
            "|    reward               | 0.49640504  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 53.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 105         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 77          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015561214 |\n",
            "|    clip_fraction        | 0.161       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0133     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 31.9        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0201     |\n",
            "|    reward               | -0.7624933  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 45.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 105         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 96          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014756277 |\n",
            "|    clip_fraction        | 0.141       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00517    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.56        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0204     |\n",
            "|    reward               | 1.2296445   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 14.8        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2021-04-06 to  2021-07-06\n",
            "PPO Sharpe Ratio:  0.010011405217654536\n",
            "======DDPG Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
            "day: 3022, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5009064.34\n",
            "total_reward: 4009064.34\n",
            "total_cost: 1282.47\n",
            "total_trades: 51375\n",
            "Sharpe: 0.880\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 76       |\n",
            "|    time_elapsed    | 157      |\n",
            "|    total_timesteps | 12092    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -65.5    |\n",
            "|    critic_loss     | 3.14e+03 |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9069     |\n",
            "|    reward          | 8.301459 |\n",
            "---------------------------------\n",
            "======DDPG Validation from:  2021-04-06 to  2021-07-06\n",
            "======Best Model Retraining from:  2009-04-01 to  2021-07-06\n",
            "======Trading from:  2021-07-06 to  2021-10-04\n",
            "============================================\n",
            "turbulence_threshold:  203.40117769841157\n",
            "======Model training from:  2009-04-01 to  2021-07-06\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_252_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | -0.0351     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -28.6       |\n",
            "|    reward             | -0.15436654 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 5.15        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 50.5      |\n",
            "|    reward             | 1.3676119 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 2.12      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -179      |\n",
            "|    reward             | 5.1952586 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 22.2      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 20         |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | 79.1       |\n",
            "|    reward             | -0.3978011 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 7.82       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 25         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -40.9      |\n",
            "|    explained_variance | 0.19       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -8.81      |\n",
            "|    reward             | 0.19138733 |\n",
            "|    std                | 0.993      |\n",
            "|    value_loss         | 6.38       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 30         |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -0.00925   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -498       |\n",
            "|    reward             | -11.956305 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 222        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 35         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -0.169     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -68.7      |\n",
            "|    reward             | 0.39965674 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 3.86       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 40          |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41         |\n",
            "|    explained_variance | -0.0865     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -164        |\n",
            "|    reward             | -0.33571205 |\n",
            "|    std                | 0.996       |\n",
            "|    value_loss         | 16.1        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 45          |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41         |\n",
            "|    explained_variance | 0.159       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | 46.5        |\n",
            "|    reward             | -0.19355251 |\n",
            "|    std                | 0.996       |\n",
            "|    value_loss         | 1.99        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 50          |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | 0.0618      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | -15.1       |\n",
            "|    reward             | -0.48996487 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 0.677       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 55        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | -0.192    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -233      |\n",
            "|    reward             | 1.1524396 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 35.2      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 60        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -0.214    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 14.9      |\n",
            "|    reward             | -2.838138 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 3.95      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 65         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.167      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 43.7       |\n",
            "|    reward             | -1.0078373 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.86       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 70        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.0727   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -17.9     |\n",
            "|    reward             | 2.1590974 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.71      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 75         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 42.5       |\n",
            "|    reward             | 0.47163484 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.17       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 80        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.000262  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -73.2     |\n",
            "|    reward             | 0.8027305 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 19.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 85        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -184      |\n",
            "|    reward             | 0.3686392 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 29.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 90        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 116       |\n",
            "|    reward             | 1.6162095 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 21.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 96        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.00951  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -1.74     |\n",
            "|    reward             | 1.1433389 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.195     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 101        |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.0113    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -38.9      |\n",
            "|    reward             | 0.97623837 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.806      |\n",
            "--------------------------------------\n",
            "======A2C Validation from:  2021-07-06 to  2021-10-04\n",
            "A2C Sharpe Ratio:  -0.0528373614471615\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 101       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 20        |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1.2801732 |\n",
            "----------------------------------\n",
            "day: 3085, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4550937.53\n",
            "total_reward: 3550937.53\n",
            "total_cost: 408098.28\n",
            "total_trades: 86397\n",
            "Sharpe: 0.873\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 39          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018978704 |\n",
            "|    clip_fraction        | 0.233       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00994    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.48        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0284     |\n",
            "|    reward               | 0.49718028  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 10.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 59          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012346724 |\n",
            "|    clip_fraction        | 0.155       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00278    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 19.6        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0154     |\n",
            "|    reward               | -0.8982904  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 50.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014507987 |\n",
            "|    clip_fraction        | 0.123       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.012      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 43.8        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.016      |\n",
            "|    reward               | -0.0212465  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 74.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 104         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 98          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016703494 |\n",
            "|    clip_fraction        | 0.186       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.0496     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.09        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0285     |\n",
            "|    reward               | 1.0109037   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 14.2        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2021-07-06 to  2021-10-04\n",
            "PPO Sharpe Ratio:  -0.05262682094979255\n",
            "======DDPG Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
            "day: 3085, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4501430.61\n",
            "total_reward: 3501430.61\n",
            "total_cost: 1082.97\n",
            "total_trades: 61741\n",
            "Sharpe: 0.737\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 76        |\n",
            "|    time_elapsed    | 161       |\n",
            "|    total_timesteps | 12344     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.66     |\n",
            "|    critic_loss     | 176       |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 9258      |\n",
            "|    reward          | 2.1645079 |\n",
            "----------------------------------\n",
            "======DDPG Validation from:  2021-07-06 to  2021-10-04\n",
            "======Best Model Retraining from:  2009-04-01 to  2021-10-04\n",
            "======Trading from:  2021-10-04 to  2022-01-03\n",
            "============================================\n",
            "turbulence_threshold:  203.40117769841157\n",
            "======Model training from:  2009-04-01 to  2021-10-04\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_315_1\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 98           |\n",
            "|    iterations         | 100          |\n",
            "|    time_elapsed       | 5            |\n",
            "|    total_timesteps    | 500          |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.3        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 99           |\n",
            "|    policy_loss        | -51.2        |\n",
            "|    reward             | -0.096114956 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 4.56         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | -36.9     |\n",
            "|    reward             | 2.9646184 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.756     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -236      |\n",
            "|    reward             | 1.1201329 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 36.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 20         |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | -53.9      |\n",
            "|    reward             | 0.03644575 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.41       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 34.1      |\n",
            "|    reward             | 0.3948357 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.08      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 30         |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -618       |\n",
            "|    reward             | -12.502208 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 338        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 35         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.0164    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -105       |\n",
            "|    reward             | 0.30092302 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 10.8       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 40          |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | 74.6        |\n",
            "|    reward             | 0.009111182 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 4.73        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 45        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -0.0124   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -131      |\n",
            "|    reward             | 2.0478728 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 16.5      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 50         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -34.9      |\n",
            "|    reward             | 0.04952446 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.24       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 55          |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | -0.0188     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | 344         |\n",
            "|    reward             | -0.11844914 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 64.7        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 60          |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | 58.9        |\n",
            "|    reward             | -0.28185675 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 6.99        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 66         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0.549      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -11.7      |\n",
            "|    reward             | -0.7957832 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.135      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 71         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -29.7      |\n",
            "|    reward             | 0.16117185 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.859      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 76         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 15.7       |\n",
            "|    reward             | 0.17504984 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.528      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 81        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 48.7      |\n",
            "|    reward             | 3.1540215 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 6.42      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 86          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | 22.9        |\n",
            "|    reward             | -0.09754947 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.677       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 91        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.171    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 99        |\n",
            "|    reward             | -1.175583 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 9.98      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 98       |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 96       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -24.6    |\n",
            "|    reward             | 0.362812 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.367    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 101        |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -146       |\n",
            "|    reward             | -1.7740997 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 13.5       |\n",
            "--------------------------------------\n",
            "======A2C Validation from:  2021-10-04 to  2022-01-03\n",
            "A2C Sharpe Ratio:  0.3744625013790845\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_315_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 106       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 19        |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 0.7561091 |\n",
            "----------------------------------\n",
            "day: 3148, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4259003.47\n",
            "total_reward: 3259003.47\n",
            "total_cost: 426068.70\n",
            "total_trades: 87901\n",
            "Sharpe: 0.790\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 101         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 40          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016951367 |\n",
            "|    clip_fraction        | 0.229       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.000697   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.25        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0307     |\n",
            "|    reward               | 0.9373541   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 9.91        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 60          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017283382 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.00633     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 33.4        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0207     |\n",
            "|    reward               | 7.712371    |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 56.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 79          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010827523 |\n",
            "|    clip_fraction        | 0.0962      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.00682     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 73.4        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.018      |\n",
            "|    reward               | -0.83698237 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 134         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 99          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013603406 |\n",
            "|    clip_fraction        | 0.172       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.0243      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.64        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0243     |\n",
            "|    reward               | 0.5904208   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 12.7        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2021-10-04 to  2022-01-03\n",
            "PPO Sharpe Ratio:  0.0593551480877188\n",
            "======DDPG Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
            "day: 3148, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4617783.42\n",
            "total_reward: 3617783.42\n",
            "total_cost: 1110.92\n",
            "total_trades: 49019\n",
            "Sharpe: 0.745\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 75        |\n",
            "|    time_elapsed    | 167       |\n",
            "|    total_timesteps | 12596     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 36.1      |\n",
            "|    critic_loss     | 656       |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 9447      |\n",
            "|    reward          | 5.4972343 |\n",
            "----------------------------------\n",
            "======DDPG Validation from:  2021-10-04 to  2022-01-03\n",
            "======Best Model Retraining from:  2009-04-01 to  2022-01-03\n",
            "======Trading from:  2022-01-03 to  2022-04-04\n",
            "Ensemble Strategy took:  25.210217595100403  minutes\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4153cda81a48>\u001b[0m in \u001b[0;36m<cell line: 163>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-4153cda81a48>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrebalance_window\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalidation_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_trade_date\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrebalance_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results/account_value_trade_{}_{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ensemble'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mdf_account_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_account_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     sharpe = (252 ** 0.5) * df_account_value.account_value.pct_change(\n\u001b[1;32m    131\u001b[0m         1).mean() / df_account_value.account_value.pct_change(1).std()\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S15p3oh8smm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(50, 126.8078079), (-50, 125.1344223), (59, 125.0349121), (-29, 122.90924835205078), (-26, 123.868042)]\n",
        "\n",
        "total_shares = 0\n",
        "total_price = 0\n",
        "\n",
        "for shares, price in data:\n",
        "    total_shares += shares\n",
        "    total_price += shares * price\n",
        "\n",
        "average_price = total_price / total_shares\n",
        "\n",
        "print(\"Average price of the stock: \", round(average_price, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJO0W8Uxsr2i",
        "outputId": "0397056e-3e73-4968-d87a-74aee73518f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average price of the stock:  168.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(50, 126.8078079), (-50, 125.1344223), (59, 125.0349121), (-29, 122.90924835205078), (-26, 123.868042)]\n",
        "\n",
        "total_shares = 0\n",
        "total_price = 0\n",
        "\n",
        "for shares, price in data:\n",
        "    total_shares += abs(shares)\n",
        "    total_price += shares * price\n",
        "\n",
        "average_price = total_price / total_shares\n",
        "\n",
        "print(\"Average price of the stock: \", round(average_price, 2))"
      ],
      "metadata": {
        "id": "diwuHZ3Q0-fA",
        "outputId": "2729289a-639b-4aea-9f47-2b7ebcd80a36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average price of the stock:  3.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Portfolio:\n",
        "    def __init__(self):\n",
        "        self.num_shares = 0\n",
        "        self.total_cost = 0.0\n",
        "        self.avg_cost = 0.0\n",
        "\n",
        "    def buy(self, num_shares, price):\n",
        "        self.total_cost += num_shares * price\n",
        "        self.num_shares += num_shares\n",
        "        self.avg_cost = self.total_cost / self.num_shares\n",
        "\n",
        "    def sell(self, num_shares):\n",
        "        self.total_cost -= self.avg_cost * num_shares\n",
        "        self.num_shares -= num_shares\n",
        "\n",
        "    def get_num_shares(self):\n",
        "        return self.num_shares\n",
        "\n",
        "    def get_total_cost(self):\n",
        "        return self.total_cost\n",
        "\n",
        "    def get_avg_cost(self):\n",
        "        return self.avg_cost\n",
        "\n",
        "# Example usage\n",
        "portfolio = Portfolio()\n",
        "\n",
        "# Buy 50 shares at $126.81\n",
        "portfolio.buy(50, 126.81)\n",
        "\n",
        "# Sell 25 shares\n",
        "portfolio.sell(25)\n",
        "\n",
        "# Buy 30 shares at $125.03\n",
        "portfolio.buy(30, 125.03)\n",
        "\n",
        "# Sell 10 shares\n",
        "portfolio.sell(10)\n",
        "\n",
        "\n",
        "print(\"Number of shares: \", portfolio.get_num_shares())\n",
        "print(\"Total cost: \", round(portfolio.get_total_cost(), 2))\n",
        "print(\"Average cost: \", round(portfolio.get_avg_cost(), 2))\n"
      ],
      "metadata": {
        "id": "EZuXe0Zo1Nig",
        "outputId": "2a4b1d3d-4f24-45d7-bb62-f781fa9f1df1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of shares:  -25\n",
            "Total cost:  0.0\n",
            "Average cost:  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Portfolio:\n",
        "    def __init__(self):\n",
        "        self.num_shares = 0\n",
        "        self.total_cost = 0.0\n",
        "        self.avg_cost = 0.0\n",
        "\n",
        "    def buy(self, num_shares, price):\n",
        "        self.total_cost += num_shares * price\n",
        "        self.num_shares += num_shares\n",
        "        self.avg_cost = self.total_cost / self.num_shares\n",
        "\n",
        "    def sell(self, num_shares):\n",
        "        self.total_cost -= self.avg_cost * num_shares\n",
        "        self.num_shares -= num_shares\n",
        "\n",
        "    def get_num_shares(self):\n",
        "        return self.num_shares\n",
        "\n",
        "    def get_total_cost(self):\n",
        "        return self.total_cost\n",
        "\n",
        "    def get_avg_cost(self):\n",
        "        return self.avg_cost\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_portfolio(data):\n",
        "        shares, prices = np.array(data).T\n",
        "        abs_shares = np.abs(shares)\n",
        "\n",
        "        total_shares = np.cumsum(shares)\n",
        "        total_cost = np.cumsum(shares * prices)\n",
        "        avg_cost = total_cost / total_shares\n",
        "\n",
        "        sell_mask = shares < 0\n",
        "        num_sold = abs_shares[sell_mask]\n",
        "        cost_sold = num_sold * avg_cost[sell_mask]\n",
        "        total_cost[sell_mask] -= cost_sold\n",
        "        total_shares[sell_mask] -= num_sold\n",
        "\n",
        "        return total_shares[-1], total_cost[-1], avg_cost[-1]\n",
        "\n",
        "# Example usage\n",
        "data = [(50, 126.81), (-25, 0.0), (30, 125.03), (-10, 0.0)]\n",
        "num_shares, total_cost, avg_cost = Portfolio.calculate_portfolio(data)\n",
        "\n",
        "print(\"Number of shares: \", num_shares)\n",
        "print(\"Total cost: \", round(total_cost, 2))\n",
        "print(\"Average cost: \", round(avg_cost, 2))\n"
      ],
      "metadata": {
        "id": "bBd99tTE1z0o",
        "outputId": "a1300a6d-be53-498e-f931-e4518a8cd602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of shares:  35.0\n",
            "Total cost:  7848.87\n",
            "Average cost:  224.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_train_trade_data.head(30)"
      ],
      "metadata": {
        "id": "yzHIcPMv2TSp",
        "outputId": "eaff2f02-fe59-45be-ec87-c2411eb2c77e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          date   tic       open       high        low      close  \\\n",
              "0   2009-01-02  AAPL   3.067143   3.251429   3.041429   2.758535   \n",
              "1   2009-01-02  AMGN  58.590000  59.080002  57.750000  43.832623   \n",
              "2   2009-01-02   AXP  18.570000  19.520000  18.400000  15.308595   \n",
              "3   2009-01-02    BA  42.799999  45.560001  42.779999  33.941101   \n",
              "4   2009-01-02   CAT  44.910000  46.980000  44.709999  31.408844   \n",
              "5   2009-01-02   CRM   8.025000   8.550000   7.912500   8.505000   \n",
              "6   2009-01-02  CSCO  16.410000  17.000000  16.250000  11.859256   \n",
              "7   2009-01-02   CVX  74.230003  77.300003  73.580002  43.677200   \n",
              "8   2009-01-02   DIS  22.760000  24.030001  22.500000  20.597496   \n",
              "9   2009-01-02    GS  84.019997  87.620003  82.190002  69.251740   \n",
              "10  2009-01-02    HD  23.070000  24.190001  22.959999  16.989412   \n",
              "11  2009-01-02   HON  31.394459  33.224930  31.003576  23.868170   \n",
              "12  2009-01-02   IBM  80.200768  83.738052  80.200768  51.950928   \n",
              "13  2009-01-02  INTC  14.690000  15.250000  14.470000   9.750701   \n",
              "14  2009-01-02   JNJ  60.130001  61.000000  59.040001  39.884251   \n",
              "15  2009-01-02   JPM  31.190001  31.639999  30.469999  22.028296   \n",
              "16  2009-01-02    KO  22.700001  23.000000  22.520000  14.725000   \n",
              "17  2009-01-02   MCD  62.380001  64.129997  62.200001  42.387321   \n",
              "18  2009-01-02   MMM  57.549999  59.389999  57.520000  39.105911   \n",
              "19  2009-01-02   MRK  29.064884  29.694656  28.921757  17.823143   \n",
              "20  2009-01-02  MSFT  19.530001  20.400000  19.370001  15.162152   \n",
              "21  2009-01-02   NKE  12.737500  13.400000  12.577500  11.112614   \n",
              "22  2009-01-02    PG  61.689999  62.970001  61.060001  40.760216   \n",
              "23  2009-01-02   TRV  45.259998  45.910000  44.130001  31.863077   \n",
              "24  2009-01-02   UNH  26.700001  27.719999  26.540001  22.622528   \n",
              "25  2009-01-02     V  13.230000  13.427500  13.060000  12.079284   \n",
              "26  2009-01-02    VZ  32.000774  32.601021  31.466175  15.976192   \n",
              "27  2009-01-02   WBA  24.650000  25.670000  24.610001  17.517086   \n",
              "28  2009-01-02   WMT  55.980000  57.509998  55.779999  41.449009   \n",
              "87  2009-01-05  AAPL   3.327500   3.435000   3.311071   2.874956   \n",
              "\n",
              "          volume  day      macd  boll_ub   boll_lb  rsi_30     cci_30  dx_30  \\\n",
              "0   7.460152e+08  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "1   6.547900e+06  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "2   1.095570e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "3   7.010200e+06  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "4   7.117200e+06  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "5   4.069200e+06  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "6   4.098060e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "7   1.369590e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "8   9.796600e+06  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "9   1.408850e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "10  1.490250e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "11  5.360776e+06  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "12  7.905877e+06  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "13  5.220820e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "14  1.163890e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "15  3.249490e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "16  1.635580e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "17  8.652700e+06  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "18  5.313900e+06  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "19  1.279336e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "20  5.008400e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "21  1.202880e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "22  1.113570e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "23  3.279700e+06  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "24  4.885900e+06  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "25  1.319920e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "26  1.484898e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "27  5.266700e+06  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "28  1.605480e+07  4.0  0.000000  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "87  1.181608e+09  0.0  0.002612  2.98139  2.652101   100.0  66.666667  100.0   \n",
              "\n",
              "    close_30_sma  close_60_sma        vix  turbulence  \n",
              "0       2.758535      2.758535  39.189999         0.0  \n",
              "1      43.832623     43.832623  39.189999         0.0  \n",
              "2      15.308595     15.308595  39.189999         0.0  \n",
              "3      33.941101     33.941101  39.189999         0.0  \n",
              "4      31.408844     31.408844  39.189999         0.0  \n",
              "5       8.505000      8.505000  39.189999         0.0  \n",
              "6      11.859256     11.859256  39.189999         0.0  \n",
              "7      43.677200     43.677200  39.189999         0.0  \n",
              "8      20.597496     20.597496  39.189999         0.0  \n",
              "9      69.251740     69.251740  39.189999         0.0  \n",
              "10     16.989412     16.989412  39.189999         0.0  \n",
              "11     23.868170     23.868170  39.189999         0.0  \n",
              "12     51.950928     51.950928  39.189999         0.0  \n",
              "13      9.750701      9.750701  39.189999         0.0  \n",
              "14     39.884251     39.884251  39.189999         0.0  \n",
              "15     22.028296     22.028296  39.189999         0.0  \n",
              "16     14.725000     14.725000  39.189999         0.0  \n",
              "17     42.387321     42.387321  39.189999         0.0  \n",
              "18     39.105911     39.105911  39.189999         0.0  \n",
              "19     17.823143     17.823143  39.189999         0.0  \n",
              "20     15.162152     15.162152  39.189999         0.0  \n",
              "21     11.112614     11.112614  39.189999         0.0  \n",
              "22     40.760216     40.760216  39.189999         0.0  \n",
              "23     31.863077     31.863077  39.189999         0.0  \n",
              "24     22.622528     22.622528  39.189999         0.0  \n",
              "25     12.079284     12.079284  39.189999         0.0  \n",
              "26     15.976192     15.976192  39.189999         0.0  \n",
              "27     17.517086     17.517086  39.189999         0.0  \n",
              "28     41.449009     41.449009  39.189999         0.0  \n",
              "87      2.816745      2.816745  39.080002         0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce71ce78-4f30-44b9-98b1-ed615f2d4fe8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>tic</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>day</th>\n",
              "      <th>macd</th>\n",
              "      <th>boll_ub</th>\n",
              "      <th>boll_lb</th>\n",
              "      <th>rsi_30</th>\n",
              "      <th>cci_30</th>\n",
              "      <th>dx_30</th>\n",
              "      <th>close_30_sma</th>\n",
              "      <th>close_60_sma</th>\n",
              "      <th>vix</th>\n",
              "      <th>turbulence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>3.067143</td>\n",
              "      <td>3.251429</td>\n",
              "      <td>3.041429</td>\n",
              "      <td>2.758535</td>\n",
              "      <td>7.460152e+08</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.758535</td>\n",
              "      <td>2.758535</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>AMGN</td>\n",
              "      <td>58.590000</td>\n",
              "      <td>59.080002</td>\n",
              "      <td>57.750000</td>\n",
              "      <td>43.832623</td>\n",
              "      <td>6.547900e+06</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>43.832623</td>\n",
              "      <td>43.832623</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>AXP</td>\n",
              "      <td>18.570000</td>\n",
              "      <td>19.520000</td>\n",
              "      <td>18.400000</td>\n",
              "      <td>15.308595</td>\n",
              "      <td>1.095570e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>15.308595</td>\n",
              "      <td>15.308595</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>BA</td>\n",
              "      <td>42.799999</td>\n",
              "      <td>45.560001</td>\n",
              "      <td>42.779999</td>\n",
              "      <td>33.941101</td>\n",
              "      <td>7.010200e+06</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>33.941101</td>\n",
              "      <td>33.941101</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>CAT</td>\n",
              "      <td>44.910000</td>\n",
              "      <td>46.980000</td>\n",
              "      <td>44.709999</td>\n",
              "      <td>31.408844</td>\n",
              "      <td>7.117200e+06</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>31.408844</td>\n",
              "      <td>31.408844</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>CRM</td>\n",
              "      <td>8.025000</td>\n",
              "      <td>8.550000</td>\n",
              "      <td>7.912500</td>\n",
              "      <td>8.505000</td>\n",
              "      <td>4.069200e+06</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>8.505000</td>\n",
              "      <td>8.505000</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>CSCO</td>\n",
              "      <td>16.410000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>16.250000</td>\n",
              "      <td>11.859256</td>\n",
              "      <td>4.098060e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>11.859256</td>\n",
              "      <td>11.859256</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>CVX</td>\n",
              "      <td>74.230003</td>\n",
              "      <td>77.300003</td>\n",
              "      <td>73.580002</td>\n",
              "      <td>43.677200</td>\n",
              "      <td>1.369590e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>43.677200</td>\n",
              "      <td>43.677200</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>DIS</td>\n",
              "      <td>22.760000</td>\n",
              "      <td>24.030001</td>\n",
              "      <td>22.500000</td>\n",
              "      <td>20.597496</td>\n",
              "      <td>9.796600e+06</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>20.597496</td>\n",
              "      <td>20.597496</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>GS</td>\n",
              "      <td>84.019997</td>\n",
              "      <td>87.620003</td>\n",
              "      <td>82.190002</td>\n",
              "      <td>69.251740</td>\n",
              "      <td>1.408850e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>69.251740</td>\n",
              "      <td>69.251740</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>HD</td>\n",
              "      <td>23.070000</td>\n",
              "      <td>24.190001</td>\n",
              "      <td>22.959999</td>\n",
              "      <td>16.989412</td>\n",
              "      <td>1.490250e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>16.989412</td>\n",
              "      <td>16.989412</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>HON</td>\n",
              "      <td>31.394459</td>\n",
              "      <td>33.224930</td>\n",
              "      <td>31.003576</td>\n",
              "      <td>23.868170</td>\n",
              "      <td>5.360776e+06</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>23.868170</td>\n",
              "      <td>23.868170</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>IBM</td>\n",
              "      <td>80.200768</td>\n",
              "      <td>83.738052</td>\n",
              "      <td>80.200768</td>\n",
              "      <td>51.950928</td>\n",
              "      <td>7.905877e+06</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>51.950928</td>\n",
              "      <td>51.950928</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>INTC</td>\n",
              "      <td>14.690000</td>\n",
              "      <td>15.250000</td>\n",
              "      <td>14.470000</td>\n",
              "      <td>9.750701</td>\n",
              "      <td>5.220820e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>9.750701</td>\n",
              "      <td>9.750701</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>JNJ</td>\n",
              "      <td>60.130001</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>59.040001</td>\n",
              "      <td>39.884251</td>\n",
              "      <td>1.163890e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>39.884251</td>\n",
              "      <td>39.884251</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>JPM</td>\n",
              "      <td>31.190001</td>\n",
              "      <td>31.639999</td>\n",
              "      <td>30.469999</td>\n",
              "      <td>22.028296</td>\n",
              "      <td>3.249490e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>22.028296</td>\n",
              "      <td>22.028296</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>KO</td>\n",
              "      <td>22.700001</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>22.520000</td>\n",
              "      <td>14.725000</td>\n",
              "      <td>1.635580e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>14.725000</td>\n",
              "      <td>14.725000</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>MCD</td>\n",
              "      <td>62.380001</td>\n",
              "      <td>64.129997</td>\n",
              "      <td>62.200001</td>\n",
              "      <td>42.387321</td>\n",
              "      <td>8.652700e+06</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>42.387321</td>\n",
              "      <td>42.387321</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>MMM</td>\n",
              "      <td>57.549999</td>\n",
              "      <td>59.389999</td>\n",
              "      <td>57.520000</td>\n",
              "      <td>39.105911</td>\n",
              "      <td>5.313900e+06</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>39.105911</td>\n",
              "      <td>39.105911</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>MRK</td>\n",
              "      <td>29.064884</td>\n",
              "      <td>29.694656</td>\n",
              "      <td>28.921757</td>\n",
              "      <td>17.823143</td>\n",
              "      <td>1.279336e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>17.823143</td>\n",
              "      <td>17.823143</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>19.530001</td>\n",
              "      <td>20.400000</td>\n",
              "      <td>19.370001</td>\n",
              "      <td>15.162152</td>\n",
              "      <td>5.008400e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>15.162152</td>\n",
              "      <td>15.162152</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>NKE</td>\n",
              "      <td>12.737500</td>\n",
              "      <td>13.400000</td>\n",
              "      <td>12.577500</td>\n",
              "      <td>11.112614</td>\n",
              "      <td>1.202880e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>11.112614</td>\n",
              "      <td>11.112614</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>PG</td>\n",
              "      <td>61.689999</td>\n",
              "      <td>62.970001</td>\n",
              "      <td>61.060001</td>\n",
              "      <td>40.760216</td>\n",
              "      <td>1.113570e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>40.760216</td>\n",
              "      <td>40.760216</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>TRV</td>\n",
              "      <td>45.259998</td>\n",
              "      <td>45.910000</td>\n",
              "      <td>44.130001</td>\n",
              "      <td>31.863077</td>\n",
              "      <td>3.279700e+06</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>31.863077</td>\n",
              "      <td>31.863077</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>UNH</td>\n",
              "      <td>26.700001</td>\n",
              "      <td>27.719999</td>\n",
              "      <td>26.540001</td>\n",
              "      <td>22.622528</td>\n",
              "      <td>4.885900e+06</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>22.622528</td>\n",
              "      <td>22.622528</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>V</td>\n",
              "      <td>13.230000</td>\n",
              "      <td>13.427500</td>\n",
              "      <td>13.060000</td>\n",
              "      <td>12.079284</td>\n",
              "      <td>1.319920e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>12.079284</td>\n",
              "      <td>12.079284</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>VZ</td>\n",
              "      <td>32.000774</td>\n",
              "      <td>32.601021</td>\n",
              "      <td>31.466175</td>\n",
              "      <td>15.976192</td>\n",
              "      <td>1.484898e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>15.976192</td>\n",
              "      <td>15.976192</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>WBA</td>\n",
              "      <td>24.650000</td>\n",
              "      <td>25.670000</td>\n",
              "      <td>24.610001</td>\n",
              "      <td>17.517086</td>\n",
              "      <td>5.266700e+06</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>17.517086</td>\n",
              "      <td>17.517086</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>WMT</td>\n",
              "      <td>55.980000</td>\n",
              "      <td>57.509998</td>\n",
              "      <td>55.779999</td>\n",
              "      <td>41.449009</td>\n",
              "      <td>1.605480e+07</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>41.449009</td>\n",
              "      <td>41.449009</td>\n",
              "      <td>39.189999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>2009-01-05</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>3.327500</td>\n",
              "      <td>3.435000</td>\n",
              "      <td>3.311071</td>\n",
              "      <td>2.874956</td>\n",
              "      <td>1.181608e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002612</td>\n",
              "      <td>2.98139</td>\n",
              "      <td>2.652101</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.816745</td>\n",
              "      <td>2.816745</td>\n",
              "      <td>39.080002</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce71ce78-4f30-44b9-98b1-ed615f2d4fe8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce71ce78-4f30-44b9-98b1-ed615f2d4fe8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce71ce78-4f30-44b9-98b1-ed615f2d4fe8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p25N1SSvRL01"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the most of your colab subscription",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}